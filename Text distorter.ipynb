{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Deep Speech neural net on text data\n",
    "\n",
    "1) Global set up - setting up logging and root directory for including the modules, files etc.\n",
    "2) Text distortion function to generate training data.\n",
    "3) Load Simple Wikipedia, clean up the data, transform into idx-s and one-hot vectors.\n",
    "4) Initialize training and validation datasets.\n",
    "5) Run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from nltk import tokenize # Text to sentences\n",
    "import pandas as pd # Train and Validation data generation\n",
    "import re\n",
    "import pprint\n",
    "from src.wikipedia import Wikipedia\n",
    "#random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortText(text, swap_type = \"symboldelete\"):\n",
    "       \n",
    "    # Distortion types\n",
    "    distortions_symbols = [\"symbolswap\",\"symboldelete\",\"symbolreplacerandom\",\"symbolreplaceprev\"]\n",
    "    distortions_words = [\"wordtrimright\", \"wordswapprev\"]\n",
    "    distortions_sentences = [\"deletespaces\"]\n",
    "    # Symbol operations: max 2 symbols per word, max 2 symbol distortion types per word\n",
    "    # Word trim: max 1 per word\n",
    "    # Delete spaces: max all spaces, min 0 spaces\n",
    "    \n",
    "    list_words = text.split()\n",
    "    \n",
    "    distortion_prob_symbol = 0.5\n",
    "    for i in range(len(list_words)):\n",
    "        r = random.random()\n",
    "        if swap_type == \"symbolswap\":\n",
    "            # SYMBOL SWAP\n",
    "            ## min distorted symbol = 0, max distorted symbol = len-2 because of indexing with 0 and this is the 1st swappable symbol\n",
    "            distorted_symbol = round((len(list_words[i]) - 2) * random.random())\n",
    "            s1 = list_words[i][distorted_symbol + 1]\n",
    "            #print(\"{}---{}-{}-{}-{}\".format(distorted_symbol, list_words[i][0:distorted_symbol - 1], list_words[i][distorted_symbol], s1, list_words[i][distorted_symbol + 1:]))\n",
    "            #print(list_words[i][0:(distorted_symbol - 1) * (distorted_symbol > 0)])\n",
    "            list_words[i] = list_words[i][0:distorted_symbol] + s1 + list_words[i][distorted_symbol] + list_words[i][distorted_symbol + 2:]\n",
    "        elif swap_type == \"symboldelete\":\n",
    "            # SYMBOL DELETE\n",
    "            ## min distorted symbol = 0, max distorted symbol = len-1\n",
    "            distorted_symbol = round((len(list_words[i]) - 1) * random.random())\n",
    "            list_words[i] = list_words[i][0:distorted_symbol] + list_words[i][distorted_symbol + 1:]\n",
    "        elif swap_type == \"symbolreplacerandom\":\n",
    "            symbols = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "            # SYMBOL REPLACE RANDOM\n",
    "            ## min distorted symbol = 0, max distorted symbol = len-1\n",
    "            distorted_symbol = round((len(list_words[i]) - 1) * random.random())\n",
    "            replace = symbols[round((len(symbols) - 1) * random.random())]\n",
    "            list_words[i] = list_words[i][0:distorted_symbol] + replace + list_words[i][distorted_symbol + 1:]\n",
    "        elif swap_type == \"symbolreplaceprev\":\n",
    "            # SYMBOL REPLACE PREVIOUS\n",
    "            ## min distorted symbol = 1, max distorted symbol = len - 1\n",
    "            distorted_symbol = round(1 + (len(list_words[i]) - 2) * random.random())\n",
    "            #print(\"{} - {}\".format(distorted_symbol, list_words[i]))\n",
    "            #print(\"{}---{}-{}-{}\".format(distorted_symbol, list_words[i][0:(distorted_symbol)], list_words[i][distorted_symbol - 1], list_words[i][distorted_symbol + 1 * (len(list_words[i]) > distorted_symbol):distorted_symbol + 1 * (len(list_words[i]) > distorted_symbol)]))\n",
    "            list_words[i] = list_words[i][0:(distorted_symbol)] + list_words[i][distorted_symbol - 1] + list_words[i][distorted_symbol + 1:]\n",
    "        \n",
    "        if swap_type == \"wordtrimright\":\n",
    "            # WORD TRIM RIGHT\n",
    "            ## min symbol = 1, max distorted symbol = len - 1\n",
    "            trim_from = round(1 + (len(list_words[i]) - 1) * random.random())\n",
    "            list_words[i] = list_words[i][:trim_from]\n",
    "    \n",
    "    if swap_type == \"wordswapprev\":\n",
    "        # WORD SWAP\n",
    "        word_id = round((len(list_words) - 2) * random.random())\n",
    "        s1 = list_words[word_id]\n",
    "        list_words[word_id] = list_words[word_id + 1]\n",
    "        list_words[word_id + 1] = s1\n",
    "        \n",
    "    if swap_type == \"deletespaces\":\n",
    "        # DELETE WHITESPACES\n",
    "        ## -2 because there are len - 1 spaces in total\n",
    "        space_id = round((len(list_words) - 2) * random.random())\n",
    "        list_words[space_id] = list_words[space_id] + list_words[space_id + 1]\n",
    "        list_words.pop(space_id + 1)\n",
    "            \n",
    "    distortedText = \" \".join(list_words)\n",
    "    return distortedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atnons lieks pziza\n",
      "ntons like pzza\n",
      "bntons lwkes puzza\n",
      "Antoos likee ppzza\n",
      "Anto like pi\n",
      "Antons pizza likes\n",
      "Antons likespizza\n"
     ]
    }
   ],
   "source": [
    "text = \"Antons likes pizza\"\n",
    "print(distortText(text=text,swap_type=\"symbolswap\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"symboldelete\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"symbolreplacerandom\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"symbolreplaceprev\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"wordtrimright\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"wordswapprev\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"deletespaces\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplewiki-latest-pages-articles-multistream.xml.bz2\n",
      "Loading parsed documents.\n",
      "Loading preprocessed documents.\n",
      "Wikipedia loaded.\n"
     ]
    }
   ],
   "source": [
    "wikipedia = Wikipedia(\n",
    "    language=\"simple\",\n",
    "    cache_directory_url=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up simple wikipedia texts\n",
    "pattern_ignored_words = re.compile(\n",
    "    r\"\"\"\n",
    "    (?:(?:thumb|thumbnail|left|right|\\d+px|upright(?:=[0-9\\.]+)?)\\|)+\n",
    "    |^\\s*\\|.+$\n",
    "    |^REDIRECT\\b\"\"\",\n",
    "    flags=re.DOTALL | re.UNICODE | re.VERBOSE | re.MULTILINE)\n",
    "pattern_new_lines = re.compile('[\\n\\r ]+', re.UNICODE)\n",
    "texts = [wikipedia.documents[i].text for i in range(len(wikipedia.documents))]\n",
    "texts = [pattern_ignored_words.sub('', texts[i]) for i in range(len(texts))]\n",
    "texts = [pattern_new_lines.sub(' ', texts[i]) for i in range(len(texts))]\n",
    "texts = [texts[i].replace(\"\\\\\", \"\") for i in range(len(texts))]\n",
    "texts = [texts[i].replace(\"\\xa0\", \" \") for i in range(len(texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple wikipedia article texts into single sentences\n",
    "\n",
    "sentences = []\n",
    "sentences += [tokenize.sent_tokenize(texts[i]) for i in range(len(texts))]\n",
    "#sentences += [texts[i].split(\". \") for i in range(len(texts))] #len(texts)\n",
    "# Now sentences is a list of lists. The next expression flattens it into one long list.\n",
    "sentences = [item for sublist in sentences for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Month Spring flowers in April in the Northern Hemisphere.',\n",
      " 'April comes between March and May, making it the fourth month of the year.',\n",
      " 'It also comes first in the year out of the four months that have 30 days, as '\n",
      " 'June, September and November are later in the year.']\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(sentences[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1142223\n",
      "648280\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "for i in reversed(range(len(sentences))):\n",
    "    if len(sentences[i]) < 20 or len(sentences[i]) > 100 \\\n",
    "        or sentences[i][0:9] == \"Category:\" \\\n",
    "        or sentences[i][0:13] == \"Related pages\" \\\n",
    "        or sentences[i][0:10] == \"References\" \\\n",
    "        or sentences[i][0:14] == \"Other websites\":\n",
    "        sentences.pop(i)\n",
    "print(len(sentences))\n",
    "\n",
    "#Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['McCarty later explained the work for the general reader.',\n",
      " 'Uses Propene is produced from fossil fuels, and from coal.',\n",
      " 'Propene is the second most important product used in the petrochemical '\n",
      " 'industry, after Ethene.',\n",
      " 'About two thirds are used to produce Polypropylene.',\n",
      " 'Propene and benzene are converted to acetone and phenol via the cumene '\n",
      " 'process.']\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(sentences[530000:530005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.0\n",
      "20 - 2262\n",
      "21 - 2398\n",
      "22 - 6644\n",
      "23 - 2670\n",
      "24 - 3251\n",
      "25 - 3208\n",
      "26 - 3194\n",
      "27 - 3488\n",
      "28 - 3590\n",
      "29 - 3899\n",
      "30 - 4177\n",
      "31 - 4754\n",
      "32 - 4765\n",
      "33 - 5145\n",
      "34 - 5282\n",
      "35 - 5572\n",
      "36 - 5968\n",
      "37 - 6002\n",
      "38 - 6286\n",
      "39 - 6727\n",
      "40 - 6990\n",
      "41 - 7439\n",
      "42 - 7753\n",
      "43 - 8142\n",
      "44 - 8835\n",
      "45 - 9162\n",
      "46 - 9368\n",
      "47 - 9875\n",
      "48 - 10349\n",
      "49 - 10546\n",
      "50 - 10651\n",
      "51 - 10699\n",
      "52 - 10742\n",
      "53 - 10845\n",
      "54 - 10767\n",
      "55 - 10812\n",
      "56 - 10723\n",
      "57 - 10611\n",
      "58 - 10619\n",
      "59 - 10610\n",
      "60 - 10586\n",
      "61 - 10580\n",
      "62 - 10482\n",
      "63 - 10394\n",
      "64 - 10473\n",
      "65 - 10232\n",
      "66 - 10094\n",
      "67 - 10029\n",
      "68 - 10016\n",
      "69 - 9925\n",
      "70 - 9713\n",
      "71 - 9810\n",
      "72 - 9634\n",
      "73 - 9984\n",
      "74 - 9663\n",
      "75 - 9281\n",
      "76 - 9452\n",
      "77 - 9147\n",
      "78 - 9420\n",
      "79 - 8992\n",
      "80 - 8905\n",
      "81 - 8782\n",
      "82 - 8805\n",
      "83 - 8615\n",
      "84 - 8441\n",
      "85 - 8453\n",
      "86 - 8247\n",
      "87 - 8388\n",
      "88 - 8352\n",
      "89 - 7819\n",
      "90 - 7911\n",
      "91 - 7842\n",
      "92 - 7697\n",
      "93 - 7515\n",
      "94 - 7169\n",
      "95 - 7359\n",
      "96 - 7289\n",
      "97 - 7207\n",
      "98 - 7034\n",
      "99 - 6955\n",
      "100 - 6768\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "sentence_lengths = [len(sentences[i]) for i in range(len(sentences))]\n",
    "print(statistics.median(sentence_lengths))\n",
    "\n",
    "from collections import defaultdict\n",
    "appearances = defaultdict(int)\n",
    "\n",
    "sentence_lengths.sort()\n",
    "\n",
    "for curr in sentence_lengths:\n",
    "    appearances[curr] += 1\n",
    "    \n",
    "a = set(sentence_lengths) \n",
    "for i in a:\n",
    "    print(\"{} - {}\".format(i, appearances[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabets = {'a':0, 'b':1, 'c':2, 'd':3, 'e':4, 'f':5, 'g':6, 'h':7, 'i':8, 'j':9, 'k':10, 'l':11, 'm':12, 'n':13, 'o':14,\n",
    "            'p':15, 'q':16, 'r':17, 's':18, 't':19, 'u':20, 'v':21, 'w':22, 'x':23, 'y':24, 'z':25, \n",
    "            '0':26, '1':27, '2':28, '3':29, '4':30, '5':31, '6':32, '7':33, '8':34, '9':35, \n",
    "            ' ':36, ',':37, '.':38, ':':39, ';':40, '\"':41, \"'\":42, '':43, '(':44, ')':45} #43 = unknown symbol\n",
    "\n",
    "idxs = [alphabets[ch] if ch in alphabets else 43 for ch in 'az 123#']\n",
    "\n",
    "idxs\n",
    "\n",
    "#one_hot = tf.one_hot(idxs, depth=len(alphabets), dtype=tf.uint8)\n",
    "\n",
    "#sess = tf.InteractiveSession()\n",
    "#one_hot.eval()\n",
    "one_hot = to_categorical(idxs, num_classes = len(alphabets))\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_idxs = []\n",
    "for i in range(len(sentences)):\n",
    "    idx = []\n",
    "    for j in sentences[i]:\n",
    "        if j in alphabets:\n",
    "            idx += [alphabets[j]]\n",
    "        else:\n",
    "            idx += [43]\n",
    "    sentences_idxs.append(idx)\n",
    "    \n",
    "#sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(len(sentences_idxs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(10000)]\n",
    "sentences_onehot = [to_categorical(sentences_idxs[i], num_classes = len(alphabets)) for i in range(10000)]\n",
    "sentences = sentences[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Generate the data examples\n",
    "# X and Y are identical for the test purposes\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    {'X': sentences_onehot,\n",
    "     'Y': sentences\n",
    "    })\n",
    "\n",
    "print(len(sentences_onehot[100][0]))\n",
    "print(len(sentences_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "648280"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "#from KerasDeepSpeech.data import combine_all_wavs_and_trans_from_csvs\n",
    "from KerasDeepSpeech.generator import BatchGenerator\n",
    "from KerasDeepSpeech.model import *\n",
    "from KerasDeepSpeech.report import ReportCallback\n",
    "from KerasDeepSpeech.utils import load_model_checkpoint, save_model, MemoryCallback\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "#######################################################\n",
    "\n",
    "# Prevent pool_allocator message\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#######################################################\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    '''\n",
    "    There are 5 simple steps to this program\n",
    "    '''\n",
    "\n",
    "    #1. combine all data into 2 dataframes (train, valid)\n",
    "    print(\"Getting data from arguments\")\n",
    "    #train_dataprops, df_train = combine_all_wavs_and_trans_from_csvs(args.train_files, sortagrad=args.sortagrad)\n",
    "    #valid_dataprops, df_valid = combine_all_wavs_and_trans_from_csvs(args.valid_files, sortagrad=args.sortagrad)\n",
    "\n",
    "    train_ratio = 0.9\n",
    "    args.model_arch = 0\n",
    "    args.opt = \"adam\"\n",
    "    args.train_steps = 0\n",
    "    args.epochs = 10\n",
    "    args.valid_steps = 0\n",
    "    args.batchsize = 32 #was 16\n",
    "    args.name = \"\"\n",
    "    args.loadcheckpointpath = \"\"\n",
    "    args.fc_size = 512\n",
    "    args.rnn_size = 512\n",
    "    args.learning_rate = 0.01\n",
    "    args.memcheck = False\n",
    "    args.tensorboard = True\n",
    "    \n",
    "    model_input_type = \"text\"\n",
    "    \n",
    "    \n",
    "    df_train = data[0:int(train_ratio * len(sentences_onehot))]\n",
    "    df_valid = data[int(train_ratio * len(sentences_onehot)) + 1:]\n",
    "\n",
    "\n",
    "    ## 2. init data generators\n",
    "    print(\"Creating data batch generators\")\n",
    "    traindata = BatchGenerator(dataframe=df_train, dataproperties=None,\n",
    "                              training=True, batch_size=args.batchsize, model_input_type=model_input_type)\n",
    "    validdata = BatchGenerator(dataframe=df_valid, dataproperties=None,\n",
    "                              training=False, batch_size=args.batchsize, model_input_type=model_input_type)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    output_dir = os.path.join('checkpoints/results',\n",
    "                                  'model%s_%s' % (args.model_arch,\n",
    "                                             args.name))\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "    ## 3. Load existing or create new model\n",
    "    if args.loadcheckpointpath:\n",
    "        # load existing\n",
    "        print(\"Loading model\")\n",
    "\n",
    "        cp = args.loadcheckpointpath\n",
    "        assert(os.path.isdir(cp))\n",
    "\n",
    "        model_path = os.path.join(cp, \"model\")\n",
    "        # assert(os.path.isfile(model_path))\n",
    "\n",
    "        model = load_model_checkpoint(model_path)\n",
    "\n",
    "\n",
    "        print(\"Model loaded\")\n",
    "    else:\n",
    "        # new model recipes here\n",
    "        print('New model DS{}'.format(args.model_arch))\n",
    "        if (args.model_arch == 0):\n",
    "            # DeepSpeech1 with Dropout\n",
    "            model = ds1_dropout(input_dim=len(alphabets), fc_size=args.fc_size, rnn_size=args.rnn_size,dropout=[0.1,0.1,0.1], output_dim=len(alphabets) + 1)\n",
    "\n",
    "        elif(args.model_arch==1):\n",
    "            # DeepSpeech1 - no dropout\n",
    "            model = ds1(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n",
    "\n",
    "        elif(args.model_arch==2):\n",
    "            # DeepSpeech2 model\n",
    "            model = ds2_gru_model(input_dim=161, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n",
    "\n",
    "        elif(args.model_arch==3):\n",
    "            # own model\n",
    "            model = ownModel(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, dropout=[0.1, 0.1, 0.1], output_dim=29)\n",
    "\n",
    "        elif(args.model_arch==4):\n",
    "            # graves model\n",
    "            model = graves(input_dim=26, rnn_size=args.rnn_size, output_dim=29, std=0.5)\n",
    "\n",
    "        elif(args.model_arch==5):\n",
    "            #cnn city\n",
    "            model = cnn_city(input_dim=161, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n",
    "\n",
    "        elif(args.model_arch == 6):\n",
    "            # constrained model\n",
    "            model = const(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n",
    "        else:\n",
    "            raise(\"model not found\")\n",
    "\n",
    "        print(model.summary(line_length=80))\n",
    "\n",
    "        #required to save the JSON\n",
    "        save_model(model, output_dir)\n",
    "\n",
    "    if (args.opt.lower() == 'sgd'):\n",
    "        opt = SGD(lr=args.learning_rate, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "    elif (args.opt.lower() == 'adam'):\n",
    "        opt = Adam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n",
    "    elif (args.opt.lower() == 'nadam'):\n",
    "        opt = Nadam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n",
    "    else:\n",
    "        raise \"optimiser not recognised\"\n",
    "\n",
    "    model.compile(optimizer=opt, loss=ctc)\n",
    "\n",
    "    ## 4. train\n",
    "\n",
    "    if args.train_steps == 0:\n",
    "        args.train_steps = len(df_train.index) // args.batchsize\n",
    "        # print(args.train_steps)\n",
    "    # we use 1/xth of the validation data at each epoch end to test val score\n",
    "    if args.valid_steps == 0:\n",
    "\n",
    "        args.valid_steps = (len(df_valid.index) // args.batchsize)\n",
    "        # print(args.valid_steps)\n",
    "\n",
    "\n",
    "    if args.memcheck:\n",
    "        cb_list = [MemoryCallback()]\n",
    "    else:\n",
    "        cb_list = []\n",
    "\n",
    "    if args.tensorboard:\n",
    "        tb_cb = TensorBoard(log_dir='./tensorboard/{}/'.format(args.name), write_graph=False, write_images=True)\n",
    "        cb_list.append(tb_cb)\n",
    "\n",
    "    y_pred = model.get_layer('ctc').input[0]\n",
    "    input_data = model.get_layer('the_input').input\n",
    "\n",
    "    report = K.function([input_data, K.learning_phase()], [y_pred])\n",
    "    report_cb = ReportCallback(report, validdata, model, args.name, save=True)\n",
    "\n",
    "    cb_list.append(report_cb)\n",
    "\n",
    "    model.fit_generator(generator=traindata.next_batch(),\n",
    "                        steps_per_epoch=args.train_steps,\n",
    "                        epochs=args.epochs,\n",
    "                        callbacks=cb_list,\n",
    "                        validation_data=validdata.next_batch(),\n",
    "                        validation_steps=args.valid_steps,\n",
    "                        initial_epoch=0,\n",
    "                        verbose=1,\n",
    "                        class_weight=None,\n",
    "                        max_q_size=10,\n",
    "                        workers=1,\n",
    "                        pickle_safe=False\n",
    "                        )\n",
    "\n",
    "    # K.clear_session()\n",
    "\n",
    "    ## These are the most important metrics\n",
    "    print(\"Mean WER   :\", report_cb.mean_wer_log)\n",
    "    print(\"Mean LER   :\", report_cb.mean_ler_log)\n",
    "    print(\"NormMeanLER:\", report_cb.norm_mean_ler_log)\n",
    "\n",
    "    # export to csv?\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from arguments\n",
      "Creating data batch generators\n",
      "New model DS0\n",
      "________________________________________________________________________________\n",
      "Layer (type)              Output Shape      Param #  Connected to               \n",
      "================================================================================\n",
      "the_input (InputLayer)    (None, None, 46)  0                                   \n",
      "________________________________________________________________________________\n",
      "time_distributed_1 (TimeD (None, None, 512) 24064    the_input[0][0]            \n",
      "________________________________________________________________________________\n",
      "time_distributed_2 (TimeD (None, None, 512) 0        time_distributed_1[0][0]   \n",
      "________________________________________________________________________________\n",
      "time_distributed_3 (TimeD (None, None, 512) 262656   time_distributed_2[0][0]   \n",
      "________________________________________________________________________________\n",
      "time_distributed_4 (TimeD (None, None, 512) 0        time_distributed_3[0][0]   \n",
      "________________________________________________________________________________\n",
      "time_distributed_5 (TimeD (None, None, 512) 262656   time_distributed_4[0][0]   \n",
      "________________________________________________________________________________\n",
      "time_distributed_6 (TimeD (None, None, 512) 0        time_distributed_5[0][0]   \n",
      "________________________________________________________________________________\n",
      "bidirectional_1 (Bidirect (None, None, 512) 4198400  time_distributed_6[0][0]   \n",
      "________________________________________________________________________________\n",
      "time_distributed_7 (TimeD (None, None, 512) 0        bidirectional_1[0][0]      \n",
      "________________________________________________________________________________\n",
      "out (TimeDistributed)     (None, None, 47)  24111    time_distributed_7[0][0]   \n",
      "________________________________________________________________________________\n",
      "the_labels (InputLayer)   (None, None)      0                                   \n",
      "________________________________________________________________________________\n",
      "input_length (InputLayer) (None, 1)         0                                   \n",
      "________________________________________________________________________________\n",
      "label_length (InputLayer) (None, 1)         0                                   \n",
      "________________________________________________________________________________\n",
      "ctc (Lambda)              (None, 1)         0        out[0][0]                  \n",
      "                                                     the_labels[0][0]           \n",
      "                                                     input_length[0][0]         \n",
      "                                                     label_length[0][0]         \n",
      "================================================================================\n",
      "Total params: 4,771,887\n",
      "Trainable params: 4,771,887\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________\n",
      "None\n",
      "Saving model at: checkpoints/results\\model0_/model.json checkpoints/results\\model0_/model.h5\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247/281 [=========================>....] - ETA: 10:38 - loss: in - ETA: 6:03 - loss: in - ETA: 4:31 - loss: i - ETA: 3:45 - loss: i - ETA: 3:17 - loss: i - ETA: 2:58 - loss: i - ETA: 2:44 - loss: i - ETA: 2:35 - loss: i - ETA: 2:27 - loss: i - ETA: 2:21 - loss: i - ETA: 2:15 - loss: i - ETA: 2:11 - loss: i - ETA: 2:07 - loss: i - ETA: 2:04 - loss: i - ETA: 2:01 - loss: i - ETA: 1:59 - loss: i - ETA: 1:56 - loss: i - ETA: 1:54 - loss: i - ETA: 1:52 - loss: i - ETA: 1:51 - loss: i - ETA: 1:49 - loss: i - ETA: 1:48 - loss: i - ETA: 1:46 - loss: i - ETA: 1:45 - loss: i - ETA: 1:44 - loss: i - ETA: 1:43 - loss: i - ETA: 1:42 - loss: i - ETA: 1:41 - loss: i - ETA: 1:40 - loss: i - ETA: 1:39 - loss: i - ETA: 1:38 - loss: i - ETA: 1:37 - loss: i - ETA: 1:36 - loss: i - ETA: 1:35 - loss: i - ETA: 1:35 - loss: i - ETA: 1:34 - loss: i - ETA: 1:33 - loss: i - ETA: 1:32 - loss: i - ETA: 1:32 - loss: i - ETA: 1:31 - loss: i - ETA: 1:30 - loss: i - ETA: 1:30 - loss: i - ETA: 1:29 - loss: i - ETA: 1:28 - loss: i - ETA: 1:28 - loss: i - ETA: 1:27 - loss: i - ETA: 1:26 - loss: i - ETA: 1:26 - loss: i - ETA: 1:25 - loss: i - ETA: 1:25 - loss: i - ETA: 1:24 - loss: i - ETA: 1:24 - loss: i - ETA: 1:23 - loss: i - ETA: 1:23 - loss: i - ETA: 1:22 - loss: i - ETA: 1:21 - loss: i - ETA: 1:21 - loss: i - ETA: 1:20 - loss: i - ETA: 1:20 - loss: i - ETA: 1:20 - loss: i - ETA: 1:19 - loss: i - ETA: 1:19 - loss: i - ETA: 1:18 - loss: i - ETA: 1:18 - loss: i - ETA: 1:17 - loss: i - ETA: 1:17 - loss: i - ETA: 1:16 - loss: i - ETA: 1:16 - loss: i - ETA: 1:15 - loss: i - ETA: 1:15 - loss: i - ETA: 1:14 - loss: i - ETA: 1:14 - loss: i - ETA: 1:13 - loss: i - ETA: 1:13 - loss: i - ETA: 1:13 - loss: i - ETA: 1:12 - loss: i - ETA: 1:12 - loss: i - ETA: 1:11 - loss: i - ETA: 1:11 - loss: i - ETA: 1:10 - loss: i - ETA: 1:10 - loss: i - ETA: 1:10 - loss: i - ETA: 1:09 - loss: i - ETA: 1:09 - loss: i - ETA: 1:08 - loss: i - ETA: 1:08 - loss: i - ETA: 1:07 - loss: i - ETA: 1:07 - loss: i - ETA: 1:07 - loss: i - ETA: 1:06 - loss: i - ETA: 1:06 - loss: i - ETA: 1:05 - loss: i - ETA: 1:05 - loss: i - ETA: 1:05 - loss: i - ETA: 1:04 - loss: i - ETA: 1:04 - loss: i - ETA: 1:04 - loss: i - ETA: 1:03 - loss: i - ETA: 1:03 - loss: i - ETA: 1:02 - loss: i - ETA: 1:02 - loss: i - ETA: 1:02 - loss: i - ETA: 1:01 - loss: i - ETA: 1:01 - loss: i - ETA: 1:01 - loss: i - ETA: 1:00 - loss: i - ETA: 1:00 - loss: i - ETA: 59s - loss: inf - ETA: 59s - loss: in - ETA: 59s - loss: in - ETA: 58s - loss: in - ETA: 58s - loss: in - ETA: 57s - loss: in - ETA: 57s - loss: in - ETA: 57s - loss: in - ETA: 56s - loss: in - ETA: 56s - loss: in - ETA: 56s - loss: in - ETA: 55s - loss: in - ETA: 55s - loss: in - ETA: 55s - loss: in - ETA: 54s - loss: in - ETA: 54s - loss: in - ETA: 54s - loss: in - ETA: 53s - loss: in - ETA: 53s - loss: in - ETA: 52s - loss: in - ETA: 52s - loss: in - ETA: 52s - loss: in - ETA: 51s - loss: in - ETA: 51s - loss: in - ETA: 51s - loss: in - ETA: 50s - loss: in - ETA: 50s - loss: in - ETA: 49s - loss: in - ETA: 49s - loss: in - ETA: 49s - loss: in - ETA: 48s - loss: in - ETA: 48s - loss: in - ETA: 48s - loss: in - ETA: 47s - loss: in - ETA: 47s - loss: in - ETA: 47s - loss: in - ETA: 46s - loss: in - ETA: 46s - loss: in - ETA: 46s - loss: in - ETA: 45s - loss: in - ETA: 45s - loss: in - ETA: 44s - loss: in - ETA: 44s - loss: in - ETA: 44s - loss: in - ETA: 43s - loss: in - ETA: 43s - loss: in - ETA: 43s - loss: in - ETA: 42s - loss: in - ETA: 42s - loss: in - ETA: 42s - loss: in - ETA: 41s - loss: in - ETA: 41s - loss: in - ETA: 41s - loss: in - ETA: 40s - loss: in - ETA: 40s - loss: in - ETA: 40s - loss: in - ETA: 39s - loss: in - ETA: 39s - loss: in - ETA: 39s - loss: in - ETA: 38s - loss: in - ETA: 38s - loss: in - ETA: 37s - loss: in - ETA: 37s - loss: in - ETA: 37s - loss: in - ETA: 36s - loss: in - ETA: 36s - loss: in - ETA: 36s - loss: in - ETA: 35s - loss: in - ETA: 35s - loss: in - ETA: 35s - loss: in - ETA: 34s - loss: in - ETA: 34s - loss: in - ETA: 34s - loss: in - ETA: 33s - loss: in - ETA: 33s - loss: in - ETA: 33s - loss: in - ETA: 32s - loss: in - ETA: 32s - loss: in - ETA: 32s - loss: in - ETA: 31s - loss: in - ETA: 31s - loss: in - ETA: 31s - loss: in - ETA: 30s - loss: in - ETA: 30s - loss: in - ETA: 30s - loss: in - ETA: 29s - loss: in - ETA: 29s - loss: in - ETA: 28s - loss: in - ETA: 28s - loss: in - ETA: 28s - loss: in - ETA: 27s - loss: in - ETA: 27s - loss: in - ETA: 27s - loss: in - ETA: 26s - loss: in - ETA: 26s - loss: in - ETA: 26s - loss: in - ETA: 25s - loss: in - ETA: 25s - loss: in - ETA: 25s - loss: in - ETA: 24s - loss: in - ETA: 24s - loss: in - ETA: 24s - loss: in - ETA: 23s - loss: in - ETA: 23s - loss: in - ETA: 23s - loss: in - ETA: 22s - loss: in - ETA: 22s - loss: in - ETA: 22s - loss: in - ETA: 21s - loss: in - ETA: 21s - loss: in - ETA: 21s - loss: in - ETA: 20s - loss: in - ETA: 20s - loss: in - ETA: 20s - loss: in - ETA: 19s - loss: in - ETA: 19s - loss: in - ETA: 19s - loss: in - ETA: 18s - loss: in - ETA: 18s - loss: in - ETA: 18s - loss: in - ETA: 17s - loss: in - ETA: 17s - loss: in - ETA: 17s - loss: in - ETA: 16s - loss: in - ETA: 16s - loss: in - ETA: 16s - loss: in - ETA: 15s - loss: in - ETA: 15s - loss: in - ETA: 15s - loss: in - ETA: 14s - loss: in - ETA: 14s - loss: in - ETA: 14s - loss: in - ETA: 13s - loss: in - ETA: 13s - loss: in - ETA: 13s - loss: in - ETA: 12s - loss: in - ETA: 12s - loss: in - ETA: 12s - loss: in - ETA: 11s - loss: in - ETA: 11s - loss: inf"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
