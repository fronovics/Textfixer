{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import pprint\n",
    "from src.wikipedia import Wikipedia\n",
    "#random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortText(text, swap_type = \"symboldelete\"):\n",
    "       \n",
    "    # Distortion types\n",
    "    distortions_symbols = [\"symbolswap\",\"symboldelete\",\"symbolreplacerandom\",\"symbolreplaceprev\"]\n",
    "    distortions_words = [\"wordtrimright\", \"wordswapprev\"]\n",
    "    distortions_sentences = [\"deletespaces\"]\n",
    "    # Symbol operations: max 2 symbols per word, max 2 symbol distortion types per word\n",
    "    # Word trim: max 1 per word\n",
    "    # Delete spaces: max all spaces, min 0 spaces\n",
    "    \n",
    "    list_words = text.split()\n",
    "    \n",
    "    distortion_prob_symbol = 0.5\n",
    "    for i in range(len(list_words)):\n",
    "        r = random.random()\n",
    "        if swap_type == \"symbolswap\":\n",
    "            # SYMBOL SWAP\n",
    "            ## min distorted symbol = 0, max distorted symbol = len-2 because of indexing with 0 and this is the 1st swappable symbol\n",
    "            distorted_symbol = round((len(list_words[i]) - 2) * random.random())\n",
    "            s1 = list_words[i][distorted_symbol + 1]\n",
    "            #print(\"{}---{}-{}-{}-{}\".format(distorted_symbol, list_words[i][0:distorted_symbol - 1], list_words[i][distorted_symbol], s1, list_words[i][distorted_symbol + 1:]))\n",
    "            #print(list_words[i][0:(distorted_symbol - 1) * (distorted_symbol > 0)])\n",
    "            list_words[i] = list_words[i][0:distorted_symbol] + s1 + list_words[i][distorted_symbol] + list_words[i][distorted_symbol + 2:]\n",
    "        elif swap_type == \"symboldelete\":\n",
    "            # SYMBOL DELETE\n",
    "            ## min distorted symbol = 0, max distorted symbol = len-1\n",
    "            distorted_symbol = round((len(list_words[i]) - 1) * random.random())\n",
    "            list_words[i] = list_words[i][0:distorted_symbol] + list_words[i][distorted_symbol + 1:]\n",
    "        elif swap_type == \"symbolreplacerandom\":\n",
    "            symbols = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "            # SYMBOL REPLACE RANDOM\n",
    "            ## min distorted symbol = 0, max distorted symbol = len-1\n",
    "            distorted_symbol = round((len(list_words[i]) - 1) * random.random())\n",
    "            replace = symbols[round((len(symbols) - 1) * random.random())]\n",
    "            list_words[i] = list_words[i][0:distorted_symbol] + replace + list_words[i][distorted_symbol + 1:]\n",
    "        elif swap_type == \"symbolreplaceprev\":\n",
    "            # SYMBOL REPLACE PREVIOUS\n",
    "            ## min distorted symbol = 1, max distorted symbol = len - 1\n",
    "            distorted_symbol = round(1 + (len(list_words[i]) - 2) * random.random())\n",
    "            #print(\"{} - {}\".format(distorted_symbol, list_words[i]))\n",
    "            #print(\"{}---{}-{}-{}\".format(distorted_symbol, list_words[i][0:(distorted_symbol)], list_words[i][distorted_symbol - 1], list_words[i][distorted_symbol + 1 * (len(list_words[i]) > distorted_symbol):distorted_symbol + 1 * (len(list_words[i]) > distorted_symbol)]))\n",
    "            list_words[i] = list_words[i][0:(distorted_symbol)] + list_words[i][distorted_symbol - 1] + list_words[i][distorted_symbol + 1:]\n",
    "        \n",
    "        if swap_type == \"wordtrimright\":\n",
    "            # WORD TRIM RIGHT\n",
    "            ## min symbol = 1, max distorted symbol = len - 1\n",
    "            trim_from = round(1 + (len(list_words[i]) - 1) * random.random())\n",
    "            list_words[i] = list_words[i][:trim_from]\n",
    "    \n",
    "    if swap_type == \"wordswapprev\":\n",
    "        # WORD SWAP\n",
    "        word_id = round((len(list_words) - 2) * random.random())\n",
    "        s1 = list_words[word_id]\n",
    "        list_words[word_id] = list_words[word_id + 1]\n",
    "        list_words[word_id + 1] = s1\n",
    "        \n",
    "    if swap_type == \"deletespaces\":\n",
    "        # DELETE WHITESPACES\n",
    "        ## -2 because there are len - 1 spaces in total\n",
    "        space_id = round((len(list_words) - 2) * random.random())\n",
    "        list_words[space_id] = list_words[space_id] + list_words[space_id + 1]\n",
    "        list_words.pop(space_id + 1)\n",
    "            \n",
    "    distortedText = \" \".join(list_words)\n",
    "    return distortedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Antons likes pizza\"\n",
    "print(distortText(text=text,swap_type=\"symbolswap\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"symboldelete\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"symbolreplacerandom\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"symbolreplaceprev\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"wordtrimright\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"wordswapprev\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"deletespaces\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplewiki-latest-pages-articles-multistream.xml.bz2\n",
      "Loading parsed documents.\n",
      "Loading preprocessed documents.\n",
      "Wikipedia loaded.\n"
     ]
    }
   ],
   "source": [
    "wikipedia = Wikipedia(\n",
    "    language=\"simple\",\n",
    "    cache_directory_url=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up simple wikipedia texts\n",
    "pattern_ignored_words = re.compile(\n",
    "    r\"\"\"\n",
    "    (?:(?:thumb|thumbnail|left|right|\\d+px|upright(?:=[0-9\\.]+)?)\\|)+\n",
    "    |^\\s*\\|.+$\n",
    "    |^REDIRECT\\b\"\"\",\n",
    "    flags=re.DOTALL | re.UNICODE | re.VERBOSE | re.MULTILINE)\n",
    "pattern_new_lines = re.compile('[\\n\\r ]+', re.UNICODE)\n",
    "texts = [wikipedia.documents[i].text for i in range(len(wikipedia.documents))]\n",
    "texts = [pattern_ignored_words.sub('', texts[i]) for i in range(len(texts))]\n",
    "texts = [pattern_new_lines.sub(' ', texts[i]) for i in range(len(texts))]\n",
    "texts = [texts[i].replace(\"\\\\\", \"\") for i in range(len(texts))]\n",
    "texts = [texts[i].replace(\"\\xa0\", \" \") for i in range(len(texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple wikipedia article texts into single sentences\n",
    "from nltk import tokenize\n",
    "sentences = []\n",
    "sentences += [tokenize.sent_tokenize(texts[i]) for i in range(len(texts))]\n",
    "#sentences += [texts[i].split(\". \") for i in range(len(texts))] #len(texts)\n",
    "# Now sentences is a list of lists. The next expression flattens it into one long list.\n",
    "sentences = [item for sublist in sentences for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(sentences[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681193\n",
      "648280\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "for i in reversed(range(len(sentences))):\n",
    "    if len(sentences[i]) < 20 or len(sentences[i]) > 100 \\\n",
    "        or sentences[i][0:9] == \"Category:\" \\\n",
    "        or sentences[i][0:13] == \"Related pages\" \\\n",
    "        or sentences[i][0:10] == \"References\" \\\n",
    "        or sentences[i][0:14] == \"Other websites\":\n",
    "        sentences.pop(i)\n",
    "print(len(sentences))\n",
    "\n",
    "#Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(sentences[530000:530300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "sentence_lengths = [len(sentences[i]) for i in range(len(sentences))]\n",
    "print(statistics.median(sentence_lengths))\n",
    "\n",
    "from collections import defaultdict\n",
    "appearances = defaultdict(int)\n",
    "\n",
    "sentence_lengths.sort()\n",
    "\n",
    "for curr in sentence_lengths:\n",
    "    appearances[curr] += 1\n",
    "    \n",
    "a = set(sentence_lengths) \n",
    "for i in a:\n",
    "    print(\"{} - {}\".format(i, appearances[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 25, 36, 27, 28, 29, 43]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabets = {'a' : 0, 'b': 1, 'c':2, 'd':3, 'e':4, 'f':5, 'g':6, 'h':7, 'i':8, 'j':9, 'k':10, 'l':11, 'm':12, 'n':13, 'o':14,\n",
    "            'p':15, 'q':16, 'r':17, 's':18, 't':19, 'u':20, 'v':21, 'w':22, 'x':23, 'y':24, 'z':25, \n",
    "            '0':26, '1':27, '2':28, '3':29, '4':30, '5':31, '6':32, '7':33, '8':34, '9':35, \n",
    "            ' ':36, ',':37, '.':38, ':':39, ';':40, '\"':41, \"'\":42, '':43, '(':44, ')':45} #43 = unknown symbol\n",
    "\n",
    "idxs = [alphabets[ch] if ch in alphabets else 43 for ch in 'az 123#']\n",
    "\n",
    "idxs\n",
    "\n",
    "one_hot = tf.one_hot(idxs, depth=len(alphabets), dtype=tf.uint8)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "one_hot.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_idxs = []\n",
    "for i in range(len(sentences)):\n",
    "    idx = []\n",
    "    for j in sentences[i]:\n",
    "        if j in alphabets:\n",
    "            idx += [alphabets[j]]\n",
    "        else:\n",
    "            idx += [43]\n",
    "    sentences_idxs.append(idx)\n",
    "    \n",
    "sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(len(sentences_idxs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'one_hot_420541:0' shape=(61, 44) dtype=uint8>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_onehot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation, Bidirectional, Reshape,Flatten, Lambda, Input, BatchNormalization\n",
    "from keras.optimizers import SGD, adam\n",
    "from keras.layers import TimeDistributed, Dropout\n",
    "from keras.activations import relu\n",
    "\n",
    "# Define CTC loss\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "\n",
    "    # hack for load_model\n",
    "    import tensorflow as tf\n",
    "\n",
    "    ''' from TF: Input requirements\n",
    "    1. sequence_length(b) <= time for all b\n",
    "    2. max(labels.indices(labels.indices[:, 1] == b, 2)) <= sequence_length(b) for all b.\n",
    "    '''\n",
    "\n",
    "    # print(\"CTC lambda inputs / shape\")\n",
    "    # print(\"y_pred:\",y_pred.shape)  # (?, 778, 30)\n",
    "    # print(\"labels:\",labels.shape)  # (?, 80)\n",
    "    # print(\"input_length:\",input_length.shape)  # (?, 1)\n",
    "    # print(\"label_length:\",label_length.shape)  # (?, 1)\n",
    "\n",
    "\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "def ds1_dropout(input_dim=26, fc_size=2048, rnn_size=512, dropout=[0.1, 0.1, 0.1], output_dim=29):\n",
    "    \"\"\" DeepSpeech 1 Implementation with Dropout\n",
    "    Architecture:\n",
    "        Input MFCC TIMEx26\n",
    "        3 Fully Connected using Clipped Relu activation function\n",
    "        3 Dropout layers between each FC\n",
    "        1 BiDirectional LSTM\n",
    "        1 Dropout applied to BLSTM\n",
    "        1 Dropout applied to FC dense\n",
    "        1 Fully connected Softmax\n",
    "    Details:\n",
    "        - Uses MFCC's rather paper's 80 linear spaced log filterbanks\n",
    "        - Uses LSTM's rather than SimpleRNN\n",
    "        - No translation of raw audio by 5ms\n",
    "        - No stride the RNN\n",
    "    Reference:\n",
    "        https://arxiv.org/abs/1412.5567\n",
    "    \"\"\"\n",
    "    from keras.utils.generic_utils import get_custom_objects\n",
    "    get_custom_objects().update({\"clipped_relu\": clipped_relu})\n",
    "    K.set_learning_phase(1)\n",
    "\n",
    "    # Creates a tensor there are usually 26 MFCC\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))  # >>(?, max_batch_seq, 26)\n",
    "\n",
    "    # First 3 FC layers\n",
    "    init = random_normal(stddev=0.046875)\n",
    "    x = TimeDistributed(Dense(fc_size, name='fc1', kernel_initializer=init, bias_initializer=init, activation=clipped_relu))(input_data)  # >>(?, 778, 2048)\n",
    "    x = TimeDistributed(Dropout(dropout[0]))(x)\n",
    "    x = TimeDistributed(Dense(fc_size, name='fc2', kernel_initializer=init, bias_initializer=init, activation=clipped_relu))(x)  # >>(?, 778, 2048)\n",
    "    x = TimeDistributed(Dropout(dropout[0]))(x)\n",
    "    x = TimeDistributed(Dense(fc_size, name='fc3', kernel_initializer=init, bias_initializer=init, activation=clipped_relu))(x)  # >>(?, 778, 2048)\n",
    "    x = TimeDistributed(Dropout(dropout[0]))(x)\n",
    "\n",
    "    # Layer 4 BiDirectional RNN\n",
    "    x = Bidirectional(LSTM(rnn_size, return_sequences=True, activation=clipped_relu, dropout=dropout[1],\n",
    "                                kernel_initializer='he_normal', name='birnn'), merge_mode='sum')(x)\n",
    "\n",
    "\n",
    "    # Layer 5+6 Time Dist Dense Layer & Softmax\n",
    "    # x = TimeDistributed(Dense(fc_size, activation=clipped_relu, kernel_initializer=init, bias_initializer=init))(x)\n",
    "    x = TimeDistributed(Dropout(dropout[2]))(x)\n",
    "    y_pred = TimeDistributed(Dense(output_dim, name=\"y_pred\", kernel_initializer=init, bias_initializer=init, activation=\"softmax\"), name=\"out\")(x)\n",
    "\n",
    "    # Change shape\n",
    "    labels = Input(name='the_labels', shape=[None,], dtype='int32')\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int32')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int32')\n",
    "\n",
    "    # Keras doesn't currently support loss funcs with extra parameters\n",
    "    # so CTC loss is implemented in a lambda layer\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred,\n",
    "                                                                       labels,\n",
    "                                                                       input_length,\n",
    "                                                                       label_length])\n",
    "\n",
    "    model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
