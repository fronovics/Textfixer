{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Deep Speech neural net on text data\n",
    "\n",
    "1. Global set up - setting up logging and root directory for including the modules, files etc.\n",
    "2. Load Simple Wikipedia, clean up the data, transform into idx-s and one-hot vectors.\n",
    "3. Initialize training and validation datasets.\n",
    "4. Run training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Global setup\n",
    "Set up logging and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from nltk import tokenize # Text to sentences\n",
    "import pandas as pd # Train and Validation data generation\n",
    "import re\n",
    "import pprint\n",
    "from src.wikipedia import Wikipedia\n",
    "#random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the Simple Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia = Wikipedia(\n",
    "    language=\"simple\",\n",
    "    cache_directory_url=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clean-up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up simple wikipedia texts\n",
    "pattern_ignored_words = re.compile(\n",
    "    r\"\"\"\n",
    "    (?:(?:thumb|thumbnail|left|right|\\d+px|upright(?:=[0-9\\.]+)?)\\|)+\n",
    "    |^\\s*\\|.+$\n",
    "    |^REDIRECT\\b\"\"\",\n",
    "    flags=re.DOTALL | re.UNICODE | re.VERBOSE | re.MULTILINE)\n",
    "pattern_new_lines = re.compile('[\\n\\r ]+', re.UNICODE)\n",
    "texts = [wikipedia.documents[i].text for i in range(len(wikipedia.documents))]\n",
    "texts = [pattern_ignored_words.sub('', texts[i]) for i in range(len(texts))]\n",
    "texts = [pattern_new_lines.sub(' ', texts[i]) for i in range(len(texts))]\n",
    "texts = [texts[i].replace(\"\\\\\", \"\") for i in range(len(texts))]\n",
    "texts = [texts[i].replace(\"\\xa0\", \" \") for i in range(len(texts))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Divide into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple wikipedia article texts into single sentences\n",
    "\n",
    "sentences = []\n",
    "sentences += [tokenize.sent_tokenize(texts[i]) for i in range(len(texts))]\n",
    "#sentences += [texts[i].split(\". \") for i## 6. Divide into sentences in range(len(texts))] #len(texts)\n",
    "# Now sentences is a list of lists. The next expression flattens it into one long list.\n",
    "sentences = [item for sublist in sentences for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(sentences[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clean-up sentences and remove too long and short ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median sentence length is 83 symbols. We remove the sentences shorter than 20 symbols and longer than 100 symbols to clean up the dataset.<br><br>\n",
    "We also remove the sentences starting with \"Category:\", \"Related pages\", \"References\", \"Other websites:\". <br>\n",
    "These are technical Wikipedia pages that we do not need. Need to check for more, e.g. \"Gallery\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences))\n",
    "for i in reversed(range(len(sentences))):\n",
    "    if len(sentences[i]) < 20 or len(sentences[i]) > 100 \\\n",
    "        or sentences[i][0:9] == \"Category:\" \\\n",
    "        or sentences[i][0:13] == \"Related pages\" \\\n",
    "        or sentences[i][0:10] == \"References\" \\\n",
    "        or sentences[i][0:14] == \"Other websites\":\n",
    "        sentences.pop(i)\n",
    "print(len(sentences))\n",
    "\n",
    "#Gallery - do something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(sentences[530000:530005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "sentence_lengths = [len(sentences[i]) for i in range(len(sentences))]\n",
    "print(statistics.median(sentence_lengths))\n",
    "\n",
    "from collections import defaultdict\n",
    "appearances = defaultdict(int)\n",
    "\n",
    "sentence_lengths.sort()\n",
    "\n",
    "for curr in sentence_lengths:\n",
    "    appearances[curr] += 1\n",
    "    \n",
    "a = set(sentence_lengths) \n",
    "for i in a:\n",
    "    print(\"{} - {}\".format(i, appearances[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate training data\n",
    "\n",
    "1. Convert sentences into IDXs (replace characters with integers).\n",
    "2. Convert IDXs into one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Generate training data\n",
    "alphabets = {'a':0, 'b':1, 'c':2, 'd':3, 'e':4, 'f':5, 'g':6, 'h':7, 'i':8, 'j':9, 'k':10, 'l':11, 'm':12, 'n':13, 'o':14,\n",
    "            'p':15, 'q':16, 'r':17, 's':18, 't':19, 'u':20, 'v':21, 'w':22, 'x':23, 'y':24, 'z':25, \n",
    "            '0':26, '1':27, '2':28, '3':29, '4':30, '5':31, '6':32, '7':33, '8':34, '9':35, \n",
    "            ' ':36, ',':37, '.':38, ':':39, ';':40, '\"':41, \"'\":42, '':43, '(':44, ')':45} #43 = unknown symbol\n",
    "\n",
    "idxs = [alphabets[ch] if ch in alphabets else 43 for ch in 'az 123#']\n",
    "\n",
    "idxs\n",
    "\n",
    "#one_hot = tf.one_hot(idxs, depth=len(alphabets), dtype=tf.uint8)\n",
    "\n",
    "#sess = tf.InteractiveSession()\n",
    "#one_hot.eval()\n",
    "one_hot = to_categorical(idxs, num_classes = len(alphabets))\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_idxs = []\n",
    "for i in range(len(sentences)):\n",
    "    idx = []\n",
    "    for j in sentences[i]:\n",
    "        if j in alphabets:\n",
    "            idx += [alphabets[j]]\n",
    "        else:\n",
    "            idx += [43]\n",
    "    sentences_idxs.append(idx)\n",
    "    \n",
    "#sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(len(sentences_idxs))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Get first 10K observations for test purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(10000)]\n",
    "sentences_onehot = [to_categorical(sentences_idxs[i], num_classes = len(alphabets)) for i in range(100000)]\n",
    "sentences = sentences[0:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data examples\n",
    "# X and Y are identical for the test purposes\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    {'X': sentences_onehot,\n",
    "     'Y': sentences\n",
    "    })\n",
    "\n",
    "print(len(sentences_onehot[100][0]))\n",
    "print(len(sentences_onehot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Initialize the DeepSpeech NN to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The original DeepSpeech paper uses the language model on top of the RNN (p. 4), see: https://arxiv.org/pdf/1412.5567.pdf</p>\n",
    "<p>I have disabled the language model in file: <i>report.py (67)</i>, because the \"KENLM\" package is hard to install on Windows. For this purpose, we need to train a new model, especially for Danish language.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "#from KerasDeepSpeech.data import combine_all_wavs_and_trans_from_csvs\n",
    "from KerasDeepSpeech.generator import BatchGenerator\n",
    "from KerasDeepSpeech.model import *\n",
    "from KerasDeepSpeech.report import ReportCallback\n",
    "from KerasDeepSpeech.utils import load_model_checkpoint, save_model, MemoryCallback\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "#######################################################\n",
    "\n",
    "# Prevent pool_allocator message\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#######################################################\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    '''\n",
    "    There are 5 simple steps to this program\n",
    "    '''\n",
    "\n",
    "    #1. combine all data into 2 dataframes (train, valid)\n",
    "    print(\"Getting data from arguments\")\n",
    "    #train_dataprops, df_train = combine_all_wavs_and_trans_from_csvs(args.train_files, sortagrad=args.sortagrad)\n",
    "    #valid_dataprops, df_valid = combine_all_wavs_and_trans_from_csvs(args.valid_files, sortagrad=args.sortagrad)\n",
    "\n",
    "    train_ratio = 0.9 #90% of data used for training, 10% for validation\n",
    "    args.model_arch = 0\n",
    "    args.opt = \"adam\"\n",
    "    args.train_steps = 0\n",
    "    args.epochs = 10\n",
    "    args.valid_steps = 0\n",
    "    args.batchsize = 32 #was 16\n",
    "    args.name = \"\"\n",
    "    args.loadcheckpointpath = \"\"\n",
    "    args.fc_size = 512\n",
    "    args.rnn_size = 512\n",
    "    args.learning_rate = 0.01\n",
    "    args.memcheck = False\n",
    "    args.tensorboard = True\n",
    "    \n",
    "    model_input_type = \"text\"\n",
    "    \n",
    "    \n",
    "    df_train = data[0:int(train_ratio * len(sentences_onehot))]\n",
    "    df_valid = data[int(train_ratio * len(sentences_onehot)):]\n",
    "\n",
    "\n",
    "    ## 2. init data generators\n",
    "    print(\"Creating data batch generators\")\n",
    "    traindata = BatchGenerator(dataframe=df_train, dataproperties=None,\n",
    "                              training=True, batch_size=args.batchsize, model_input_type=model_input_type)\n",
    "    validdata = BatchGenerator(dataframe=df_valid, dataproperties=None,\n",
    "                              training=False, batch_size=args.batchsize, model_input_type=model_input_type)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    output_dir = os.path.join('checkpoints/results',\n",
    "                                  'model%s_%s' % (args.model_arch,\n",
    "                                             args.name))\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "    ## 3. Load existing or create new model\n",
    "    if args.loadcheckpointpath:\n",
    "        # load existing\n",
    "        print(\"Loading model\")\n",
    "\n",
    "        cp = args.loadcheckpointpath\n",
    "        assert(os.path.isdir(cp))\n",
    "\n",
    "        model_path = os.path.join(cp, \"model\")\n",
    "        # assert(os.path.isfile(model_path))\n",
    "\n",
    "        model = load_model_checkpoint(model_path)\n",
    "\n",
    "\n",
    "        print(\"Model loaded\")\n",
    "    else:\n",
    "        # new model recipes here\n",
    "        print('New model DS{}'.format(args.model_arch))\n",
    "        if (args.model_arch == 0):\n",
    "            # DeepSpeech1 with Dropout\n",
    "            model = ds1_dropout(input_dim=len(alphabets), fc_size=args.fc_size, rnn_size=args.rnn_size,dropout=[0.1,0.1,0.1], output_dim=len(alphabets) + 1)\n",
    "\n",
    "        elif(args.model_arch==1):\n",
    "            # DeepSpeech1 - no dropout\n",
    "            model = ds1(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n",
    "\n",
    "        elif(args.model_arch==2):\n",
    "            # DeepSpeech2 model\n",
    "            model = ds2_gru_model(input_dim=161, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n",
    "\n",
    "        elif(args.model_arch==3):\n",
    "            # own model\n",
    "            model = ownModel(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, dropout=[0.1, 0.1, 0.1], output_dim=29)\n",
    "\n",
    "        elif(args.model_arch==4):\n",
    "            # graves model\n",
    "            model = graves(input_dim=26, rnn_size=args.rnn_size, output_dim=29, std=0.5)\n",
    "\n",
    "        elif(args.model_arch==5):\n",
    "            #cnn city\n",
    "            model = cnn_city(input_dim=161, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n",
    "\n",
    "        elif(args.model_arch == 6):\n",
    "            # constrained model\n",
    "            model = const(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n",
    "        else:\n",
    "            raise(\"model not found\")\n",
    "\n",
    "        print(model.summary(line_length=80))\n",
    "\n",
    "        #required to save the JSON\n",
    "        save_model(model, output_dir)\n",
    "\n",
    "    if (args.opt.lower() == 'sgd'):\n",
    "        opt = SGD(lr=args.learning_rate, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "    elif (args.opt.lower() == 'adam'):\n",
    "        opt = Adam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n",
    "    elif (args.opt.lower() == 'nadam'):\n",
    "        opt = Nadam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n",
    "    else:\n",
    "        raise \"optimiser not recognised\"\n",
    "\n",
    "    model.compile(optimizer=opt, loss=ctc)\n",
    "\n",
    "    ## 4. train\n",
    "\n",
    "    if args.train_steps == 0:\n",
    "        args.train_steps = len(df_train.index) // args.batchsize\n",
    "        # print(args.train_steps)\n",
    "    # we use 1/xth of the validation data at each epoch end to test val score\n",
    "    if args.valid_steps == 0:\n",
    "\n",
    "        args.valid_steps = (len(df_valid.index) // args.batchsize)\n",
    "        # print(args.valid_steps)\n",
    "\n",
    "\n",
    "    if args.memcheck:\n",
    "        cb_list = [MemoryCallback()]\n",
    "    else:\n",
    "        cb_list = []\n",
    "\n",
    "    if args.tensorboard:\n",
    "        tb_cb = TensorBoard(log_dir='./tensorboard/{}/'.format(args.name), write_graph=False, write_images=True)\n",
    "        cb_list.append(tb_cb)\n",
    "\n",
    "    y_pred = model.get_layer('ctc').input[0]\n",
    "    input_data = model.get_layer('the_input').input\n",
    "\n",
    "    report = K.function([input_data, K.learning_phase()], [y_pred])\n",
    "    report_cb = ReportCallback(report, validdata, model, args.name, save=True)\n",
    "\n",
    "    cb_list.append(report_cb)\n",
    "\n",
    "    model.fit_generator(generator=traindata.next_batch(),\n",
    "                        steps_per_epoch=args.train_steps,\n",
    "                        epochs=args.epochs,\n",
    "                        callbacks=cb_list,\n",
    "                        validation_data=validdata.next_batch(),\n",
    "                        validation_steps=args.valid_steps,\n",
    "                        initial_epoch=0,\n",
    "                        verbose=1,\n",
    "                        class_weight=None,\n",
    "                        max_q_size=10,\n",
    "                        workers=1,\n",
    "                        pickle_safe=False\n",
    "                        )\n",
    "\n",
    "    # K.clear_session()\n",
    "\n",
    "    ## These are the most important metrics\n",
    "    print(\"Mean WER   :\", report_cb.mean_wer_log)\n",
    "    print(\"Mean LER   :\", report_cb.mean_ler_log)\n",
    "    print(\"NormMeanLER:\", report_cb.norm_mean_ler_log)\n",
    "\n",
    "    # export to csv?\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
