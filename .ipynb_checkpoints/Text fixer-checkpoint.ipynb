{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Deep Speech neural net on text data\n",
    "\n",
    "1) Global set up - setting up logging and root directory for including the modules, files etc.\n",
    "2) Text distortion function to generate training data.\n",
    "3) Load Simple Wikipedia, clean up the data, transform into idx-s and one-hot vectors.\n",
    "4) Initialize training and validation datasets.\n",
    "5) Run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"global_setup.py\") as setupfile:\n",
    "        exec(setupfile.read())\n",
    "except FileNotFoundError:\n",
    "    print('Setup already completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from nltk import tokenize # Text to sentences\n",
    "import pandas as pd # Train and Validation data generation\n",
    "import re\n",
    "import pprint\n",
    "from src.wikipedia import Wikipedia\n",
    "#random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortText(text, swap_type = \"symboldelete\"):\n",
    "       \n",
    "    # Distortion types\n",
    "    distortions_symbols = [\"symbolswap\",\"symboldelete\",\"symbolreplacerandom\",\"symbolreplaceprev\"]\n",
    "    distortions_words = [\"wordtrimright\", \"wordswapprev\"]\n",
    "    distortions_sentences = [\"deletespaces\"]\n",
    "    # Symbol operations: max 2 symbols per word, max 2 symbol distortion types per word\n",
    "    # Word trim: max 1 per word\n",
    "    # Delete spaces: max all spaces, min 0 spaces\n",
    "    \n",
    "    list_words = text.split()\n",
    "    \n",
    "    distortion_prob_symbol = 0.5\n",
    "    for i in range(len(list_words)):\n",
    "        r = random.random()\n",
    "        if swap_type == \"symbolswap\":\n",
    "            # SYMBOL SWAP\n",
    "            ## min distorted symbol = 0, max distorted symbol = len-2 because of indexing with 0 and this is the 1st swappable symbol\n",
    "            distorted_symbol = round((len(list_words[i]) - 2) * random.random())\n",
    "            s1 = list_words[i][distorted_symbol + 1]\n",
    "            #print(\"{}---{}-{}-{}-{}\".format(distorted_symbol, list_words[i][0:distorted_symbol - 1], list_words[i][distorted_symbol], s1, list_words[i][distorted_symbol + 1:]))\n",
    "            #print(list_words[i][0:(distorted_symbol - 1) * (distorted_symbol > 0)])\n",
    "            list_words[i] = list_words[i][0:distorted_symbol] + s1 + list_words[i][distorted_symbol] + list_words[i][distorted_symbol + 2:]\n",
    "        elif swap_type == \"symboldelete\":\n",
    "            # SYMBOL DELETE\n",
    "            ## min distorted symbol = 0, max distorted symbol = len-1\n",
    "            distorted_symbol = round((len(list_words[i]) - 1) * random.random())\n",
    "            list_words[i] = list_words[i][0:distorted_symbol] + list_words[i][distorted_symbol + 1:]\n",
    "        elif swap_type == \"symbolreplacerandom\":\n",
    "            symbols = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "            # SYMBOL REPLACE RANDOM\n",
    "            ## min distorted symbol = 0, max distorted symbol = len-1\n",
    "            distorted_symbol = round((len(list_words[i]) - 1) * random.random())\n",
    "            replace = symbols[round((len(symbols) - 1) * random.random())]\n",
    "            list_words[i] = list_words[i][0:distorted_symbol] + replace + list_words[i][distorted_symbol + 1:]\n",
    "        elif swap_type == \"symbolreplaceprev\":\n",
    "            # SYMBOL REPLACE PREVIOUS\n",
    "            ## min distorted symbol = 1, max distorted symbol = len - 1\n",
    "            distorted_symbol = round(1 + (len(list_words[i]) - 2) * random.random())\n",
    "            #print(\"{} - {}\".format(distorted_symbol, list_words[i]))\n",
    "            #print(\"{}---{}-{}-{}\".format(distorted_symbol, list_words[i][0:(distorted_symbol)], list_words[i][distorted_symbol - 1], list_words[i][distorted_symbol + 1 * (len(list_words[i]) > distorted_symbol):distorted_symbol + 1 * (len(list_words[i]) > distorted_symbol)]))\n",
    "            list_words[i] = list_words[i][0:(distorted_symbol)] + list_words[i][distorted_symbol - 1] + list_words[i][distorted_symbol + 1:]\n",
    "        \n",
    "        if swap_type == \"wordtrimright\":\n",
    "            # WORD TRIM RIGHT\n",
    "            ## min symbol = 1, max distorted symbol = len - 1\n",
    "            trim_from = round(1 + (len(list_words[i]) - 1) * random.random())\n",
    "            list_words[i] = list_words[i][:trim_from]\n",
    "    \n",
    "    if swap_type == \"wordswapprev\":\n",
    "        # WORD SWAP\n",
    "        word_id = round((len(list_words) - 2) * random.random())\n",
    "        s1 = list_words[word_id]\n",
    "        list_words[word_id] = list_words[word_id + 1]\n",
    "        list_words[word_id + 1] = s1\n",
    "        \n",
    "    if swap_type == \"deletespaces\":\n",
    "        # DELETE WHITESPACES\n",
    "        ## -2 because there are len - 1 spaces in total\n",
    "        space_id = round((len(list_words) - 2) * random.random())\n",
    "        list_words[space_id] = list_words[space_id] + list_words[space_id + 1]\n",
    "        list_words.pop(space_id + 1)\n",
    "            \n",
    "    distortedText = \" \".join(list_words)\n",
    "    return distortedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Antons likes pizza\"\n",
    "print(distortText(text=text,swap_type=\"symbolswap\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"symboldelete\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"symbolreplacerandom\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"symbolreplaceprev\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"wordtrimright\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"wordswapprev\"))\n",
    "\n",
    "print(distortText(text=text,swap_type=\"deletespaces\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia = Wikipedia(\n",
    "    language=\"simple\",\n",
    "    cache_directory_url=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up simple wikipedia texts\n",
    "pattern_ignored_words = re.compile(\n",
    "    r\"\"\"\n",
    "    (?:(?:thumb|thumbnail|left|right|\\d+px|upright(?:=[0-9\\.]+)?)\\|)+\n",
    "    |^\\s*\\|.+$\n",
    "    |^REDIRECT\\b\"\"\",\n",
    "    flags=re.DOTALL | re.UNICODE | re.VERBOSE | re.MULTILINE)\n",
    "pattern_new_lines = re.compile('[\\n\\r ]+', re.UNICODE)\n",
    "texts = [wikipedia.documents[i].text for i in range(len(wikipedia.documents))]\n",
    "texts = [pattern_ignored_words.sub('', texts[i]) for i in range(len(texts))]\n",
    "texts = [pattern_new_lines.sub(' ', texts[i]) for i in range(len(texts))]\n",
    "texts = [texts[i].replace(\"\\\\\", \"\") for i in range(len(texts))]\n",
    "texts = [texts[i].replace(\"\\xa0\", \" \") for i in range(len(texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple wikipedia article texts into single sentences\n",
    "\n",
    "sentences = []\n",
    "sentences += [tokenize.sent_tokenize(texts[i]) for i in range(len(texts))]\n",
    "#sentences += [texts[i].split(\". \") for i in range(len(texts))] #len(texts)\n",
    "# Now sentences is a list of lists. The next expression flattens it into one long list.\n",
    "sentences = [item for sublist in sentences for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(sentences[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences))\n",
    "for i in reversed(range(len(sentences))):\n",
    "    if len(sentences[i]) < 20 or len(sentences[i]) > 100 \\\n",
    "        or sentences[i][0:9] == \"Category:\" \\\n",
    "        or sentences[i][0:13] == \"Related pages\" \\\n",
    "        or sentences[i][0:10] == \"References\" \\\n",
    "        or sentences[i][0:14] == \"Other websites\":\n",
    "        sentences.pop(i)\n",
    "print(len(sentences))\n",
    "\n",
    "#Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(sentences[530000:530005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "sentence_lengths = [len(sentences[i]) for i in range(len(sentences))]\n",
    "print(statistics.median(sentence_lengths))\n",
    "\n",
    "from collections import defaultdict\n",
    "appearances = defaultdict(int)\n",
    "\n",
    "sentence_lengths.sort()\n",
    "\n",
    "for curr in sentence_lengths:\n",
    "    appearances[curr] += 1\n",
    "    \n",
    "a = set(sentence_lengths) \n",
    "for i in a:\n",
    "    print(\"{} - {}\".format(i, appearances[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = {'a':0, 'b':1, 'c':2, 'd':3, 'e':4, 'f':5, 'g':6, 'h':7, 'i':8, 'j':9, 'k':10, 'l':11, 'm':12, 'n':13, 'o':14,\n",
    "            'p':15, 'q':16, 'r':17, 's':18, 't':19, 'u':20, 'v':21, 'w':22, 'x':23, 'y':24, 'z':25, \n",
    "            '0':26, '1':27, '2':28, '3':29, '4':30, '5':31, '6':32, '7':33, '8':34, '9':35, \n",
    "            ' ':36, ',':37, '.':38, ':':39, ';':40, '\"':41, \"'\":42, '':43, '(':44, ')':45} #43 = unknown symbol\n",
    "\n",
    "idxs = [alphabets[ch] if ch in alphabets else 43 for ch in 'az 123#']\n",
    "\n",
    "idxs\n",
    "\n",
    "#one_hot = tf.one_hot(idxs, depth=len(alphabets), dtype=tf.uint8)\n",
    "\n",
    "#sess = tf.InteractiveSession()\n",
    "#one_hot.eval()\n",
    "one_hot = to_categorical(idxs, num_classes = len(alphabets))\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_idxs = []\n",
    "for i in range(len(sentences)):\n",
    "    idx = []\n",
    "    for j in sentences[i]:\n",
    "        if j in alphabets:\n",
    "            idx += [alphabets[j]]\n",
    "        else:\n",
    "            idx += [43]\n",
    "    sentences_idxs.append(idx)\n",
    "    \n",
    "#sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(len(sentences_idxs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(10000)]\n",
    "sentences_onehot = [to_categorical(sentences_idxs[i], num_classes = len(alphabets)) for i in range(10000)]\n",
    "sentences = sentences[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data examples\n",
    "# X and Y are identical for the test purposes\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    {'X': sentences_onehot,\n",
    "     'Y': sentences\n",
    "    })\n",
    "\n",
    "print(len(sentences_onehot[100][0]))\n",
    "print(len(sentences_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "#from KerasDeepSpeech.data import combine_all_wavs_and_trans_from_csvs\n",
    "from KerasDeepSpeech.generator import BatchGenerator\n",
    "from KerasDeepSpeech.model import *\n",
    "from KerasDeepSpeech.report import ReportCallback\n",
    "from KerasDeepSpeech.utils import load_model_checkpoint, save_model, MemoryCallback\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "#######################################################\n",
    "\n",
    "# Prevent pool_allocator message\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#######################################################\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    '''\n",
    "    There are 5 simple steps to this program\n",
    "    '''\n",
    "\n",
    "    #1. combine all data into 2 dataframes (train, valid)\n",
    "    print(\"Getting data from arguments\")\n",
    "    #train_dataprops, df_train = combine_all_wavs_and_trans_from_csvs(args.train_files, sortagrad=args.sortagrad)\n",
    "    #valid_dataprops, df_valid = combine_all_wavs_and_trans_from_csvs(args.valid_files, sortagrad=args.sortagrad)\n",
    "\n",
    "    train_ratio = 0.9\n",
    "    args.model_arch = 0\n",
    "    args.opt = \"adam\"\n",
    "    args.train_steps = 0\n",
    "    args.epochs = 10\n",
    "    args.valid_steps = 0\n",
    "    args.batchsize = 32 #was 16\n",
    "    args.name = \"\"\n",
    "    args.loadcheckpointpath = \"\"\n",
    "    args.fc_size = 512\n",
    "    args.rnn_size = 512\n",
    "    args.learning_rate = 0.01\n",
    "    args.memcheck = False\n",
    "    args.tensorboard = True\n",
    "    \n",
    "    model_input_type = \"text\"\n",
    "    \n",
    "    \n",
    "    df_train = data[0:int(train_ratio * len(sentences_onehot))]\n",
    "    df_valid = data[int(train_ratio * len(sentences_onehot)) + 1:]\n",
    "\n",
    "\n",
    "    ## 2. init data generators\n",
    "    print(\"Creating data batch generators\")\n",
    "    traindata = BatchGenerator(dataframe=df_train, dataproperties=None,\n",
    "                              training=True, batch_size=args.batchsize, model_input_type=model_input_type)\n",
    "    validdata = BatchGenerator(dataframe=df_valid, dataproperties=None,\n",
    "                              training=False, batch_size=args.batchsize, model_input_type=model_input_type)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    output_dir = os.path.join('checkpoints/results',\n",
    "                                  'model%s_%s' % (args.model_arch,\n",
    "                                             args.name))\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "    ## 3. Load existing or create new model\n",
    "    if args.loadcheckpointpath:\n",
    "        # load existing\n",
    "        print(\"Loading model\")\n",
    "\n",
    "        cp = args.loadcheckpointpath\n",
    "        assert(os.path.isdir(cp))\n",
    "\n",
    "        model_path = os.path.join(cp, \"model\")\n",
    "        # assert(os.path.isfile(model_path))\n",
    "\n",
    "        model = load_model_checkpoint(model_path)\n",
    "\n",
    "\n",
    "        print(\"Model loaded\")\n",
    "    else:\n",
    "        # new model recipes here\n",
    "        print('New model DS{}'.format(args.model_arch))\n",
    "        if (args.model_arch == 0):\n",
    "            # DeepSpeech1 with Dropout\n",
    "            model = ds1_dropout(input_dim=len(alphabets), fc_size=args.fc_size, rnn_size=args.rnn_size,dropout=[0.1,0.1,0.1], output_dim=len(alphabets) + 1)\n",
    "\n",
    "        elif(args.model_arch==1):\n",
    "            # DeepSpeech1 - no dropout\n",
    "            model = ds1(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n",
    "\n",
    "        elif(args.model_arch==2):\n",
    "            # DeepSpeech2 model\n",
    "            model = ds2_gru_model(input_dim=161, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n",
    "\n",
    "        elif(args.model_arch==3):\n",
    "            # own model\n",
    "            model = ownModel(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, dropout=[0.1, 0.1, 0.1], output_dim=29)\n",
    "\n",
    "        elif(args.model_arch==4):\n",
    "            # graves model\n",
    "            model = graves(input_dim=26, rnn_size=args.rnn_size, output_dim=29, std=0.5)\n",
    "\n",
    "        elif(args.model_arch==5):\n",
    "            #cnn city\n",
    "            model = cnn_city(input_dim=161, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n",
    "\n",
    "        elif(args.model_arch == 6):\n",
    "            # constrained model\n",
    "            model = const(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n",
    "        else:\n",
    "            raise(\"model not found\")\n",
    "\n",
    "        print(model.summary(line_length=80))\n",
    "\n",
    "        #required to save the JSON\n",
    "        save_model(model, output_dir)\n",
    "\n",
    "    if (args.opt.lower() == 'sgd'):\n",
    "        opt = SGD(lr=args.learning_rate, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "    elif (args.opt.lower() == 'adam'):\n",
    "        opt = Adam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n",
    "    elif (args.opt.lower() == 'nadam'):\n",
    "        opt = Nadam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n",
    "    else:\n",
    "        raise \"optimiser not recognised\"\n",
    "\n",
    "    model.compile(optimizer=opt, loss=ctc)\n",
    "\n",
    "    ## 4. train\n",
    "\n",
    "    if args.train_steps == 0:\n",
    "        args.train_steps = len(df_train.index) // args.batchsize\n",
    "        # print(args.train_steps)\n",
    "    # we use 1/xth of the validation data at each epoch end to test val score\n",
    "    if args.valid_steps == 0:\n",
    "\n",
    "        args.valid_steps = (len(df_valid.index) // args.batchsize)\n",
    "        # print(args.valid_steps)\n",
    "\n",
    "\n",
    "    if args.memcheck:\n",
    "        cb_list = [MemoryCallback()]\n",
    "    else:\n",
    "        cb_list = []\n",
    "\n",
    "    if args.tensorboard:\n",
    "        tb_cb = TensorBoard(log_dir='./tensorboard/{}/'.format(args.name), write_graph=False, write_images=True)\n",
    "        cb_list.append(tb_cb)\n",
    "\n",
    "    y_pred = model.get_layer('ctc').input[0]\n",
    "    input_data = model.get_layer('the_input').input\n",
    "\n",
    "    report = K.function([input_data, K.learning_phase()], [y_pred])\n",
    "    report_cb = ReportCallback(report, validdata, model, args.name, save=True)\n",
    "\n",
    "    cb_list.append(report_cb)\n",
    "\n",
    "    model.fit_generator(generator=traindata.next_batch(),\n",
    "                        steps_per_epoch=args.train_steps,\n",
    "                        epochs=args.epochs,\n",
    "                        callbacks=cb_list,\n",
    "                        validation_data=validdata.next_batch(),\n",
    "                        validation_steps=args.valid_steps,\n",
    "                        initial_epoch=0,\n",
    "                        verbose=1,\n",
    "                        class_weight=None,\n",
    "                        max_q_size=10,\n",
    "                        workers=1,\n",
    "                        pickle_safe=False\n",
    "                        )\n",
    "\n",
    "    # K.clear_session()\n",
    "\n",
    "    ## These are the most important metrics\n",
    "    print(\"Mean WER   :\", report_cb.mean_wer_log)\n",
    "    print(\"Mean LER   :\", report_cb.mean_ler_log)\n",
    "    print(\"NormMeanLER:\", report_cb.norm_mean_ler_log)\n",
    "\n",
    "    # export to csv?\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
