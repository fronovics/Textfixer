2018-12-03 09:49:11,980 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-3-07f851602e1f> in <module>()
      1 import random
----> 2 seed(12345)

NameError: name 'seed' is not defined
2018-12-03 09:49:23,981 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-6-ca8b98d1fe66> in <module>()
      1 text = "Antons likes pizza"
----> 2 print(distortText(text=text))

<ipython-input-4-315315263b06> in distortText(text, d_whitespaces)
     17             distorted_symbol = round((len(list_words[i]) - 1) * random.random())
     18             s1 = list_words[i][distorted_symbol - 1]
---> 19             list_words[i][distorted_symbol - 1] = list_words[i][distorted_symbol]
     20             list_words[i][distorted_symbol] = s1
     21     

TypeError: 'str' object does not support item assignment
2018-12-03 10:11:00,660 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-38-faba5e5ab314>", line 22
    else if swap_type == "symboldelete":
          ^
SyntaxError: invalid syntax

2018-12-03 10:19:00,909 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-52-358794607166>", line 23
    + list_words[i][distorted_symbol]
    ^
IndentationError: unexpected indent

2018-12-03 10:19:17,731 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-53-024b169880a1>", line 22
    list_words[i] = list_words[i][0:(distorted_symbol - 1) * (distorted_symbol > 0)] \                          + list_words[i][distorted_symbol]
                                                                                                                                                  ^
SyntaxError: unexpected character after line continuation character

2018-12-03 10:19:27,101 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-54-4f2a77b36d3a>", line 23
    + s1
    ^
IndentationError: unexpected indent

2018-12-03 10:37:01,823 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-108-30bb955ad8f5> in <module>()
      1 text = "Antons likes pizza"
----> 2 print(distortText(text=text,swap_type="symbolreplaceprev"))

<ipython-input-107-a5189cd3faab> in distortText(text, swap_type)
     38             ## min distorted symbol = 1, max distorted symbol = len
     39             distorted_symbol = round(1 + len(list_words[i]) * random.random())
---> 40             list_words[i] = list_words[i][0:(distorted_symbol - 1)] + list_words[i][distorted_symbol] + list_words[i][distorted_symbol + 1:]
     41             
     42     distortedText = " ".join(list_words)

IndexError: string index out of range
2018-12-03 10:41:16,794 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-117-30bb955ad8f5> in <module>()
      1 text = "Antons likes pizza"
----> 2 print(distortText(text=text,swap_type="symbolreplaceprev"))

<ipython-input-116-71dd2168a0d4> in distortText(text, swap_type)
     39             distorted_symbol = round(1 + len(list_words[i]) * random.random())
     40             print(distorted_symbol)
---> 41             print("{}---{}-{}-{}".format(distorted_symbol, list_words[i][0:(distorted_symbol)], list_words[i][distorted_symbol - 1], list_words[i][distorted_symbol + 1:]))
     42             list_words[i] = list_words[i][0:(distorted_symbol)] + list_words[i][distorted_symbol - 1] + list_words[i][distorted_symbol + 1:]
     43             

IndexError: string index out of range
2018-12-03 10:43:18,487 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-124-30bb955ad8f5> in <module>()
      1 text = "Antons likes pizza"
----> 2 print(distortText(text=text,swap_type="symbolreplaceprev"))

<ipython-input-122-1017b4399059> in distortText(text, swap_type)
     38             ## min distorted symbol = 1, max distorted symbol = len
     39             distorted_symbol = round(1 + len(list_words[i]) * random.random())
---> 40             print("{}---{}-{}-{}".format(distorted_symbol, list_words[i][0:(distorted_symbol)], list_words[i][distorted_symbol - 1], list_words[i][distorted_symbol + 1:]))
     41             list_words[i] = list_words[i][0:(distorted_symbol)] + list_words[i][distorted_symbol - 1] + list_words[i][distorted_symbol + 1:]
     42             

IndexError: string index out of range
2018-12-03 10:44:29,212 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-128-30bb955ad8f5> in <module>()
      1 text = "Antons likes pizza"
----> 2 print(distortText(text=text,swap_type="symbolreplaceprev"))

<ipython-input-125-8442fe1cf01d> in distortText(text, swap_type)
     38             ## min distorted symbol = 1, max distorted symbol = len
     39             distorted_symbol = round(1 + len(list_words[i]) * random.random())
---> 40             print("{}---{}-{}-{}".format(distorted_symbol, list_words[i][0:(distorted_symbol)], list_words[i][distorted_symbol - 1], list_words[i][distorted_symbol + 1 * (len(list_words[i]) > distorted_symbol):]))
     41             list_words[i] = list_words[i][0:(distorted_symbol)] + list_words[i][distorted_symbol - 1] + list_words[i][distorted_symbol + 1:]
     42             

IndexError: string index out of range
2018-12-03 10:45:15,251 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-130-30bb955ad8f5> in <module>()
      1 text = "Antons likes pizza"
----> 2 print(distortText(text=text,swap_type="symbolreplaceprev"))

<ipython-input-129-a4e7b9c1bc0a> in distortText(text, swap_type)
     39             distorted_symbol = round(1 + len(list_words[i]) * random.random())
     40             print("{} - {}".format(distorted_symbol, list_words[i]))
---> 41             print("{}---{}-{}-{}".format(distorted_symbol, list_words[i][0:(distorted_symbol)], list_words[i][distorted_symbol - 1], list_words[i][distorted_symbol + 1 * (len(list_words[i]) > distorted_symbol):]))
     42             list_words[i] = list_words[i][0:(distorted_symbol)] + list_words[i][distorted_symbol - 1] + list_words[i][distorted_symbol + 1:]
     43             

IndexError: string index out of range
2018-12-04 17:15:14,531 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-3-72f1f9fafc96>", line 55
    distortedText = " ".join(list_words)
    ^
IndentationError: expected an indented block

2018-12-04 17:15:14,838 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-4-bbaf1578fe9c> in <module>()
      1 text = "Antons likes pizza"
----> 2 print(distortText(text=text,swap_type="symbolswap"))
      3 
      4 print(distortText(text=text,swap_type="symboldelete"))
      5 

NameError: name 'distortText' is not defined
2018-12-04 17:15:39,261 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-7-72f1f9fafc96>", line 55
    distortedText = " ".join(list_words)
    ^
IndentationError: expected an indented block

2018-12-07 10:38:09,310 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
<ipython-input-13-07f62de81b22> in <module>()
      1 wikipedia = Wikipedia(
      2     language="English",
----> 3     cache_directory_url=False
      4 )

~\Documents\GitHub\Textfixer\src\wikipedia.py in __init__(self, language, cache_directory_url, maximum_number_of_documents)
    152                 if not self.__path.exists():
    153                     print("Downloading Wikipedia documents.")
--> 154                     self._download_wikipedia()
    155                 self._parse_documents()
    156                 self._vectorise_documents()

~\Documents\GitHub\Textfixer\src\wikipedia.py in _download_wikipedia(self)
    291         retrieve_file(
    292             url=self.__dump_url,
--> 293             path=self.__path
    294         )
    295 

~\Documents\GitHub\Textfixer\src\utility\connectivity.py in retrieve_file(url, path, title)
    109     ensure_directory(path.parent)
    110 
--> 111     site = urllib.request.urlopen(url)
    112     meta = site.info()
    113     total_size = int(meta["Content-Length"])

~\AppData\Local\Continuum\anaconda3\lib\urllib\request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context)
    221     else:
    222         opener = _opener
--> 223     return opener.open(url, data, timeout)
    224 
    225 def install_opener(opener):

~\AppData\Local\Continuum\anaconda3\lib\urllib\request.py in open(self, fullurl, data, timeout)
    530         for processor in self.process_response.get(protocol, []):
    531             meth = getattr(processor, meth_name)
--> 532             response = meth(req, response)
    533 
    534         return response

~\AppData\Local\Continuum\anaconda3\lib\urllib\request.py in http_response(self, request, response)
    640         if not (200 <= code < 300):
    641             response = self.parent.error(
--> 642                 'http', request, response, code, msg, hdrs)
    643 
    644         return response

~\AppData\Local\Continuum\anaconda3\lib\urllib\request.py in error(self, proto, *args)
    568         if http_err:
    569             args = (dict, 'default', 'http_error_default') + orig_args
--> 570             return self._call_chain(*args)
    571 
    572 # XXX probably also want an abstract factory that knows when it makes

~\AppData\Local\Continuum\anaconda3\lib\urllib\request.py in _call_chain(self, chain, kind, meth_name, *args)
    502         for handler in handlers:
    503             func = getattr(handler, meth_name)
--> 504             result = func(*args)
    505             if result is not None:
    506                 return result

~\AppData\Local\Continuum\anaconda3\lib\urllib\request.py in http_error_default(self, req, fp, code, msg, hdrs)
    648 class HTTPDefaultErrorHandler(BaseHandler):
    649     def http_error_default(self, req, fp, code, msg, hdrs):
--> 650         raise HTTPError(req.full_url, code, msg, hdrs, fp)
    651 
    652 class HTTPRedirectHandler(BaseHandler):

HTTPError: HTTP Error 404: Not Found
2018-12-07 10:40:17,557 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
<ipython-input-3-07f62de81b22> in <module>()
      1 wikipedia = Wikipedia(
      2     language="English",
----> 3     cache_directory_url=False
      4 )

~\Documents\GitHub\Textfixer\src\wikipedia.py in __init__(self, language, cache_directory_url, maximum_number_of_documents)
    153                 if not self.__path.exists():
    154                     print("Downloading Wikipedia documents.")
--> 155                     self._download_wikipedia()
    156                 self._parse_documents()
    157                 self._vectorise_documents()

~\Documents\GitHub\Textfixer\src\wikipedia.py in _download_wikipedia(self)
    292         retrieve_file(
    293             url=self.__dump_url,
--> 294             path=self.__path
    295         )
    296 

~\Documents\GitHub\Textfixer\src\utility\connectivity.py in retrieve_file(url, path, title)
    109     ensure_directory(path.parent)
    110 
--> 111     site = urllib.request.urlopen(url)
    112     meta = site.info()
    113     total_size = int(meta["Content-Length"])

~\AppData\Local\Continuum\anaconda3\lib\urllib\request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context)
    221     else:
    222         opener = _opener
--> 223     return opener.open(url, data, timeout)
    224 
    225 def install_opener(opener):

~\AppData\Local\Continuum\anaconda3\lib\urllib\request.py in open(self, fullurl, data, timeout)
    530         for processor in self.process_response.get(protocol, []):
    531             meth = getattr(processor, meth_name)
--> 532             response = meth(req, response)
    533 
    534         return response

~\AppData\Local\Continuum\anaconda3\lib\urllib\request.py in http_response(self, request, response)
    640         if not (200 <= code < 300):
    641             response = self.parent.error(
--> 642                 'http', request, response, code, msg, hdrs)
    643 
    644         return response

~\AppData\Local\Continuum\anaconda3\lib\urllib\request.py in error(self, proto, *args)
    568         if http_err:
    569             args = (dict, 'default', 'http_error_default') + orig_args
--> 570             return self._call_chain(*args)
    571 
    572 # XXX probably also want an abstract factory that knows when it makes

~\AppData\Local\Continuum\anaconda3\lib\urllib\request.py in _call_chain(self, chain, kind, meth_name, *args)
    502         for handler in handlers:
    503             func = getattr(handler, meth_name)
--> 504             result = func(*args)
    505             if result is not None:
    506                 return result

~\AppData\Local\Continuum\anaconda3\lib\urllib\request.py in http_error_default(self, req, fp, code, msg, hdrs)
    648 class HTTPDefaultErrorHandler(BaseHandler):
    649     def http_error_default(self, req, fp, code, msg, hdrs):
--> 650         raise HTTPError(req.full_url, code, msg, hdrs, fp)
    651 
    652 class HTTPRedirectHandler(BaseHandler):

HTTPError: HTTP Error 404: Not Found
2018-12-07 10:43:14,795 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
LookupError                               Traceback (most recent call last)
<ipython-input-3-e9a5803aa41e> in <module>()
      1 wikipedia = Wikipedia(
      2     language="simple",
----> 3     cache_directory_url=False
      4 )

~\Documents\GitHub\Textfixer\src\wikipedia.py in __init__(self, language, cache_directory_url, maximum_number_of_documents)
     64         self.documents = None  # type: list[WikipediaDocument]
     65 
---> 66         self.__language_code = pycountry.languages.lookup(language).alpha_2
     67         self.__maximum_number_of_documents = maximum_number_of_documents
     68 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pycountry\db.py in load_if_needed(self, *args, **kw)
     43             with self._load_lock:
     44                 self._load()
---> 45         return f(self, *args, **kw)
     46     return load_if_needed
     47 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pycountry\db.py in lookup(self, value)
    126                 if v.lower() == value:
    127                     return candidate
--> 128         raise LookupError('Could not find a record for %r' % value)

LookupError: Could not find a record for 'simple'
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\client\session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).
  warnings.warn('An interactive session is already active. This can '
2018-12-07 14:46:49,746 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-14-93802338607a>", line 1
    wikipedia.documents[0.text
                             ^
SyntaxError: invalid syntax

2018-12-07 14:48:36,864 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-16-0e0307e3fdd1> in <module>()
----> 1 pattern = re.compile('[\n\r ]+', re.UNICODE)
      2 texts = [wikipedia.documents[i].text for i in range(len(wikipedia.documents))]
      3 texts = [pattern.sub(' ', texts[i]) for i in range(len(texts))]
      4 wikipedia.documents[0].text

NameError: name 're' is not defined
2018-12-07 15:10:53,460 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
error                                     Traceback (most recent call last)
<ipython-input-28-4def7b2fc7f9> in <module>()
      6     flags=re.DOTALL | re.UNICODE | re.VERBOSE | re.MULTILINE)
      7 pattern_new_lines = re.compile('[\n\r ]+', re.UNICODE)
----> 8 pattern_slashes = re.compile('[\]+', re.UNICODE)
      9 texts = [wikipedia.documents[i].text for i in range(len(wikipedia.documents))]
     10 texts = [pattern_ignored_words.sub('', texts[i]) for i in range(len(texts))]

~\AppData\Local\Continuum\anaconda3\lib\re.py in compile(pattern, flags)
    231 def compile(pattern, flags=0):
    232     "Compile a regular expression pattern, returning a pattern object."
--> 233     return _compile(pattern, flags)
    234 
    235 def purge():

~\AppData\Local\Continuum\anaconda3\lib\re.py in _compile(pattern, flags)
    299     if not sre_compile.isstring(pattern):
    300         raise TypeError("first argument must be string or compiled pattern")
--> 301     p = sre_compile.compile(pattern, flags)
    302     if not (flags & DEBUG):
    303         if len(_cache) >= _MAXCACHE:

~\AppData\Local\Continuum\anaconda3\lib\sre_compile.py in compile(p, flags)
    560     if isstring(p):
    561         pattern = p
--> 562         p = sre_parse.parse(p, flags)
    563     else:
    564         pattern = None

~\AppData\Local\Continuum\anaconda3\lib\sre_parse.py in parse(str, flags, pattern)
    853 
    854     try:
--> 855         p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)
    856     except Verbose:
    857         # the VERBOSE flag was switched on inside the pattern.  to be

~\AppData\Local\Continuum\anaconda3\lib\sre_parse.py in _parse_sub(source, state, verbose, nested)
    414     while True:
    415         itemsappend(_parse(source, state, verbose, nested + 1,
--> 416                            not nested and not items))
    417         if not sourcematch("|"):
    418             break

~\AppData\Local\Continuum\anaconda3\lib\sre_parse.py in _parse(source, state, verbose, nested, first)
    521                 if this is None:
    522                     raise source.error("unterminated character set",
--> 523                                        source.tell() - here)
    524                 if this == "]" and set != start:
    525                     break

error: unterminated character set at position 0
2018-12-07 15:11:06,380 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
error                                     Traceback (most recent call last)
<ipython-input-29-91e315871e8e> in <module>()
      6     flags=re.DOTALL | re.UNICODE | re.VERBOSE | re.MULTILINE)
      7 pattern_new_lines = re.compile('[\n\r ]+', re.UNICODE)
----> 8 pattern_slashes = re.compile('[\\]+', re.UNICODE)
      9 texts = [wikipedia.documents[i].text for i in range(len(wikipedia.documents))]
     10 texts = [pattern_ignored_words.sub('', texts[i]) for i in range(len(texts))]

~\AppData\Local\Continuum\anaconda3\lib\re.py in compile(pattern, flags)
    231 def compile(pattern, flags=0):
    232     "Compile a regular expression pattern, returning a pattern object."
--> 233     return _compile(pattern, flags)
    234 
    235 def purge():

~\AppData\Local\Continuum\anaconda3\lib\re.py in _compile(pattern, flags)
    299     if not sre_compile.isstring(pattern):
    300         raise TypeError("first argument must be string or compiled pattern")
--> 301     p = sre_compile.compile(pattern, flags)
    302     if not (flags & DEBUG):
    303         if len(_cache) >= _MAXCACHE:

~\AppData\Local\Continuum\anaconda3\lib\sre_compile.py in compile(p, flags)
    560     if isstring(p):
    561         pattern = p
--> 562         p = sre_parse.parse(p, flags)
    563     else:
    564         pattern = None

~\AppData\Local\Continuum\anaconda3\lib\sre_parse.py in parse(str, flags, pattern)
    853 
    854     try:
--> 855         p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)
    856     except Verbose:
    857         # the VERBOSE flag was switched on inside the pattern.  to be

~\AppData\Local\Continuum\anaconda3\lib\sre_parse.py in _parse_sub(source, state, verbose, nested)
    414     while True:
    415         itemsappend(_parse(source, state, verbose, nested + 1,
--> 416                            not nested and not items))
    417         if not sourcematch("|"):
    418             break

~\AppData\Local\Continuum\anaconda3\lib\sre_parse.py in _parse(source, state, verbose, nested, first)
    521                 if this is None:
    522                     raise source.error("unterminated character set",
--> 523                                        source.tell() - here)
    524                 if this == "]" and set != start:
    525                     break

error: unterminated character set at position 0
2018-12-07 15:35:31,712 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-48-6b85796ad33d> in <module>()
      4             ' ':36, ',':37, '.':38, ':':39, ';':40, '"':41, "'":42}
      5 
----> 6 idxs = [alphabets[ch] for ch in 'az 123#']
      7 
      8 idxs

<ipython-input-48-6b85796ad33d> in <listcomp>(.0)
      4             ' ':36, ',':37, '.':38, ':':39, ';':40, '"':41, "'":42}
      5 
----> 6 idxs = [alphabets[ch] for ch in 'az 123#']
      7 
      8 idxs

KeyError: '#'
2018-12-07 15:35:48,301 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-49-cbb4617f1776>", line 6
    idxs = [alphabets[ch] for ch in 'az 123#' else '']
                                                 ^
SyntaxError: invalid syntax

2018-12-07 15:35:55,127 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-50-da2f83575bb9>", line 6
    idxs = [alphabets[ch] for ch if in 'az 123#' else '']
                                  ^
SyntaxError: invalid syntax

2018-12-10 10:18:29,187 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-9-f1157d5a70b6> in <module>()
      1 # Simple wikipedia article texts into single sentences
----> 2 sentences += [texts[i].split(". ") for i in range(len(texts))]

NameError: name 'sentences' is not defined
2018-12-10 10:18:29,191 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-10 10:18:29,191 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'a80a12705e774615b72abeb9ed6fe641', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 10, 9, 18, 17, 940727, tzinfo=tzutc())}, 'msg_id': 'a80a12705e774615b72abeb9ed6fe641', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'print(sentences[10000:10020])', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-10 10:29:10,753 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-24-7906e2bca1e0> in <module>()
----> 1 print(sentences[1]) #[10000:10003]

IndexError: list index out of range
2018-12-10 10:32:04,631 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-37-8ec4d2ac4ac4>", line 3
    sentences += [texts[i].split(". ")[] for i in range(2)] #len(texts)
                                       ^
SyntaxError: invalid syntax

2018-12-10 10:39:18,417 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-48-4b1ce954a603> in <module>()
      1 print(sentences) #[10000:10003]
----> 2 print(flat_list)

NameError: name 'flat_list' is not defined
2018-12-10 10:46:08,655 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-65-352080607cf0>", line 3
    s1 += [item if item != "" for item in sentences]
                                ^
SyntaxError: invalid syntax

2018-12-10 10:46:51,053 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-66-ace2dcb3b155>", line 3
    s1 += [item if item != "" else continue for item in sentences]
                                          ^
SyntaxError: invalid syntax

2018-12-10 10:47:27,715 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-67-352080607cf0>", line 3
    s1 += [item if item != "" for item in sentences]
                                ^
SyntaxError: invalid syntax

2018-12-10 10:49:53,911 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-71-6ccfe856b0dd>", line 3
    s1 += [item if item != "" else continue for item in sentences]
                                          ^
SyntaxError: invalid syntax

2018-12-10 10:50:44,592 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-72-21820864a1a3>", line 3
    s1 += [item if item != "" else pass for item in sentences]
                                      ^
SyntaxError: invalid syntax

2018-12-10 11:04:13,434 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-114-47905bbc940d> in <module>()
----> 1 statistics.median(sentence_lengths[0:10000])

NameError: name 'statistics' is not defined
2018-12-10 11:04:17,206 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-115-f921ee793ef7> in <module>()
----> 1 median(sentence_lengths[0:10000])

NameError: name 'median' is not defined
2018-12-10 11:07:09,375 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-118-f528281fa7ab> in <module>()
      1 import collections
      2 counter=collections.Counter(sentence_lengths)
----> 3 counter()

TypeError: 'Counter' object is not callable
2018-12-10 11:08:42,887 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-120-0df212862c23> in <module>()
----> 1 a = set(a) 

NameError: name 'a' is not defined
2018-12-10 11:12:04,440 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
<ipython-input-124-6fd9a13b11ba> in <module>()
      1 for i in sentence_lengths:
----> 2     print("{} - {}".format(i, appearances[i]))

~\AppData\Local\Continuum\anaconda3\lib\site-packages\colorama\ansitowin32.py in write(self, text)
     38 
     39     def write(self, text):
---> 40         self.__convertor.write(text)
     41 
     42 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\colorama\ansitowin32.py in write(self, text)
    139     def write(self, text):
    140         if self.strip or self.convert:
--> 141             self.write_and_convert(text)
    142         else:
    143             self.wrapped.write(text)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\colorama\ansitowin32.py in write_and_convert(self, text)
    167             self.convert_ansi(*match.groups())
    168             cursor = end
--> 169         self.write_plain_text(text, cursor, len(text))
    170 
    171 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\colorama\ansitowin32.py in write_plain_text(self, text, start, end)
    173         if start < end:
    174             self.wrapped.write(text[start:end])
--> 175             self.wrapped.flush()
    176 
    177 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel\iostream.py in flush(self)
    347                 self.pub_thread.schedule(evt.set)
    348                 # and give a timeout to avoid
--> 349                 if not evt.wait(self.flush_timeout):
    350                     # write directly to __stderr__ instead of warning because
    351                     # if this is happening sys.stderr may be the problem.

~\AppData\Local\Continuum\anaconda3\lib\threading.py in wait(self, timeout)
    549             signaled = self._flag
    550             if not signaled:
--> 551                 signaled = self._cond.wait(timeout)
    552             return signaled
    553 

~\AppData\Local\Continuum\anaconda3\lib\threading.py in wait(self, timeout)
    297             else:
    298                 if timeout > 0:
--> 299                     gotit = waiter.acquire(True, timeout)
    300                 else:
    301                     gotit = waiter.acquire(False)

KeyboardInterrupt: 
2018-12-10 11:12:04,443 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-10 11:12:04,443 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'eaeae72c71684611853bf5ceed4ef387', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'kernel_info_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 10, 10, 12, 4, 425495, tzinfo=tzutc())}, 'msg_id': 'eaeae72c71684611853bf5ceed4ef387', 'msg_type': 'kernel_info_request', 'parent_header': {}, 'metadata': {}, 'content': {}, 'buffers': []}
2018-12-10 11:25:54,326 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
<ipython-input-139-83de78a07a5b> in <module>()
----> 1 pprint.pprint(sentences[30000:300300])

~\AppData\Local\Continuum\anaconda3\lib\pprint.py in pprint(object, stream, indent, width, depth, compact)
     51         stream=stream, indent=indent, width=width, depth=depth,
     52         compact=compact)
---> 53     printer.pprint(object)
     54 
     55 def pformat(object, indent=1, width=80, depth=None, *, compact=False):

~\AppData\Local\Continuum\anaconda3\lib\pprint.py in pprint(self, object)
    137 
    138     def pprint(self, object):
--> 139         self._format(object, self._stream, 0, 0, {}, 0)
    140         self._stream.write("\n")
    141 

~\AppData\Local\Continuum\anaconda3\lib\pprint.py in _format(self, object, stream, indent, allowance, context, level)
    165             if p is not None:
    166                 context[objid] = 1
--> 167                 p(self, object, stream, indent, allowance, context, level + 1)
    168                 del context[objid]
    169                 return

~\AppData\Local\Continuum\anaconda3\lib\pprint.py in _pprint_list(self, object, stream, indent, allowance, context, level)
    208         stream.write('[')
    209         self._format_items(object, stream, indent, allowance + 1,
--> 210                            context, level)
    211         stream.write(']')
    212 

~\AppData\Local\Continuum\anaconda3\lib\pprint.py in _format_items(self, items, stream, indent, allowance, context, level)
    383                     write(rep)
    384                     continue
--> 385             write(delim)
    386             delim = delimnl
    387             self._format(ent, stream, indent,

~\AppData\Local\Continuum\anaconda3\lib\site-packages\colorama\ansitowin32.py in write(self, text)
     38 
     39     def write(self, text):
---> 40         self.__convertor.write(text)
     41 
     42 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\colorama\ansitowin32.py in write(self, text)
    139     def write(self, text):
    140         if self.strip or self.convert:
--> 141             self.write_and_convert(text)
    142         else:
    143             self.wrapped.write(text)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\colorama\ansitowin32.py in write_and_convert(self, text)
    167             self.convert_ansi(*match.groups())
    168             cursor = end
--> 169         self.write_plain_text(text, cursor, len(text))
    170 
    171 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\colorama\ansitowin32.py in write_plain_text(self, text, start, end)
    173         if start < end:
    174             self.wrapped.write(text[start:end])
--> 175             self.wrapped.flush()
    176 
    177 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel\iostream.py in flush(self)
    347                 self.pub_thread.schedule(evt.set)
    348                 # and give a timeout to avoid
--> 349                 if not evt.wait(self.flush_timeout):
    350                     # write directly to __stderr__ instead of warning because
    351                     # if this is happening sys.stderr may be the problem.

~\AppData\Local\Continuum\anaconda3\lib\threading.py in wait(self, timeout)
    549             signaled = self._flag
    550             if not signaled:
--> 551                 signaled = self._cond.wait(timeout)
    552             return signaled
    553 

~\AppData\Local\Continuum\anaconda3\lib\threading.py in wait(self, timeout)
    297             else:
    298                 if timeout > 0:
--> 299                     gotit = waiter.acquire(True, timeout)
    300                 else:
    301                     gotit = waiter.acquire(False)

KeyboardInterrupt: 
2018-12-10 11:25:54,329 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-10 11:25:54,329 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '8f3659cc960f45398000be375cd0c091', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'kernel_info_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 10, 10, 25, 54, 300574, tzinfo=tzutc())}, 'msg_id': '8f3659cc960f45398000be375cd0c091', 'msg_type': 'kernel_info_request', 'parent_header': {}, 'metadata': {}, 'content': {}, 'buffers': []}
2018-12-10 11:29:20,076 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-140-5c1d7b0f99ac>", line 4
    sentences += [tokenize.sent_tokenize(texts[i]) for i in range(len(texts))]]
                                                                              ^
SyntaxError: invalid syntax

2018-12-10 11:46:38,437 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-162-15730326fb03>", line 3
    if len(sentences[i]) < 20 or len(sentences[i]) > 100
                                                        ^
SyntaxError: invalid syntax

2018-12-10 11:46:47,068 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-163-8c08a346620b>", line 3
    if len(sentences[i]) < 20 or len(sentences[i]) > 100        or sentences[0:9] == "Category:"
                                                                                                ^
SyntaxError: invalid syntax

2018-12-10 12:55:08,844 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-173-496d5995df3f> in <module>()
      3     idx = []
      4     for j in range(len(sentences[i])):
----> 5         if ch in alphabets:
      6             idx += alphabets[j]
      7         else:

NameError: name 'ch' is not defined
2018-12-10 12:55:31,451 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-174-a36d296af0f2> in <module>()
      6             idx += alphabets[j]
      7         else:
----> 8             idx += 43
      9     sentences_idxs.append(idx)

TypeError: 'int' object is not iterable
2018-12-10 12:56:20,882 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-178-50cac9cbdafe> in <module>()
      4     for j in sentences[i]:
      5         if j in alphabets:
----> 6             idx += alphabets[j]
      7         else:
      8             idx += [43]

TypeError: 'int' object is not iterable
ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

2018-12-10 13:27:22,044 - (Windows 10 nt) IPKernelApp - ERROR - Exception in message handler:
Traceback (most recent call last):
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py", line 2961, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-181-0721602b9405>", line 1, in <module>
    sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(len(sentences_idxs))]
  File "<ipython-input-181-0721602b9405>", line 1, in <listcomp>
    sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(len(sentences_idxs))]
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py", line 2429, in one_hot
    name)
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py", line 4443, in one_hot
    off_value=off_value, axis=axis, name=name)
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\util\deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py", line 3155, in create_op
    op_def=op_def)
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py", line 1731, in __init__
    control_input_ops)
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py", line 1561, in _create_c_op
    c_api.TF_AddInput(op_desc, op_input._as_tf_output())
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py", line 571, in _as_tf_output
    return c_api_util.tf_output(self.op._c_op, self.value_index)
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\c_api_util.py", line 184, in tf_output
    ret = c_api.TF_Output()
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py", line 957, in __init__
    this = _pywrap_tensorflow_internal.new_TF_Output()
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py", line 2901, in run_ast_nodes
    if self.run_code(code, result):
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py", line 2964, in run_code
    sys.excepthook = old_excepthook
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py", line 1863, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py", line 2662, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py", line 2785, in _run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py", line 2926, in run_ast_nodes
    self.showtraceback()
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py", line 1866, in showtraceback
    value, tb, tb_offset=tb_offset)
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\ultratb.py", line 1373, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\ultratb.py", line 1281, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\ultratb.py", line 1144, in structured_traceback
    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)
TypeError: must be str, not list

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel\kernelbase.py", line 233, in dispatch_shell
    handler(stream, idents, msg)
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel\kernelbase.py", line 399, in execute_request
    user_expressions, allow_stdin)
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel\ipkernel.py", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel\zmqshell.py", line 537, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File "C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py", line 2666, in run_cell
    self.events.trigger('post_run_cell', result)
UnboundLocalError: local variable 'result' referenced before assignment
2018-12-10 14:00:12,856 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
<ipython-input-186-a9692a3bfc65> in <module>()
----> 1 sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(100000)]

<ipython-input-186-a9692a3bfc65> in <listcomp>(.0)
----> 1 sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(100000)]

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py in one_hot(indices, depth, on_value, off_value, axis, dtype, name)
   2427 
   2428     return gen_array_ops.one_hot(indices, depth, on_value, off_value, axis,
-> 2429                                  name)
   2430 
   2431 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py in one_hot(indices, depth, on_value, off_value, axis, name)
   4441     _, _, _op = _op_def_lib._apply_op_helper(
   4442         "OneHot", indices=indices, depth=depth, on_value=on_value,
-> 4443         off_value=off_value, axis=axis, name=name)
   4444     _result = _op.outputs[:]
   4445     _inputs_flat = _op.inputs

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    508                 dtype=dtype,
    509                 as_ref=input_arg.is_ref,
--> 510                 preferred_dtype=default_dtype)
    511           except TypeError as err:
    512             if dtype is None:

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
   1092 
   1093     if ret is None:
-> 1094       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1095 
   1096     if ret is NotImplemented:

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    215                                          as_ref=False):
    216   _ = as_ref
--> 217   return constant(v, dtype=dtype, name=name)
    218 
    219 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name, verify_shape)
    200       attrs={"value": tensor_value,
    201              "dtype": dtype_value},
--> 202       name=name).outputs[0]
    203   return const_tensor
    204 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\util\deprecation.py in new_func(*args, **kwargs)
    452                 'in a future version' if date is None else ('after %s' % date),
    453                 instructions)
--> 454       return func(*args, **kwargs)
    455     return tf_decorator.make_decorator(func, new_func, 'deprecated',
    456                                        _add_deprecated_arg_notice_to_docstring(

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in create_op(***failed resolving arguments***)
   3153           input_types=input_types,
   3154           original_op=self._default_original_op,
-> 3155           op_def=op_def)
   3156       self._create_op_helper(ret, compute_device=compute_device)
   3157     return ret

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)
   1729           op_def, inputs, node_def.attr)
   1730       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,
-> 1731                                 control_input_ops)
   1732 
   1733     # Initialize self._outputs.

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in _create_c_op(graph, node_def, inputs, control_inputs)
   1574 
   1575   try:
-> 1576     c_op = c_api.TF_FinishOperation(op_desc)
   1577   except errors.InvalidArgumentError as e:
   1578     # Convert to ValueError for backwards compatibility.

KeyboardInterrupt: 
2018-12-10 14:00:14,074 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-10 14:00:14,127 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '7dd3999764f04efd80f112ab8832a036', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'kernel_info_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 10, 12, 59, 26, 228424, tzinfo=tzutc())}, 'msg_id': '7dd3999764f04efd80f112ab8832a036', 'msg_type': 'kernel_info_request', 'parent_header': {}, 'metadata': {}, 'content': {}, 'buffers': []}
2018-12-10 14:00:14,491 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-10 14:00:14,493 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '11de655ac1684e418e1944a7401b2842', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'kernel_info_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 10, 12, 59, 34, 823193, tzinfo=tzutc())}, 'msg_id': '11de655ac1684e418e1944a7401b2842', 'msg_type': 'kernel_info_request', 'parent_header': {}, 'metadata': {}, 'content': {}, 'buffers': []}
2018-12-10 14:00:14,551 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-10 14:00:14,690 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '9b91e5c59b9a467e9dfede12ea901fb3', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 10, 12, 59, 38, 429486, tzinfo=tzutc())}, 'msg_id': '9b91e5c59b9a467e9dfede12ea901fb3', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(1000)]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-10 14:00:14,775 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-10 14:00:14,776 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '138a86d2d600470cb65d3dc86958ec0a', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'kernel_info_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 10, 12, 59, 54, 833220, tzinfo=tzutc())}, 'msg_id': '138a86d2d600470cb65d3dc86958ec0a', 'msg_type': 'kernel_info_request', 'parent_header': {}, 'metadata': {}, 'content': {}, 'buffers': []}
2018-12-10 14:00:14,779 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-10 14:00:14,780 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'd7ce4da460934ffe87f6d6f07cded1c8', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 10, 13, 0, 0, 320665, tzinfo=tzutc())}, 'msg_id': 'd7ce4da460934ffe87f6d6f07cded1c8', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(10)]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
Using TensorFlow backend.
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\client\session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).
  ```python
Using TensorFlow backend.
2018-12-11 10:44:04,375 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-13-854c9cb04f16> in <module>()
----> 1 sentences_onehot.size()

AttributeError: 'list' object has no attribute 'size'
2018-12-11 10:44:44,127 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-14-ce2c911f582d> in <module>()
----> 1 sentences_onehot.shape()

AttributeError: 'list' object has no attribute 'shape'
2018-12-11 10:44:49,465 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-15-97039a39c76c> in <module>()
----> 1 sentences_onehot.shape

AttributeError: 'list' object has no attribute 'shape'
2018-12-12 15:28:37,513 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-24-863f1b1121ef>", line 1
    x:hello=goodbye
            ^
SyntaxError: invalid character in identifier

2018-12-12 15:29:26,446 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-29-e25cbdf1e334> in <module>()
----> 1 print(x['hello'])

TypeError: string indices must be integers
2018-12-12 15:34:31,980 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-31-3072f3690396>", line 3
    p = idx * x:(idx+1) * x
               ^
SyntaxError: invalid syntax

2018-12-12 15:34:54,135 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-32-368bdf338474>", line 3
    idx * x:(idx+1) * x
                       ^
SyntaxError: illegal target for annotation

2018-12-12 15:58:07,903 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-34-d31d1fabfb10> in <module>()
      2 # X and Y are identical for the test purposes
      3 
----> 4 data = pd.DataFrame(
      5     {'X': sentences_onehot,
      6      'Y': sentences_onehot

NameError: name 'pd' is not defined
2018-12-12 16:23:50,635 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-37-3b5898d1338d> in <module>()
----> 1 len(data[X])

NameError: name 'X' is not defined
Using TensorFlow backend.
2018-12-13 11:05:13,141 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-16-13af6da12290> in <module>()
      1 p = data["X"].copy()
      2 p = np.transpose(p)
----> 3 p = pad_sequences(p, maxlen = 100, dtype='int',padding='post',truncating='post')

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras_preprocessing\sequence.py in pad_sequences(sequences, maxlen, dtype, padding, truncating, value)
     96             raise ValueError('Shape of sample %s of sequence at position %s '
     97                              'is different from expected shape %s' %
---> 98                              (trunc.shape[1:], idx, sample_shape))
     99 
    100         if padding == 'post':

ValueError: Shape of sample (46,) of sequence at position 6 is different from expected shape (44,)
2018-12-13 11:17:29,158 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-35-c955f446af15> in <module>()
      7 from keras.optimizers import Adam, Nadam
      8 
----> 9 from kerasdeepspeech.data import combine_all_wavs_and_trans_from_csvs
     10 from kerasdeepspeech.generator import BatchGenerator
     11 from kerasdeepspeech.model import *

ModuleNotFoundError: No module named 'kerasdeepspeech'
2018-12-13 11:18:00,448 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-36-e76200579ba8> in <module>()
      7 from keras.optimizers import Adam, Nadam
      8 
----> 9 from KerasDeepSpeech.data import combine_all_wavs_and_trans_from_csvs
     10 from kerasdeepspeech.generator import BatchGenerator
     11 from kerasdeepspeech.model import *

~\Documents\GitHub\Textfixer\KerasDeepSpeech\data.py in <module>()
      2 import os
      3 import pandas as pd
----> 4 import char_map
      5 from utils import text_to_int_sequence
      6 

ModuleNotFoundError: No module named 'char_map'
2018-12-13 11:18:23,322 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-37-e421fdf95741> in <module>()
      8 
      9 #from KerasDeepSpeech.data import combine_all_wavs_and_trans_from_csvs
---> 10 from KerasDeepSpeech.generator import BatchGenerator
     11 from KerasDeepSpeech.model import *
     12 from KerasDeepSpeech.report import ReportCallback

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in <module>()
      4 from sklearn.utils import shuffle
      5 
----> 6 import python_speech_features as p
      7 import scipy.io.wavfile as wav
      8 import scipy

ModuleNotFoundError: No module named 'python_speech_features'
Using TensorFlow backend.
2018-12-13 11:19:38,886 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-6-035f8099e2d1> in <module>()
----> 1 print(len(sentences))
      2 for i in reversed(range(len(sentences))):
      3     if len(sentences[i]) < 20 or len(sentences[i]) > 100         or sentences[i][0:9] == "Category:"         or sentences[i][0:13] == "Related pages"         or sentences[i][0:10] == "References"         or sentences[i][0:14] == "Other websites":
      4         sentences.pop(i)
      5 print(len(sentences))

NameError: name 'sentences' is not defined
2018-12-13 11:19:38,889 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 11:19:38,889 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '2bfc6f5176ce43f5ac291d420f292fad', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 10, 19, 17, 224740, tzinfo=tzutc())}, 'msg_id': '2bfc6f5176ce43f5ac291d420f292fad', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'alphabets = {\'a\' : 0, \'b\': 1, \'c\':2, \'d\':3, \'e\':4, \'f\':5, \'g\':6, \'h\':7, \'i\':8, \'j\':9, \'k\':10, \'l\':11, \'m\':12, \'n\':13, \'o\':14,\n            \'p\':15, \'q\':16, \'r\':17, \'s\':18, \'t\':19, \'u\':20, \'v\':21, \'w\':22, \'x\':23, \'y\':24, \'z\':25, \n            \'0\':26, \'1\':27, \'2\':28, \'3\':29, \'4\':30, \'5\':31, \'6\':32, \'7\':33, \'8\':34, \'9\':35, \n            \' \':36, \',\':37, \'.\':38, \':\':39, \';\':40, \'"\':41, "\'":42, \'\':43, \'(\':44, \')\':45} #43 = unknown symbol\n\nidxs = [alphabets[ch] if ch in alphabets else 43 for ch in \'az 123#\']\n\nidxs\n\n#one_hot = tf.one_hot(idxs, depth=len(alphabets), dtype=tf.uint8)\n\n#sess = tf.InteractiveSession()\n#one_hot.eval()\none_hot = to_categorical(idxs, num_classes = len(alphabets))\none_hot', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 11:19:38,891 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 11:19:38,891 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '3075ed8db5744e6080a355f6ba18c729', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 10, 19, 20, 101389, tzinfo=tzutc())}, 'msg_id': '3075ed8db5744e6080a355f6ba18c729', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'sentences_idxs = []\nfor i in range(len(sentences)):\n    idx = []\n    for j in sentences[i]:\n        if j in alphabets:\n            idx += [alphabets[j]]\n        else:\n            idx += [43]\n    sentences_idxs.append(idx)\n    \n#sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(len(sentences_idxs))]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 11:19:38,893 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 11:19:38,893 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'e2c23283c4db47ad8de1a676b103ddc2', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 10, 19, 21, 840012, tzinfo=tzutc())}, 'msg_id': 'e2c23283c4db47ad8de1a676b103ddc2', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': '#sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(10000)]\nsentences_onehot = [to_categorical(sentences_idxs[i], num_classes = len(alphabets)) for i in range(100000)]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 11:19:38,895 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 11:19:38,895 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'ed80a34b6e674a078a7221c15561ab73', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 10, 19, 25, 415650, tzinfo=tzutc())}, 'msg_id': 'ed80a34b6e674a078a7221c15561ab73', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': "# Generate the data examples\n# X and Y are identical for the test purposes\n\ndata = pd.DataFrame(\n    {'X': sentences_onehot,\n     'Y': sentences_onehot\n    })\n\nlen(sentences_onehot[100][0])\nlen(sentences_onehot)", 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 11:19:38,896 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 11:19:38,897 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '508293d553344bfcadd64ec1f37a1265', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 10, 19, 27, 446292, tzinfo=tzutc())}, 'msg_id': '508293d553344bfcadd64ec1f37a1265', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': '#####################################################\n\nimport os\n\nimport keras\nfrom keras.callbacks import TensorBoard\nfrom keras.optimizers import Adam, Nadam\n\n#from KerasDeepSpeech.data import combine_all_wavs_and_trans_from_csvs\nfrom KerasDeepSpeech.generator import BatchGenerator\nfrom KerasDeepSpeech.model import *\nfrom KerasDeepSpeech.report import ReportCallback\nfrom KerasDeepSpeech.utils import load_model_checkpoint, save_model, MemoryCallback\n\n#####################################################\n\n\n#######################################################\n\n# Prevent pool_allocator message\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n#######################################################\n\n\ndef main(args):\n    \'\'\'\n    There are 5 simple steps to this program\n    \'\'\'\n\n    #1. combine all data into 2 dataframes (train, valid)\n    print("Getting data from arguments")\n    #train_dataprops, df_train = combine_all_wavs_and_trans_from_csvs(args.train_files, sortagrad=args.sortagrad)\n    #valid_dataprops, df_valid = combine_all_wavs_and_trans_from_csvs(args.valid_files, sortagrad=args.sortagrad)\n\n    train_ratio = 0.9\n    args.model_arch = 0\n    args.opt = "adam"\n    args.train_steps = 0\n    args.epochs = 50\n    args.valid_steps = 0\n    model_input_type = "text"\n    \n    df_train = data[0:int(train_ratio * len(sentences_onehot))]\n    df_valid = data[int(train_ratio * len(sentences_onehot)) + 1:]\n\n\n    ## 2. init data generators\n    print("Creating data batch generators")\n    traindata = BatchGenerator(dataframe=df_train, dataproperties=None,\n                              training=True, batch_size=args.batchsize, model_input_type=model_input_type)\n    validdata = BatchGenerator(dataframe=df_valid, dataproperties=None,\n                              training=False, batch_size=args.batchsize, model_input_type=model_input_type)\n\n\n\n\n    output_dir = os.path.join(\'checkpoints/results\',\n                                  \'model%s_%s\' % (args.model_arch,\n                                             args.name))\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir)\n\n\n    ## 3. Load existing or create new model\n    if args.loadcheckpointpath:\n        # load existing\n        print("Loading model")\n\n        cp = args.loadcheckpointpath\n        assert(os.path.isdir(cp))\n\n        model_path = os.path.join(cp, "model")\n        # assert(os.path.isfile(model_path))\n\n        model = load_model_checkpoint(model_path)\n\n\n        print("Model loaded")\n    else:\n        # new model recipes here\n        print(\'New model DS{}\'.format(args.model_arch))\n        if (args.model_arch == 0):\n            # DeepSpeech1 with Dropout\n            model = ds1_dropout(input_dim=len(alphabets), fc_size=args.fc_size, rnn_size=args.rnn_size,dropout=[0.1,0.1,0.1], output_dim=len(alphabets))\n\n        elif(args.model_arch==1):\n            # DeepSpeech1 - no dropout\n            model = ds1(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n\n        elif(args.model_arch==2):\n            # DeepSpeech2 model\n            model = ds2_gru_model(input_dim=161, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n\n        elif(args.model_arch==3):\n            # own model\n            model = ownModel(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, dropout=[0.1, 0.1, 0.1], output_dim=29)\n\n        elif(args.model_arch==4):\n            # graves model\n            model = graves(input_dim=26, rnn_size=args.rnn_size, output_dim=29, std=0.5)\n\n        elif(args.model_arch==5):\n            #cnn city\n            model = cnn_city(input_dim=161, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n\n        elif(args.model_arch == 6):\n            # constrained model\n            model = const(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n        else:\n            raise("model not found")\n\n        print(model.summary(line_length=80))\n\n        #required to save the JSON\n        save_model(model, output_dir)\n\n    if (args.opt.lower() == \'sgd\'):\n        opt = SGD(lr=args.learning_rate, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n    elif (args.opt.lower() == \'adam\'):\n        opt = Adam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n    elif (args.opt.lower() == \'nadam\'):\n        opt = Nadam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n    else:\n        raise "optimiser not recognised"\n\n    model.compile(optimizer=opt, loss=ctc)\n\n    ## 4. train\n\n    if args.train_steps == 0:\n        args.train_steps = len(df_train.index) // args.batchsize\n        # print(args.train_steps)\n    # we use 1/xth of the validation data at each epoch end to test val score\n    if args.valid_steps == 0:\n\n        args.valid_steps = (len(df_valid.index) // args.batchsize)\n        # print(args.valid_steps)\n\n\n    if args.memcheck:\n        cb_list = [MemoryCallback()]\n    else:\n        cb_list = []\n\n    if args.tensorboard:\n        tb_cb = TensorBoard(log_dir=\'./tensorboard/{}/\'.format(args.name), write_graph=False, write_images=True)\n        cb_list.append(tb_cb)\n\n    y_pred = model.get_layer(\'ctc\').input[0]\n    input_data = model.get_layer(\'the_input\').input\n\n    report = K.function([input_data, K.learning_phase()], [y_pred])\n    report_cb = ReportCallback(report, validdata, model, args.name, save=True)\n\n    cb_list.append(report_cb)\n\n    model.fit_generator(generator=traindata.next_batch(),\n                        steps_per_epoch=args.train_steps,\n                        epochs=args.epochs,\n                        callbacks=cb_list,\n                        validation_data=validdata.next_batch(),\n                        validation_steps=args.valid_steps,\n                        initial_epoch=0,\n                        verbose=1,\n                        class_weight=None,\n                        max_q_size=10,\n                        workers=1,\n                        pickle_safe=False\n                        )\n\n    # K.clear_session()\n\n    ## These are the most important metrics\n    print("Mean WER   :", report_cb.mean_wer_log)\n    print("Mean LER   :", report_cb.mean_ler_log)\n    print("NormMeanLER:", report_cb.norm_mean_ler_log)\n\n    # export to csv?\n    K.clear_session()', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 11:21:44,753 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-13-e421fdf95741> in <module>()
      8 
      9 #from KerasDeepSpeech.data import combine_all_wavs_and_trans_from_csvs
---> 10 from KerasDeepSpeech.generator import BatchGenerator
     11 from KerasDeepSpeech.model import *
     12 from KerasDeepSpeech.report import ReportCallback

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in <module>()
     15 from keras.preprocessing.sequence import pad_sequences
     16 
---> 17 from utils import text_to_int_sequence
     18 
     19 # Data batch generator, responsible for providing the data to fit_generator

ModuleNotFoundError: No module named 'utils'
2018-12-13 11:22:04,332 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-14-e421fdf95741> in <module>()
     10 from KerasDeepSpeech.generator import BatchGenerator
     11 from KerasDeepSpeech.model import *
---> 12 from KerasDeepSpeech.report import ReportCallback
     13 from KerasDeepSpeech.utils import load_model_checkpoint, save_model, MemoryCallback
     14 

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in <module>()
      1 from keras import callbacks
----> 2 from text import *
      3 
      4 import itertools
      5 import numpy as np

ModuleNotFoundError: No module named 'text'
2018-12-13 11:33:51,656 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-15-e421fdf95741> in <module>()
     10 from KerasDeepSpeech.generator import BatchGenerator
     11 from KerasDeepSpeech.model import *
---> 12 from KerasDeepSpeech.report import ReportCallback
     13 from KerasDeepSpeech.utils import load_model_checkpoint, save_model, MemoryCallback
     14 

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in <module>()
      1 from keras import callbacks
----> 2 from text import *
      3 
      4 import itertools
      5 import numpy as np

ModuleNotFoundError: No module named 'text'
2018-12-13 11:34:41,700 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-16-e421fdf95741> in <module>()
     10 from KerasDeepSpeech.generator import BatchGenerator
     11 from KerasDeepSpeech.model import *
---> 12 from KerasDeepSpeech.report import ReportCallback
     13 from KerasDeepSpeech.utils import load_model_checkpoint, save_model, MemoryCallback
     14 

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in <module>()
      1 from keras import callbacks
----> 2 from KerasDeepSpeech.text import *
      3 
      4 import itertools
      5 import numpy as np

~\Documents\GitHub\Textfixer\KerasDeepSpeech\text.py in <module>()
      2 
      3 
----> 4 import kenlm
      5 import re
      6 from heapq import heapify

ModuleNotFoundError: No module named 'kenlm'
2018-12-13 11:42:43,647 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
<ipython-input-17-e421fdf95741> in <module>()
     10 from KerasDeepSpeech.generator import BatchGenerator
     11 from KerasDeepSpeech.model import *
---> 12 from KerasDeepSpeech.report import ReportCallback
     13 from KerasDeepSpeech.utils import load_model_checkpoint, save_model, MemoryCallback
     14 

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in <module>()
      1 from keras import callbacks
----> 2 from KerasDeepSpeech.text import *
      3 
      4 import itertools
      5 import numpy as np

~\Documents\GitHub\Textfixer\KerasDeepSpeech\text.py in <module>()
    143 MODEL = None
    144 # Load known word set
--> 145 with open('./lm/words.txt') as f:
    146     WORDS = set(words(f.read()))

FileNotFoundError: [Errno 2] No such file or directory: './lm/words.txt'
2018-12-13 11:43:44,931 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-18-e421fdf95741> in <module>()
     10 from KerasDeepSpeech.generator import BatchGenerator
     11 from KerasDeepSpeech.model import *
---> 12 from KerasDeepSpeech.report import ReportCallback
     13 from KerasDeepSpeech.utils import load_model_checkpoint, save_model, MemoryCallback
     14 

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in <module>()
      9 import keras.backend as K
     10 
---> 11 from KerasDeepSpeech.utils import save_model, int_to_text_sequence
     12 
     13 class ReportCallback(callbacks.Callback):

~\Documents\GitHub\Textfixer\KerasDeepSpeech\utils.py in <module>()
----> 1 from char_map import char_map, index_map
      2 
      3 
      4 from pympler import muppy, summary, tracker, classtracker
      5 from pympler.garbagegraph import GarbageGraph, start_debug_garbage

ModuleNotFoundError: No module named 'char_map'
2018-12-13 11:45:01,848 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-19-e421fdf95741> in <module>()
     10 from KerasDeepSpeech.generator import BatchGenerator
     11 from KerasDeepSpeech.model import *
---> 12 from KerasDeepSpeech.report import ReportCallback
     13 from KerasDeepSpeech.utils import load_model_checkpoint, save_model, MemoryCallback
     14 

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in <module>()
      9 import keras.backend as K
     10 
---> 11 from KerasDeepSpeech.utils import save_model, int_to_text_sequence
     12 
     13 class ReportCallback(callbacks.Callback):

~\Documents\GitHub\Textfixer\KerasDeepSpeech\utils.py in <module>()
      7 import types
      8 
----> 9 import resource
     10 import tensorflow as tf
     11 import keras

ModuleNotFoundError: No module named 'resource'
2018-12-13 11:45:43,051 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-20-e421fdf95741> in <module>()
     10 from KerasDeepSpeech.generator import BatchGenerator
     11 from KerasDeepSpeech.model import *
---> 12 from KerasDeepSpeech.report import ReportCallback
     13 from KerasDeepSpeech.utils import load_model_checkpoint, save_model, MemoryCallback
     14 

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in <module>()
      9 import keras.backend as K
     10 
---> 11 from KerasDeepSpeech.utils import save_model, int_to_text_sequence
     12 
     13 class ReportCallback(callbacks.Callback):

~\Documents\GitHub\Textfixer\KerasDeepSpeech\utils.py in <module>()
      7 import types
      8 
----> 9 import resource
     10 import tensorflow as tf
     11 import keras

ModuleNotFoundError: No module named 'resource'
2018-12-13 11:45:47,030 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-21-e421fdf95741> in <module>()
     10 from KerasDeepSpeech.generator import BatchGenerator
     11 from KerasDeepSpeech.model import *
---> 12 from KerasDeepSpeech.report import ReportCallback
     13 from KerasDeepSpeech.utils import load_model_checkpoint, save_model, MemoryCallback
     14 

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in <module>()
      9 import keras.backend as K
     10 
---> 11 from KerasDeepSpeech.utils import save_model, int_to_text_sequence
     12 
     13 class ReportCallback(callbacks.Callback):

~\Documents\GitHub\Textfixer\KerasDeepSpeech\utils.py in <module>()
      7 import types
      8 
----> 9 import resource
     10 import tensorflow as tf
     11 import keras

ModuleNotFoundError: No module named 'resource'
Using TensorFlow backend.
2018-12-13 11:47:52,106 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-12-e421fdf95741> in <module>()
      8 
      9 #from KerasDeepSpeech.data import combine_all_wavs_and_trans_from_csvs
---> 10 from KerasDeepSpeech.generator import BatchGenerator
     11 from KerasDeepSpeech.model import *
     12 from KerasDeepSpeech.report import ReportCallback

~\Documents\GitHub\Textfixer\KerasDeepSpeech\__init__.py in <module>()
      1 from KerasDeepSpeech.text import wer, wers, lers, levenshtein, get_model, words, log_probability, correction, candidate_words, known_words, edits1, edits2
----> 2 from KerasDeepSpeech.utils import text_to_int_sequence, int_to_text_sequence, save_trimmed_model, save_model, load_model_checkpoint, load_cmodel_checkpoint, MemoryCallback

~\Documents\GitHub\Textfixer\KerasDeepSpeech\utils.py in <module>()
      7 import types
      8 
----> 9 import resource
     10 import tensorflow as tf
     11 import keras

ModuleNotFoundError: No module named 'resource'
2018-12-13 12:48:45,845 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-13-e421fdf95741> in <module>()
      8 
      9 #from KerasDeepSpeech.data import combine_all_wavs_and_trans_from_csvs
---> 10 from KerasDeepSpeech.generator import BatchGenerator
     11 from KerasDeepSpeech.model import *
     12 from KerasDeepSpeech.report import ReportCallback

~\Documents\GitHub\Textfixer\KerasDeepSpeech\__init__.py in <module>()
      1 from KerasDeepSpeech.text import wer, wers, lers, levenshtein, get_model, words, log_probability, correction, candidate_words, known_words, edits1, edits2
----> 2 from KerasDeepSpeech.utils import text_to_int_sequence, int_to_text_sequence, save_trimmed_model, save_model, load_model_checkpoint, load_cmodel_checkpoint, MemoryCallback

~\Documents\GitHub\Textfixer\KerasDeepSpeech\utils.py in <module>()
     20 import yaml
     21 
---> 22 from model import clipped_relu, selu
     23 
     24 # these text/int characters are modified

ModuleNotFoundError: No module named 'model'
2018-12-13 12:52:30,203 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-15-1b246a61c8e9> in <module>()
----> 1 main(args = [])

<ipython-input-14-e421fdf95741> in main(args)
     35 
     36     train_ratio = 0.9
---> 37     args.model_arch = 0
     38     args.opt = "adam"
     39     args.train_steps = 0

AttributeError: 'list' object has no attribute 'model_arch'
2018-12-13 12:53:08,956 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-16-cf614b15586f>", line 1
    main(args = {"model_arch " = 0, "opt" = "adam", "train_steps" = 0})
                               ^
SyntaxError: invalid syntax

2018-12-13 12:53:22,364 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-17-3371ba65d607> in <module>()
----> 1 main(args = {"model_arch ":0, "opt":"adam", "train_steps":0})

<ipython-input-14-e421fdf95741> in main(args)
     35 
     36     train_ratio = 0.9
---> 37     args.model_arch = 0
     38     args.opt = "adam"
     39     args.train_steps = 0

AttributeError: 'dict' object has no attribute 'model_arch'
2018-12-13 12:53:46,056 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-18-5281c26d1a65> in <module>()
      1 args = {"model_arch ":0, "opt":"adam", "train_steps":0}
----> 2 main()

TypeError: main() missing 1 required positional argument: 'args'
2018-12-13 12:53:50,958 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-19-45fb0c26ae11> in <module>()
----> 1 args.model_arch

AttributeError: 'dict' object has no attribute 'model_arch'
2018-12-13 12:54:26,285 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-20-42aa1c4e1c9c> in <module>()
      1 args = None
----> 2 args.model_arch = 1

AttributeError: 'NoneType' object has no attribute 'model_arch'
2018-12-13 12:56:41,249 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-21-6cbf28955e0b> in <module>()
----> 1 args = someobject
      2 args.model_arch = lambda: None
      3 setattr(args.model_arch, 'model_arch', 0)

NameError: name 'someobject' is not defined
2018-12-13 12:56:52,254 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-22-88292efd88b3> in <module>()
      1 args = object
----> 2 args.model_arch = lambda: None
      3 setattr(args.model_arch, 'model_arch', 0)

TypeError: can't set attributes of built-in/extension type 'object'
2018-12-13 13:17:07,459 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-24-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-14-e421fdf95741> in main(args)
     49     print("Creating data batch generators")
     50     traindata = BatchGenerator(dataframe=df_train, dataproperties=None,
---> 51                               training=True, batch_size=args.batchsize, model_input_type=model_input_type)
     52     validdata = BatchGenerator(dataframe=df_valid, dataproperties=None,
     53                               training=False, batch_size=args.batchsize, model_input_type=model_input_type)

AttributeError: 'Object' object has no attribute 'batchsize'
2018-12-13 13:17:48,307 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-27-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-25-2f7cb6373c7c> in main(args)
     50     print("Creating data batch generators")
     51     traindata = BatchGenerator(dataframe=df_train, dataproperties=None,
---> 52                               training=True, batch_size=args.batchsize, model_input_type=model_input_type)
     53     validdata = BatchGenerator(dataframe=df_valid, dataproperties=None,
     54                               training=False, batch_size=args.batchsize, model_input_type=model_input_type)

AttributeError: 'Object' object has no attribute 'batchsize'
2018-12-13 13:18:02,731 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-29-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-28-cec29f6850bb> in main(args)
     59     output_dir = os.path.join('checkpoints/results',
     60                                   'model%s_%s' % (args.model_arch,
---> 61                                              args.name))
     62     if not os.path.isdir(output_dir):
     63         os.makedirs(output_dir)

AttributeError: 'Object' object has no attribute 'name'
2018-12-13 13:19:00,944 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-31-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-30-a5ff188d26b9> in main(args)
     67 
     68     ## 3. Load existing or create new model
---> 69     if args.loadcheckpointpath:
     70         # load existing
     71         print("Loading model")

AttributeError: 'Object' object has no attribute 'loadcheckpointpath'
2018-12-13 13:19:37,385 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-33-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-32-961dfc395485> in main(args)
     87         if (args.model_arch == 0):
     88             # DeepSpeech1 with Dropout
---> 89             model = ds1_dropout(input_dim=len(alphabets), fc_size=args.fc_size, rnn_size=args.rnn_size,dropout=[0.1,0.1,0.1], output_dim=len(alphabets))
     90 
     91         elif(args.model_arch==1):

AttributeError: 'Object' object has no attribute 'fc_size'
2018-12-13 13:19:59,934 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-35-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-34-b7ecd7e344cc> in main(args)
     88         if (args.model_arch == 0):
     89             # DeepSpeech1 with Dropout
---> 90             model = ds1_dropout(input_dim=len(alphabets), fc_size=args.fc_size, rnn_size=args.rnn_size,dropout=[0.1,0.1,0.1], output_dim=len(alphabets))
     91 
     92         elif(args.model_arch==1):

AttributeError: 'Object' object has no attribute 'rnn_size'
2018-12-13 13:20:18,702 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-37-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-36-f0393971dd25> in main(args)
    125         opt = SGD(lr=args.learning_rate, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)
    126     elif (args.opt.lower() == 'adam'):
--> 127         opt = Adam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)
    128     elif (args.opt.lower() == 'nadam'):
    129         opt = Nadam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)

AttributeError: 'Object' object has no attribute 'learning_rate'
2018-12-13 13:21:02,965 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-39-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-38-a844607765da> in main(args)
    146 
    147 
--> 148     if args.memcheck:
    149         cb_list = [MemoryCallback()]
    150     else:

AttributeError: 'Object' object has no attribute 'memcheck'
2018-12-13 13:21:23,251 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-41-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-40-d04ac9c1d7ef> in main(args)
    153         cb_list = []
    154 
--> 155     if args.tensorboard:
    156         tb_cb = TensorBoard(log_dir='./tensorboard/{}/'.format(args.name), write_graph=False, write_images=True)
    157         cb_list.append(tb_cb)

AttributeError: 'Object' object has no attribute 'tensorboard'
2018-12-13 13:21:51,193 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-43-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-42-8c64b3d0f577> in main(args)
    162 
    163     report = K.function([input_data, K.learning_phase()], [y_pred])
--> 164     report_cb = ReportCallback(report, validdata, model, args.name, save=True)
    165 
    166     cb_list.append(report_cb)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in __init__(self, test_func, validdata, model, runtimestr, save)
     27         self.val_best_norm_mean_ed = 0
     28 
---> 29         self.lm = get_model()
     30 
     31         self.model = model

~\Documents\GitHub\Textfixer\KerasDeepSpeech\text.py in get_model()
     88     if MODEL is None:
     89         #MODEL = kenlm.Model('./lm/timit-lm.klm')
---> 90         MODEL = kenlm.Model('./lm/libri-timit-lm.klm')
     91     return MODEL
     92 

NameError: name 'kenlm' is not defined
2018-12-13 13:24:00,975 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-45-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-44-8c64b3d0f577> in main(args)
    162 
    163     report = K.function([input_data, K.learning_phase()], [y_pred])
--> 164     report_cb = ReportCallback(report, validdata, model, args.name, save=True)
    165 
    166     cb_list.append(report_cb)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in __init__(self, test_func, validdata, model, runtimestr, save)
     27         self.val_best_norm_mean_ed = 0
     28 
---> 29         self.lm = get_model()
     30 
     31         self.model = model

~\Documents\GitHub\Textfixer\KerasDeepSpeech\text.py in get_model()
     88     if MODEL is None:
     89         pass
---> 90         #MODEL = kenlm.Model('./lm/timit-lm.klm')
     91         #MODEL = kenlm.Model('./lm/libri-timit-lm.klm')
     92     return MODEL

NameError: name 'kenlm' is not defined
Using TensorFlow backend.
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=5625, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=624, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-13 13:26:33,770 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    148             try:
--> 149                 ret = self.get_batch(self.cur_index)
    150             except:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
     83             X_data = np.array([pad_sequences(item, maxlen = max_val, dtype='int',padding='post',truncating='post') for item in batch_x])
---> 84             assert (X_data.shape == (self.batch_size, max_val, 46))
     85 

AssertionError: 

During handling of the above exception, another exception occurred:

AssertionError                            Traceback (most recent call last)
<ipython-input-18-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-16-8c64b3d0f577> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    175             batch_index = 0
    176             while steps_done < steps_per_epoch:
--> 177                 generator_output = next(output_generator)
    178 
    179                 if not hasattr(generator_output, '__len__'):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in get(self)
    791             success, value = self.queue.get()
    792             if not success:
--> 793                 six.reraise(value.__class__, value, value.__traceback__)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in _data_generator_task(self)
    656                             # => Serialize calls to
    657                             # infinite iterator/generator's next() function
--> 658                             generator_output = next(self._generator)
    659                             self.queue.put((True, generator_output))
    660                         else:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    151                 print("data error - this shouldn't happen - try next batch")
    152                 self.cur_index += 1
--> 153                 ret = self.get_batch(self.cur_index)
    154 
    155             self.cur_index += 1

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
     82             #X_data = np.array([make_mfcc_shape(item, padlen=max_val) for item in batch_x])
     83             X_data = np.array([pad_sequences(item, maxlen = max_val, dtype='int',padding='post',truncating='post') for item in batch_x])
---> 84             assert (X_data.shape == (self.batch_size, max_val, 46))
     85 
     86         # print("1. X_data shape:", X_data.shape)

AssertionError: 
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=5625, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=624, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-13 13:28:08,232 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    148 
--> 149             try:
    150                 ret = self.get_batch(self.cur_index)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
     83             X_data = np.array([pad_sequences(item, maxlen = max_val, dtype='int',padding='post',truncating='post') for item in batch_x])
---> 84             print(X_data.shape)
     85             assert (X_data.shape == (self.batch_size, max_val, 46))

AssertionError: 

During handling of the above exception, another exception occurred:

AssertionError                            Traceback (most recent call last)
<ipython-input-19-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-16-8c64b3d0f577> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    175             batch_index = 0
    176             while steps_done < steps_per_epoch:
--> 177                 generator_output = next(output_generator)
    178 
    179                 if not hasattr(generator_output, '__len__'):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in get(self)
    791             success, value = self.queue.get()
    792             if not success:
--> 793                 six.reraise(value.__class__, value, value.__traceback__)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in _data_generator_task(self)
    656                             # => Serialize calls to
    657                             # infinite iterator/generator's next() function
--> 658                             generator_output = next(self._generator)
    659                             self.queue.put((True, generator_output))
    660                         else:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    151             except:
    152                 print("data error - this shouldn't happen - try next batch")
--> 153                 self.cur_index += 1
    154                 ret = self.get_batch(self.cur_index)
    155 

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
     82             #X_data = np.array([make_mfcc_shape(item, padlen=max_val) for item in batch_x])
     83             X_data = np.array([pad_sequences(item, maxlen = max_val, dtype='int',padding='post',truncating='post') for item in batch_x])
---> 84             print(X_data.shape)
     85             assert (X_data.shape == (self.batch_size, max_val, 46))
     86 

AssertionError: 
Using TensorFlow backend.
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=5625, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=624, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-13 13:30:31,378 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    149             try:
--> 150                 ret = self.get_batch(self.cur_index)
    151             except:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
     84             print(X_data.shape)
---> 85             assert (X_data.shape == (self.batch_size, max_val, 46))
     86 

AssertionError: 

During handling of the above exception, another exception occurred:

AssertionError                            Traceback (most recent call last)
<ipython-input-19-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-17-8c64b3d0f577> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    175             batch_index = 0
    176             while steps_done < steps_per_epoch:
--> 177                 generator_output = next(output_generator)
    178 
    179                 if not hasattr(generator_output, '__len__'):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in get(self)
    791             success, value = self.queue.get()
    792             if not success:
--> 793                 six.reraise(value.__class__, value, value.__traceback__)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in _data_generator_task(self)
    656                             # => Serialize calls to
    657                             # infinite iterator/generator's next() function
--> 658                             generator_output = next(self._generator)
    659                             self.queue.put((True, generator_output))
    660                         else:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    152                 print("data error - this shouldn't happen - try next batch")
    153                 self.cur_index += 1
--> 154                 ret = self.get_batch(self.cur_index)
    155 
    156             self.cur_index += 1

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
     83             X_data = np.array([pad_sequences(item, maxlen = max_val, dtype='int',padding='post',truncating='post') for item in batch_x])
     84             print(X_data.shape)
---> 85             assert (X_data.shape == (self.batch_size, max_val, 46))
     86 
     87         # print("1. X_data shape:", X_data.shape)

AssertionError: 
Using TensorFlow backend.
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=5625, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=624, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-13 13:35:19,291 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    151             try:
--> 152                 ret = self.get_batch(self.cur_index)
    153             except:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
     86             print(len(X_data[0][0]))
---> 87             assert (X_data.shape == (self.batch_size, max_val, 46))
     88 

AssertionError: 

During handling of the above exception, another exception occurred:

AssertionError                            Traceback (most recent call last)
<ipython-input-19-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-17-8c64b3d0f577> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    175             batch_index = 0
    176             while steps_done < steps_per_epoch:
--> 177                 generator_output = next(output_generator)
    178 
    179                 if not hasattr(generator_output, '__len__'):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in get(self)
    791             success, value = self.queue.get()
    792             if not success:
--> 793                 six.reraise(value.__class__, value, value.__traceback__)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in _data_generator_task(self)
    656                             # => Serialize calls to
    657                             # infinite iterator/generator's next() function
--> 658                             generator_output = next(self._generator)
    659                             self.queue.put((True, generator_output))
    660                         else:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    154                 print("data error - this shouldn't happen - try next batch")
    155                 self.cur_index += 1
--> 156                 ret = self.get_batch(self.cur_index)
    157 
    158             self.cur_index += 1

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
     85             print(len(X_data[0]))
     86             print(len(X_data[0][0]))
---> 87             assert (X_data.shape == (self.batch_size, max_val, 46))
     88 
     89         # print("1. X_data shape:", X_data.shape)

AssertionError: 
2018-12-13 13:39:34,828 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-20-caa5bacc11fb> in <module>()
      1 from keras.preprocessing.sequence import pad_sequences
----> 2 batch_x = df_train["X"][0:16]
      3 print(len(batch_x))
      4 #X_data = np.array([pad_sequences(item, maxlen = max_val, dtype='int',padding='post',truncating='post') for item in batch_x])

NameError: name 'df_train' is not defined
2018-12-13 13:40:20,456 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-22-d6e65a580a30> in <module>()
      3 batch_x = df_train["X"][0:16]
      4 print(len(batch_x))
----> 5 X_data = np.array([pad_sequences(item, maxlen = max_val, dtype='int',padding='post',truncating='post') for item in batch_x])

<ipython-input-22-d6e65a580a30> in <listcomp>(.0)
      3 batch_x = df_train["X"][0:16]
      4 print(len(batch_x))
----> 5 X_data = np.array([pad_sequences(item, maxlen = max_val, dtype='int',padding='post',truncating='post') for item in batch_x])

NameError: name 'max_val' is not defined
2018-12-13 13:43:08,276 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-25-2b211e6cd726> in <module>()
----> 1 X_data.shape()

TypeError: 'tuple' object is not callable
2018-12-13 13:47:36,375 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-32-7b2aa0776a21> in <module>()
----> 1 l.shape

AttributeError: 'list' object has no attribute 'shape'
Using TensorFlow backend.
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=5625, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=624, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-13 13:55:43,427 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    151             try:
--> 152                 ret = self.get_batch(self.cur_index)
    153             except:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
    103         # print("2. labels values=", labels)
--> 104         assert(labels.shape == (self.batch_size, max_y))
    105 

AssertionError: 

During handling of the above exception, another exception occurred:

AssertionError                            Traceback (most recent call last)
<ipython-input-19-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-17-8c64b3d0f577> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    175             batch_index = 0
    176             while steps_done < steps_per_epoch:
--> 177                 generator_output = next(output_generator)
    178 
    179                 if not hasattr(generator_output, '__len__'):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in get(self)
    791             success, value = self.queue.get()
    792             if not success:
--> 793                 six.reraise(value.__class__, value, value.__traceback__)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in _data_generator_task(self)
    656                             # => Serialize calls to
    657                             # infinite iterator/generator's next() function
--> 658                             generator_output = next(self._generator)
    659                             self.queue.put((True, generator_output))
    660                         else:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    154                 print("data error - this shouldn't happen - try next batch")
    155                 self.cur_index += 1
--> 156                 ret = self.get_batch(self.cur_index)
    157 
    158             self.cur_index += 1

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
    102         # print("2. labels shape:", labels.shape)
    103         # print("2. labels values=", labels)
--> 104         assert(labels.shape == (self.batch_size, max_y))
    105 
    106         # 3. input_length (required for CTC loss)

AssertionError: 
2018-12-13 13:55:43,431 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 13:55:43,432 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'ecfbac80671c4f9489f0543000b3c7c8', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 12, 53, 33, 357405, tzinfo=tzutc())}, 'msg_id': 'ecfbac80671c4f9489f0543000b3c7c8', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'from keras.preprocessing.sequence import pad_sequences\ndf_train = data[0:int(0.9 * len(sentences_onehot))]\nbatch_x = df_train["X"][0:16]\n\n\nx_val = [len(item) for item in batch_x]\nmax_val = max(x_val)\nl = []\n\nfor item in batch_x:\n    s = np.transpose(item)\n    l.append(pad_sequences(item, maxlen = max_val, dtype=\'int\',padding=\'post\',truncating=\'post\'))\n    \nX_data = np.array(l)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 13:55:43,433 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 13:55:43,433 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '013d27ac63384c2d8fe8f57fd0b9446d', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 12, 53, 33, 358406, tzinfo=tzutc())}, 'msg_id': '013d27ac63384c2d8fe8f57fd0b9446d', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': "X_data = np.array([np.transpose(pad_sequences(np.transpose(item), maxlen = max_val, dtype='int',padding='post',truncating='post')) for item in batch_x])", 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 13:55:43,435 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 13:55:43,435 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'd87027af204e4aa78cfff6cce9007aaa', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 12, 53, 33, 363418, tzinfo=tzutc())}, 'msg_id': 'd87027af204e4aa78cfff6cce9007aaa', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'X_data[0].shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 13:55:43,436 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 13:55:43,436 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '35ec12c2070144b488ee1144623a033a', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 12, 53, 33, 365461, tzinfo=tzutc())}, 'msg_id': '35ec12c2070144b488ee1144623a033a', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'batch_x[0].shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
Using TensorFlow backend.
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=5625, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=624, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-13 13:58:57,260 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    151             try:
--> 152                 ret = self.get_batch(self.cur_index)
    153             except:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
    103         # print("2. labels values=", labels)
--> 104         assert(labels.shape == (self.batch_size, max_y))
    105 

AssertionError: 

During handling of the above exception, another exception occurred:

AssertionError                            Traceback (most recent call last)
<ipython-input-19-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-17-8c64b3d0f577> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    175             batch_index = 0
    176             while steps_done < steps_per_epoch:
--> 177                 generator_output = next(output_generator)
    178 
    179                 if not hasattr(generator_output, '__len__'):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in get(self)
    791             success, value = self.queue.get()
    792             if not success:
--> 793                 six.reraise(value.__class__, value, value.__traceback__)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in _data_generator_task(self)
    656                             # => Serialize calls to
    657                             # infinite iterator/generator's next() function
--> 658                             generator_output = next(self._generator)
    659                             self.queue.put((True, generator_output))
    660                         else:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    154                 print("data error - this shouldn't happen - try next batch")
    155                 self.cur_index += 1
--> 156                 ret = self.get_batch(self.cur_index)
    157 
    158             self.cur_index += 1

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
    102         # print("2. labels shape:", labels.shape)
    103         # print("2. labels values=", labels)
--> 104         assert(labels.shape == (self.batch_size, max_y))
    105 
    106         # 3. input_length (required for CTC loss)

AssertionError: 
2018-12-13 13:58:57,264 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 13:58:57,264 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'f68516f9346d47d6ad7cff6c0a14cfd6', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 12, 56, 52, 734538, tzinfo=tzutc())}, 'msg_id': 'f68516f9346d47d6ad7cff6c0a14cfd6', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'from keras.preprocessing.sequence import pad_sequences\ndf_train = data[0:int(0.9 * len(sentences_onehot))]\nbatch_x = df_train["X"][0:16]\n\n\nx_val = [len(item) for item in batch_x]\nmax_val = max(x_val)\nl = []\n\nfor item in batch_x:\n    s = np.transpose(item)\n    l.append(pad_sequences(item, maxlen = max_val, dtype=\'int\',padding=\'post\',truncating=\'post\'))\n    \nX_data = np.array(l)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 13:58:57,265 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 13:58:57,265 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '7280828754e2453e9a22b7a1d12dfa0e', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 12, 56, 52, 735552, tzinfo=tzutc())}, 'msg_id': '7280828754e2453e9a22b7a1d12dfa0e', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': "X_data = np.array([np.transpose(pad_sequences(np.transpose(item), maxlen = max_val, dtype='int',padding='post',truncating='post')) for item in batch_x])", 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 13:58:57,267 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 13:58:57,267 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'd48fcc5cb7f64a5bb5b8feabae0ca198', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 12, 56, 52, 737550, tzinfo=tzutc())}, 'msg_id': 'd48fcc5cb7f64a5bb5b8feabae0ca198', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'X_data[0].shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 13:58:57,268 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 13:58:57,268 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'fbc7970e206742469557a5838606f4a5', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 12, 56, 52, 738549, tzinfo=tzutc())}, 'msg_id': 'fbc7970e206742469557a5838606f4a5', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'batch_x[0].shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 14:00:39,275 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-20-1593b401035a> in <module>()
----> 1 batch_y = df_train["Y"][0:16]
      2 y_val = [len(l) for l in batch_y]
      3 max_y = max(y_val)
      4 labels = np.array([np.transpose(pad_sequences(np.transpose(item), maxlen = max_y, dtype='int',padding='post',truncating='post')) for item in batch_x])
      5 labels.shape

NameError: name 'df_train' is not defined
2018-12-13 16:32:40,499 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-27-4ff96cd972be> in <module>()
----> 1 list(alphabets.keys())[list(alphabets.values()).index(50)]

ValueError: 50 is not in list
Using TensorFlow backend.
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=5625, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=624, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-13 16:40:12,605 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-19-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-17-8c64b3d0f577> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    211                 outs = model.train_on_batch(x, y,
    212                                             sample_weight=sample_weight,
--> 213                                             class_weight=class_weight)
    214 
    215                 outs = to_list(outs)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in train_on_batch(self, x, y, sample_weight, class_weight)
   1207             x, y,
   1208             sample_weight=sample_weight,
-> 1209             class_weight=class_weight)
   1210         if self._uses_dynamic_learning_phase():
   1211             ins = x + y + sample_weights + [1.]

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)
    747             feed_input_shapes,
    748             check_batch_axis=False,  # Don't enforce the batch size.
--> 749             exception_prefix='input')
    750 
    751         if y is not None:

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    125                         ': expected ' + names[i] + ' to have ' +
    126                         str(len(shape)) + ' dimensions, but got array '
--> 127                         'with shape ' + str(data_shape))
    128                 if not check_batch_axis:
    129                     data_shape = data_shape[1:]

ValueError: Error when checking input: expected the_labels to have 2 dimensions, but got array with shape (16, 92, 46)
2018-12-13 16:40:12,608 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 16:40:12,608 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'd715ecab718c47a7945c7a2ddcd35efb', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 15, 37, 47, 203705, tzinfo=tzutc())}, 'msg_id': 'd715ecab718c47a7945c7a2ddcd35efb', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'from keras.preprocessing.sequence import pad_sequences\ndf_train = data[0:int(0.9 * len(sentences_onehot))]\nbatch_x = df_train["X"][0:16]\n\n\nx_val = [len(item) for item in batch_x]\nmax_val = max(x_val)\nl = []\n\nfor item in batch_x:\n    s = np.transpose(item)\n    l.append(pad_sequences(item, maxlen = max_val, dtype=\'int\',padding=\'post\',truncating=\'post\'))\n    \nX_data = np.array(l)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 16:40:12,609 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 16:40:12,610 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '576c4177af134cc8a4899d10fbf42c6e', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 15, 37, 47, 205726, tzinfo=tzutc())}, 'msg_id': '576c4177af134cc8a4899d10fbf42c6e', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': "X_data = np.array([np.transpose(pad_sequences(np.transpose(item), maxlen = max_val, dtype='int',padding='post',truncating='post')) for item in batch_x])", 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 16:40:12,611 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 16:40:12,611 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '7d31e1f059784b89844017c24a10b933', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 15, 37, 47, 209722, tzinfo=tzutc())}, 'msg_id': '7d31e1f059784b89844017c24a10b933', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'X_data[0].shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 16:40:12,613 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 16:40:12,613 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'd02a9e1f3a6e4078bec40f3ab7d88e99', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 15, 37, 47, 211224, tzinfo=tzutc())}, 'msg_id': 'd02a9e1f3a6e4078bec40f3ab7d88e99', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'batch_x[0].shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 16:40:12,615 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 16:40:12,615 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'cc0834cd2f7344c087b50243d1da1f5c', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 15, 37, 47, 212731, tzinfo=tzutc())}, 'msg_id': 'cc0834cd2f7344c087b50243d1da1f5c', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'batch_y = df_train["Y"][0:16]\ny_val = [len(l) for l in batch_y]\nmax_y = max(y_val)\nlabels = np.array([np.transpose(pad_sequences(np.transpose(item), maxlen = max_y, dtype=\'int\',padding=\'post\',truncating=\'post\')) for item in batch_x])\nlabels.shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-13 16:40:12,617 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-13 16:40:12,617 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '95a05a20250d4d838566940d1d69dfc4', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 13, 15, 37, 47, 216836, tzinfo=tzutc())}, 'msg_id': '95a05a20250d4d838566940d1d69dfc4', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'list(alphabets.keys())[list(alphabets.values()).index(1)]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
Using TensorFlow backend.
2018-12-14 10:27:07,371 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-15-b2f08c7db5dc> in <module>()
      4 data = pd.DataFrame(
      5     {'X': sentences_onehot,
----> 6      'Y': sentences_idxs
      7     })
      8 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy)
    346                                  dtype=dtype, copy=copy)
    347         elif isinstance(data, dict):
--> 348             mgr = self._init_dict(data, index, columns, dtype=dtype)
    349         elif isinstance(data, ma.MaskedArray):
    350             import numpy.ma.mrecords as mrecords

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in _init_dict(self, data, index, columns, dtype)
    457             arrays = [data[k] for k in keys]
    458 
--> 459         return _arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)
    460 
    461     def _init_ndarray(self, values, index, columns, dtype=None, copy=False):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in _arrays_to_mgr(arrays, arr_names, index, columns, dtype)
   7354     # figure out the index, if necessary
   7355     if index is None:
-> 7356         index = extract_index(arrays)
   7357 
   7358     # don't force copy because getting jammed in an ndarray anyway

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in extract_index(data)
   7400             lengths = list(set(raw_lengths))
   7401             if len(lengths) > 1:
-> 7402                 raise ValueError('arrays must all be same length')
   7403 
   7404             if have_dicts:

ValueError: arrays must all be same length
2018-12-14 10:27:07,374 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 10:27:07,374 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '52ffb60ca5ea47ca8f3415cd6405fec0', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 9, 25, 10, 163999, tzinfo=tzutc())}, 'msg_id': '52ffb60ca5ea47ca8f3415cd6405fec0', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'len(data["X"][1])', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 10:27:07,376 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 10:27:07,376 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '5a6c824a0ef9400f8bf34118eae272e1', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 9, 25, 10, 166505, tzinfo=tzutc())}, 'msg_id': '5a6c824a0ef9400f8bf34118eae272e1', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': '#####################################################\n\nimport os\n\nimport keras\nfrom keras.callbacks import TensorBoard\nfrom keras.optimizers import Adam, Nadam\n\n#from KerasDeepSpeech.data import combine_all_wavs_and_trans_from_csvs\nfrom KerasDeepSpeech.generator import BatchGenerator\nfrom KerasDeepSpeech.model import *\nfrom KerasDeepSpeech.report import ReportCallback\nfrom KerasDeepSpeech.utils import load_model_checkpoint, save_model, MemoryCallback\n\n#####################################################\n\n\n#######################################################\n\n# Prevent pool_allocator message\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n#######################################################\n\n\ndef main(args):\n    \'\'\'\n    There are 5 simple steps to this program\n    \'\'\'\n\n    #1. combine all data into 2 dataframes (train, valid)\n    print("Getting data from arguments")\n    #train_dataprops, df_train = combine_all_wavs_and_trans_from_csvs(args.train_files, sortagrad=args.sortagrad)\n    #valid_dataprops, df_valid = combine_all_wavs_and_trans_from_csvs(args.valid_files, sortagrad=args.sortagrad)\n\n    train_ratio = 0.9\n    args.model_arch = 0\n    args.opt = "adam"\n    args.train_steps = 0\n    args.epochs = 50\n    args.valid_steps = 0\n    args.batchsize = 16\n    args.name = ""\n    args.loadcheckpointpath = ""\n    args.fc_size = 512\n    args.rnn_size = 512\n    args.learning_rate = 0.01\n    args.memcheck = False\n    args.tensorboard = True\n    \n    model_input_type = "text"\n    \n    \n    df_train = data[0:int(train_ratio * len(sentences_onehot))]\n    df_valid = data[int(train_ratio * len(sentences_onehot)) + 1:]\n\n\n    ## 2. init data generators\n    print("Creating data batch generators")\n    traindata = BatchGenerator(dataframe=df_train, dataproperties=None,\n                              training=True, batch_size=args.batchsize, model_input_type=model_input_type)\n    validdata = BatchGenerator(dataframe=df_valid, dataproperties=None,\n                              training=False, batch_size=args.batchsize, model_input_type=model_input_type)\n\n\n\n\n    output_dir = os.path.join(\'checkpoints/results\',\n                                  \'model%s_%s\' % (args.model_arch,\n                                             args.name))\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir)\n\n\n    ## 3. Load existing or create new model\n    if args.loadcheckpointpath:\n        # load existing\n        print("Loading model")\n\n        cp = args.loadcheckpointpath\n        assert(os.path.isdir(cp))\n\n        model_path = os.path.join(cp, "model")\n        # assert(os.path.isfile(model_path))\n\n        model = load_model_checkpoint(model_path)\n\n\n        print("Model loaded")\n    else:\n        # new model recipes here\n        print(\'New model DS{}\'.format(args.model_arch))\n        if (args.model_arch == 0):\n            # DeepSpeech1 with Dropout\n            model = ds1_dropout(input_dim=len(alphabets), fc_size=args.fc_size, rnn_size=args.rnn_size,dropout=[0.1,0.1,0.1], output_dim=len(alphabets))\n\n        elif(args.model_arch==1):\n            # DeepSpeech1 - no dropout\n            model = ds1(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n\n        elif(args.model_arch==2):\n            # DeepSpeech2 model\n            model = ds2_gru_model(input_dim=161, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n\n        elif(args.model_arch==3):\n            # own model\n            model = ownModel(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, dropout=[0.1, 0.1, 0.1], output_dim=29)\n\n        elif(args.model_arch==4):\n            # graves model\n            model = graves(input_dim=26, rnn_size=args.rnn_size, output_dim=29, std=0.5)\n\n        elif(args.model_arch==5):\n            #cnn city\n            model = cnn_city(input_dim=161, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n\n        elif(args.model_arch == 6):\n            # constrained model\n            model = const(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n        else:\n            raise("model not found")\n\n        print(model.summary(line_length=80))\n\n        #required to save the JSON\n        save_model(model, output_dir)\n\n    if (args.opt.lower() == \'sgd\'):\n        opt = SGD(lr=args.learning_rate, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n    elif (args.opt.lower() == \'adam\'):\n        opt = Adam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n    elif (args.opt.lower() == \'nadam\'):\n        opt = Nadam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n    else:\n        raise "optimiser not recognised"\n\n    model.compile(optimizer=opt, loss=ctc)\n\n    ## 4. train\n\n    if args.train_steps == 0:\n        args.train_steps = len(df_train.index) // args.batchsize\n        # print(args.train_steps)\n    # we use 1/xth of the validation data at each epoch end to test val score\n    if args.valid_steps == 0:\n\n        args.valid_steps = (len(df_valid.index) // args.batchsize)\n        # print(args.valid_steps)\n\n\n    if args.memcheck:\n        cb_list = [MemoryCallback()]\n    else:\n        cb_list = []\n\n    if args.tensorboard:\n        tb_cb = TensorBoard(log_dir=\'./tensorboard/{}/\'.format(args.name), write_graph=False, write_images=True)\n        cb_list.append(tb_cb)\n\n    y_pred = model.get_layer(\'ctc\').input[0]\n    input_data = model.get_layer(\'the_input\').input\n\n    report = K.function([input_data, K.learning_phase()], [y_pred])\n    report_cb = ReportCallback(report, validdata, model, args.name, save=True)\n\n    cb_list.append(report_cb)\n\n    model.fit_generator(generator=traindata.next_batch(),\n                        steps_per_epoch=args.train_steps,\n                        epochs=args.epochs,\n                        callbacks=cb_list,\n                        validation_data=validdata.next_batch(),\n                        validation_steps=args.valid_steps,\n                        initial_epoch=0,\n                        verbose=1,\n                        class_weight=None,\n                        max_q_size=10,\n                        workers=1,\n                        pickle_safe=False\n                        )\n\n    # K.clear_session()\n\n    ## These are the most important metrics\n    print("Mean WER   :", report_cb.mean_wer_log)\n    print("Mean LER   :", report_cb.mean_ler_log)\n    print("NormMeanLER:", report_cb.norm_mean_ler_log)\n\n    # export to csv?\n    K.clear_session()', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 10:27:07,377 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 10:27:07,378 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'c3ff28329e984fae80ac013aecca174f', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 9, 25, 10, 167008, tzinfo=tzutc())}, 'msg_id': 'c3ff28329e984fae80ac013aecca174f', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'class Object(object):\n    pass\n\nargs = Object()', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 10:27:07,379 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 10:27:07,379 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '2dceeddaf3304c00a5ad134fda172bbf', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 9, 25, 10, 170015, tzinfo=tzutc())}, 'msg_id': '2dceeddaf3304c00a5ad134fda172bbf', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'main(args)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 10:27:07,381 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 10:27:07,381 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '53d5aa9568c3452faa73e730163eb8bb', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 9, 25, 10, 171017, tzinfo=tzutc())}, 'msg_id': '53d5aa9568c3452faa73e730163eb8bb', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'from keras.preprocessing.sequence import pad_sequences\ndf_train = data[0:int(0.9 * len(sentences_onehot))]\nbatch_x = df_train["X"][0:16]\n\n\nx_val = [len(item) for item in batch_x]\nmax_val = max(x_val)\nl = []\n\nfor item in batch_x:\n    s = np.transpose(item)\n    l.append(pad_sequences(item, maxlen = max_val, dtype=\'int\',padding=\'post\',truncating=\'post\'))\n    \nX_data = np.array(l)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 10:27:07,382 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 10:27:07,383 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '4414cbf2a6364ea882a93deeb12bd2bd', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 9, 25, 10, 171017, tzinfo=tzutc())}, 'msg_id': '4414cbf2a6364ea882a93deeb12bd2bd', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': "X_data = np.array([np.transpose(pad_sequences(np.transpose(item), maxlen = max_val, dtype='int',padding='post',truncating='post')) for item in batch_x])", 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 10:27:07,384 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 10:27:07,384 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'a3f636fdcae640bc8df25d2e1fd96fd9', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 9, 25, 10, 171519, tzinfo=tzutc())}, 'msg_id': 'a3f636fdcae640bc8df25d2e1fd96fd9', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'X_data[0].shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 10:27:07,385 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 10:27:07,385 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '38f8ddeceb33422fbcbf288bac68f4b1', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 9, 25, 10, 173021, tzinfo=tzutc())}, 'msg_id': '38f8ddeceb33422fbcbf288bac68f4b1', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'batch_x[0].shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 10:27:07,387 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 10:27:07,387 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '2a4573b0428f45caacb4b438af9a2394', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 9, 25, 10, 174023, tzinfo=tzutc())}, 'msg_id': '2a4573b0428f45caacb4b438af9a2394', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'batch_y = df_train["Y"][0:16]\ny_val = [len(l) for l in batch_y]\nmax_y = max(y_val)\nlabels = np.array([np.transpose(pad_sequences(np.transpose(item), maxlen = max_y, dtype=\'int\',padding=\'post\',truncating=\'post\')) for item in batch_x])\nlabels.shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 10:27:07,389 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 10:27:07,389 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'ddc673614aea42428ac5d3c59ef43465', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 9, 25, 10, 175611, tzinfo=tzutc())}, 'msg_id': 'ddc673614aea42428ac5d3c59ef43465', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'list(alphabets.keys())[list(alphabets.values()).index(1)]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 10:42:00,815 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-16-77ca22ab64cd> in <module>()
      4 data = pd.DataFrame(
      5     {'X': sentences_onehot,
----> 6      'Y': sentences
      7     })
      8 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy)
    346                                  dtype=dtype, copy=copy)
    347         elif isinstance(data, dict):
--> 348             mgr = self._init_dict(data, index, columns, dtype=dtype)
    349         elif isinstance(data, ma.MaskedArray):
    350             import numpy.ma.mrecords as mrecords

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in _init_dict(self, data, index, columns, dtype)
    457             arrays = [data[k] for k in keys]
    458 
--> 459         return _arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)
    460 
    461     def _init_ndarray(self, values, index, columns, dtype=None, copy=False):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in _arrays_to_mgr(arrays, arr_names, index, columns, dtype)
   7354     # figure out the index, if necessary
   7355     if index is None:
-> 7356         index = extract_index(arrays)
   7357 
   7358     # don't force copy because getting jammed in an ndarray anyway

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in extract_index(data)
   7400             lengths = list(set(raw_lengths))
   7401             if len(lengths) > 1:
-> 7402                 raise ValueError('arrays must all be same length')
   7403 
   7404             if have_dicts:

ValueError: arrays must all be same length
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=5625, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=624, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-14 10:43:34,054 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    150             try:
--> 151                 ret = self.get_batch(self.cur_index)
    152             except:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
     98         #labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])
---> 99         labels = np.array([pad_sequences(item, maxlen = max_y, dtype='int',padding='post',truncating='post') for item in batch_y_trans])
    100         print("2. labels shape:", labels.shape)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in <listcomp>(.0)
     98         #labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])
---> 99         labels = np.array([pad_sequences(item, maxlen = max_y, dtype='int',padding='post',truncating='post') for item in batch_y_trans])
    100         print("2. labels shape:", labels.shape)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras_preprocessing\sequence.py in pad_sequences(sequences, maxlen, dtype, padding, truncating, value)
     59             raise ValueError('`sequences` must be a list of iterables. '
---> 60                              'Found non-iterable: ' + str(x))
     61         lengths.append(len(x))

ValueError: `sequences` must be a list of iterables. Found non-iterable: 43

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-23-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-21-8c64b3d0f577> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    175             batch_index = 0
    176             while steps_done < steps_per_epoch:
--> 177                 generator_output = next(output_generator)
    178 
    179                 if not hasattr(generator_output, '__len__'):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in get(self)
    791             success, value = self.queue.get()
    792             if not success:
--> 793                 six.reraise(value.__class__, value, value.__traceback__)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in _data_generator_task(self)
    656                             # => Serialize calls to
    657                             # infinite iterator/generator's next() function
--> 658                             generator_output = next(self._generator)
    659                             self.queue.put((True, generator_output))
    660                         else:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    153                 print("data error - this shouldn't happen - try next batch")
    154                 self.cur_index += 1
--> 155                 ret = self.get_batch(self.cur_index)
    156 
    157             self.cur_index += 1

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
     97         print(max_y)
     98         #labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])
---> 99         labels = np.array([pad_sequences(item, maxlen = max_y, dtype='int',padding='post',truncating='post') for item in batch_y_trans])
    100         print("2. labels shape:", labels.shape)
    101         # print("2. labels values=", labels)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in <listcomp>(.0)
     97         print(max_y)
     98         #labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])
---> 99         labels = np.array([pad_sequences(item, maxlen = max_y, dtype='int',padding='post',truncating='post') for item in batch_y_trans])
    100         print("2. labels shape:", labels.shape)
    101         # print("2. labels values=", labels)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras_preprocessing\sequence.py in pad_sequences(sequences, maxlen, dtype, padding, truncating, value)
     58         if not hasattr(x, '__len__'):
     59             raise ValueError('`sequences` must be a list of iterables. '
---> 60                              'Found non-iterable: ' + str(x))
     61         lengths.append(len(x))
     62 

ValueError: `sequences` must be a list of iterables. Found non-iterable: 43
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=5625, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=624, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-14 11:18:08,590 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    150             try:
--> 151                 ret = self.get_batch(self.cur_index)
    152             except:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
     98         #labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])
---> 99         labels = np.array([pad_sequences(item, maxlen = max_y, dtype='int',padding='post',truncating='post') for item in batch_y_trans])
    100         print("2. labels shape:", labels.shape)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in <listcomp>(.0)
     98         #labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])
---> 99         labels = np.array([pad_sequences(item, maxlen = max_y, dtype='int',padding='post',truncating='post') for item in batch_y_trans])
    100         print("2. labels shape:", labels.shape)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras_preprocessing\sequence.py in pad_sequences(sequences, maxlen, dtype, padding, truncating, value)
     93         # check `trunc` has expected shape
---> 94         trunc = np.asarray(trunc, dtype=dtype)
     95         if trunc.shape[1:] != sample_shape:

~\AppData\Local\Continuum\anaconda3\lib\site-packages\numpy\core\numeric.py in asarray(a, dtype, order)
    500     """
--> 501     return array(a, dtype, copy=False, order=order)
    502 

ValueError: invalid literal for int() with base 10: 'T'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-26-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-21-8c64b3d0f577> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    175             batch_index = 0
    176             while steps_done < steps_per_epoch:
--> 177                 generator_output = next(output_generator)
    178 
    179                 if not hasattr(generator_output, '__len__'):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in get(self)
    791             success, value = self.queue.get()
    792             if not success:
--> 793                 six.reraise(value.__class__, value, value.__traceback__)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in _data_generator_task(self)
    656                             # => Serialize calls to
    657                             # infinite iterator/generator's next() function
--> 658                             generator_output = next(self._generator)
    659                             self.queue.put((True, generator_output))
    660                         else:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    153                 print("data error - this shouldn't happen - try next batch")
    154                 self.cur_index += 1
--> 155                 ret = self.get_batch(self.cur_index)
    156 
    157             self.cur_index += 1

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
     97         print(max_y)
     98         #labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])
---> 99         labels = np.array([pad_sequences(item, maxlen = max_y, dtype='int',padding='post',truncating='post') for item in batch_y_trans])
    100         print("2. labels shape:", labels.shape)
    101         # print("2. labels values=", labels)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in <listcomp>(.0)
     97         print(max_y)
     98         #labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])
---> 99         labels = np.array([pad_sequences(item, maxlen = max_y, dtype='int',padding='post',truncating='post') for item in batch_y_trans])
    100         print("2. labels shape:", labels.shape)
    101         # print("2. labels values=", labels)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras_preprocessing\sequence.py in pad_sequences(sequences, maxlen, dtype, padding, truncating, value)
     92 
     93         # check `trunc` has expected shape
---> 94         trunc = np.asarray(trunc, dtype=dtype)
     95         if trunc.shape[1:] != sample_shape:
     96             raise ValueError('Shape of sample %s of sequence at position %s '

~\AppData\Local\Continuum\anaconda3\lib\site-packages\numpy\core\numeric.py in asarray(a, dtype, order)
    499 
    500     """
--> 501     return array(a, dtype, copy=False, order=order)
    502 
    503 

ValueError: invalid literal for int() with base 10: 'Q'
Using TensorFlow backend.
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=5625, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=624, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-14 11:21:46,311 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    150             try:
--> 151                 ret = self.get_batch(self.cur_index)
    152             except:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
     97         print(max_y)
---> 98         labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])
     99         #labels = np.array([pad_sequences(item, maxlen = max_y, dtype='int',padding='post',truncating='post') for item in batch_y_trans])

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in <listcomp>(.0)
     97         print(max_y)
---> 98         labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])
     99         #labels = np.array([pad_sequences(item, maxlen = max_y, dtype='int',padding='post',truncating='post') for item in batch_y_trans])

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_intseq(trans, max_intseq_length)
    211     # PAD
--> 212     t = text_to_int_sequence(trans)
    213     while (len(t) < max_intseq_length):

~\Documents\GitHub\Textfixer\KerasDeepSpeech\utils.py in text_to_int_sequence(text)
     42     #    int_sequence.append(ch)
---> 43     return [alphabets[ch] if ch in alphabets else 43 for ch in text.lower]
     44     #return int_sequence

TypeError: 'builtin_function_or_method' object is not iterable

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-19-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-17-8c64b3d0f577> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    175             batch_index = 0
    176             while steps_done < steps_per_epoch:
--> 177                 generator_output = next(output_generator)
    178 
    179                 if not hasattr(generator_output, '__len__'):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in get(self)
    791             success, value = self.queue.get()
    792             if not success:
--> 793                 six.reraise(value.__class__, value, value.__traceback__)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in _data_generator_task(self)
    656                             # => Serialize calls to
    657                             # infinite iterator/generator's next() function
--> 658                             generator_output = next(self._generator)
    659                             self.queue.put((True, generator_output))
    660                         else:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    153                 print("data error - this shouldn't happen - try next batch")
    154                 self.cur_index += 1
--> 155                 ret = self.get_batch(self.cur_index)
    156 
    157             self.cur_index += 1

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
     96         max_y = max(y_val)
     97         print(max_y)
---> 98         labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])
     99         #labels = np.array([pad_sequences(item, maxlen = max_y, dtype='int',padding='post',truncating='post') for item in batch_y_trans])
    100         print("2. labels shape:", labels.shape)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in <listcomp>(.0)
     96         max_y = max(y_val)
     97         print(max_y)
---> 98         labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])
     99         #labels = np.array([pad_sequences(item, maxlen = max_y, dtype='int',padding='post',truncating='post') for item in batch_y_trans])
    100         print("2. labels shape:", labels.shape)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_intseq(trans, max_intseq_length)
    210 def get_intseq(trans, max_intseq_length=80):
    211     # PAD
--> 212     t = text_to_int_sequence(trans)
    213     while (len(t) < max_intseq_length):
    214         t.append(36)  # replace with a space char to pad

~\Documents\GitHub\Textfixer\KerasDeepSpeech\utils.py in text_to_int_sequence(text)
     41     #        ch = char_map[c]
     42     #    int_sequence.append(ch)
---> 43     return [alphabets[ch] if ch in alphabets else 43 for ch in text.lower]
     44     #return int_sequence
     45 

TypeError: 'builtin_function_or_method' object is not iterable
2018-12-14 11:21:46,314 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:21:46,315 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'e91b46c3ae074d0288620e3bab94551b', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 19, 33, 235283, tzinfo=tzutc())}, 'msg_id': 'e91b46c3ae074d0288620e3bab94551b', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'from keras.preprocessing.sequence import pad_sequences\ndf_train = data[0:int(0.9 * len(sentences_onehot))]\nbatch_x = df_train["X"][0:16]\n\n\nx_val = [len(item) for item in batch_x]\nmax_val = max(x_val)\nl = []\n\nfor item in batch_x:\n    s = np.transpose(item)\n    l.append(pad_sequences(item, maxlen = max_val, dtype=\'int\',padding=\'post\',truncating=\'post\'))\n    \nX_data = np.array(l)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 11:21:46,316 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:21:46,316 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'bda0a444bf414e198bbb052f9fc9d127', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 19, 33, 236286, tzinfo=tzutc())}, 'msg_id': 'bda0a444bf414e198bbb052f9fc9d127', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': "X_data = np.array([np.transpose(pad_sequences(np.transpose(item), maxlen = max_val, dtype='int',padding='post',truncating='post')) for item in batch_x])", 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 11:21:46,317 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:21:46,317 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '98adab3a969741ea89873d1255cd828d', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 19, 33, 237288, tzinfo=tzutc())}, 'msg_id': '98adab3a969741ea89873d1255cd828d', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'X_data[0].shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 11:21:46,319 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:21:46,319 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '1161e6ce620f43f78ab74a1fd6a3fa47', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 19, 33, 239294, tzinfo=tzutc())}, 'msg_id': '1161e6ce620f43f78ab74a1fd6a3fa47', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'batch_x[0].shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 11:21:46,321 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:21:46,321 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '40fa1abfe35646aebe7c91766ee008db', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 19, 33, 241302, tzinfo=tzutc())}, 'msg_id': '40fa1abfe35646aebe7c91766ee008db', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'batch_y = df_train["Y"][0:16]\ny_val = [len(l) for l in batch_y]\nmax_y = max(y_val)\nlabels = np.array([np.transpose(pad_sequences(np.transpose(item), maxlen = max_y, dtype=\'int\',padding=\'post\',truncating=\'post\')) for item in batch_x])\nlabels.shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 11:21:46,322 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:21:46,322 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'b496d6f0d59941aea73338aa6b5c6d56', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 19, 33, 242302, tzinfo=tzutc())}, 'msg_id': 'b496d6f0d59941aea73338aa6b5c6d56', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'list(alphabets.keys())[list(alphabets.values()).index(1)]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=5625, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=624, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-14 11:33:58,963 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    150             try:
--> 151                 ret = self.get_batch(self.cur_index)
    152             except:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
     97         print(max_y)
---> 98         labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])
     99         #labels = np.array([pad_sequences(item, maxlen = max_y, dtype='int',padding='post',truncating='post') for item in batch_y_trans])

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in <listcomp>(.0)
     97         print(max_y)
---> 98         labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])
     99         #labels = np.array([pad_sequences(item, maxlen = max_y, dtype='int',padding='post',truncating='post') for item in batch_y_trans])

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_intseq(trans, max_intseq_length)
    211     # PAD
--> 212     t = text_to_int_sequence(trans)
    213     while (len(t) < max_intseq_length):

~\Documents\GitHub\Textfixer\KerasDeepSpeech\utils.py in text_to_int_sequence(text)
     42     #    int_sequence.append(ch)
---> 43     return [alphabets[ch] if ch in alphabets else 43 for ch in text.lower()]
     44     #return int_sequence

TypeError: 'builtin_function_or_method' object is not iterable

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-20-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-17-8c64b3d0f577> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    175             batch_index = 0
    176             while steps_done < steps_per_epoch:
--> 177                 generator_output = next(output_generator)
    178 
    179                 if not hasattr(generator_output, '__len__'):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in get(self)
    791             success, value = self.queue.get()
    792             if not success:
--> 793                 six.reraise(value.__class__, value, value.__traceback__)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in _data_generator_task(self)
    656                             # => Serialize calls to
    657                             # infinite iterator/generator's next() function
--> 658                             generator_output = next(self._generator)
    659                             self.queue.put((True, generator_output))
    660                         else:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    153                 print("data error - this shouldn't happen - try next batch")
    154                 self.cur_index += 1
--> 155                 ret = self.get_batch(self.cur_index)
    156 
    157             self.cur_index += 1

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
     96         max_y = max(y_val)
     97         print(max_y)
---> 98         labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])
     99         #labels = np.array([pad_sequences(item, maxlen = max_y, dtype='int',padding='post',truncating='post') for item in batch_y_trans])
    100         print("2. labels shape:", labels.shape)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in <listcomp>(.0)
     96         max_y = max(y_val)
     97         print(max_y)
---> 98         labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])
     99         #labels = np.array([pad_sequences(item, maxlen = max_y, dtype='int',padding='post',truncating='post') for item in batch_y_trans])
    100         print("2. labels shape:", labels.shape)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_intseq(trans, max_intseq_length)
    210 def get_intseq(trans, max_intseq_length=80):
    211     # PAD
--> 212     t = text_to_int_sequence(trans)
    213     while (len(t) < max_intseq_length):
    214         t.append(36)  # replace with a space char to pad

~\Documents\GitHub\Textfixer\KerasDeepSpeech\utils.py in text_to_int_sequence(text)
     41     #        ch = char_map[c]
     42     #    int_sequence.append(ch)
---> 43     return [alphabets[ch] if ch in alphabets else 43 for ch in text.lower()]
     44     #return int_sequence
     45 

TypeError: 'builtin_function_or_method' object is not iterable
Using TensorFlow backend.
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=5625, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=624, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-14 11:36:25,652 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    150             try:
--> 151                 ret = self.get_batch(self.cur_index)
    152             except:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
    102         #assert(labels.shape == (self.batch_size, max_y))
--> 103         assert(labels.shape == (self.batch_size, max_y, 46))
    104 

AssertionError: 

During handling of the above exception, another exception occurred:

AssertionError                            Traceback (most recent call last)
<ipython-input-19-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-17-8c64b3d0f577> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    175             batch_index = 0
    176             while steps_done < steps_per_epoch:
--> 177                 generator_output = next(output_generator)
    178 
    179                 if not hasattr(generator_output, '__len__'):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in get(self)
    791             success, value = self.queue.get()
    792             if not success:
--> 793                 six.reraise(value.__class__, value, value.__traceback__)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\data_utils.py in _data_generator_task(self)
    656                             # => Serialize calls to
    657                             # infinite iterator/generator's next() function
--> 658                             generator_output = next(self._generator)
    659                             self.queue.put((True, generator_output))
    660                         else:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in next_batch(self)
    153                 print("data error - this shouldn't happen - try next batch")
    154                 self.cur_index += 1
--> 155                 ret = self.get_batch(self.cur_index)
    156 
    157             self.cur_index += 1

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in get_batch(self, idx)
    101         # print("2. labels values=", labels)
    102         #assert(labels.shape == (self.batch_size, max_y))
--> 103         assert(labels.shape == (self.batch_size, max_y, 46))
    104 
    105         # 3. input_length (required for CTC loss)

AssertionError: 
2018-12-14 11:36:25,655 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:36:25,655 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '23acd7542d9247a3887c9bf8b697ce9d', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 34, 11, 673812, tzinfo=tzutc())}, 'msg_id': '23acd7542d9247a3887c9bf8b697ce9d', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'from keras.preprocessing.sequence import pad_sequences\ndf_train = data[0:int(0.9 * len(sentences_onehot))]\nbatch_x = df_train["X"][0:16]\n\n\nx_val = [len(item) for item in batch_x]\nmax_val = max(x_val)\nl = []\n\nfor item in batch_x:\n    s = np.transpose(item)\n    l.append(pad_sequences(item, maxlen = max_val, dtype=\'int\',padding=\'post\',truncating=\'post\'))\n    \nX_data = np.array(l)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 11:36:25,656 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:36:25,656 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'd9cf8dfd474443ed851294292980367c', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 34, 11, 675844, tzinfo=tzutc())}, 'msg_id': 'd9cf8dfd474443ed851294292980367c', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': "X_data = np.array([np.transpose(pad_sequences(np.transpose(item), maxlen = max_val, dtype='int',padding='post',truncating='post')) for item in batch_x])", 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 11:36:25,658 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:36:25,658 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'b9d1a2c858bc4e98b418aaf2685bd366', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 34, 11, 676847, tzinfo=tzutc())}, 'msg_id': 'b9d1a2c858bc4e98b418aaf2685bd366', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'X_data[0].shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 11:36:25,660 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:36:25,660 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '80b1280f6e0043a982f173602297411e', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 34, 11, 677823, tzinfo=tzutc())}, 'msg_id': '80b1280f6e0043a982f173602297411e', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'batch_x[0].shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 11:36:25,661 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:36:25,661 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '6f790cef948640e18aa1892f5ac71695', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 34, 11, 678826, tzinfo=tzutc())}, 'msg_id': '6f790cef948640e18aa1892f5ac71695', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'batch_y = df_train["Y"][0:16]\ny_val = [len(l) for l in batch_y]\nmax_y = max(y_val)\nlabels = np.array([np.transpose(pad_sequences(np.transpose(item), maxlen = max_y, dtype=\'int\',padding=\'post\',truncating=\'post\')) for item in batch_x])\nlabels.shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 11:36:25,663 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:36:25,663 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '28b338e5c33f499d82be900a13c4aa2b', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 34, 11, 682836, tzinfo=tzutc())}, 'msg_id': '28b338e5c33f499d82be900a13c4aa2b', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'list(alphabets.keys())[list(alphabets.values()).index(1)]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
Using TensorFlow backend.
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=5625, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=624, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-14 11:50:49,951 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-19-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-17-8c64b3d0f577> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    211                 outs = model.train_on_batch(x, y,
    212                                             sample_weight=sample_weight,
--> 213                                             class_weight=class_weight)
    214 
    215                 outs = to_list(outs)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in train_on_batch(self, x, y, sample_weight, class_weight)
   1213             ins = x + y + sample_weights
   1214         self._make_train_function()
-> 1215         outputs = self.train_function(ins)
   1216         return unpack_singleton(outputs)
   1217 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in __call__(self, inputs)
   2664                 return self._legacy_call(inputs)
   2665 
-> 2666             return self._call(inputs)
   2667         else:
   2668             if py_any(is_tensor(x) for x in inputs):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in _call(self, inputs)
   2634                                 symbol_vals,
   2635                                 session)
-> 2636         fetched = self._callable_fn(*array_vals)
   2637         return fetched[:len(self.outputs)]
   2638 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\client\session.py in __call__(self, *args, **kwargs)
   1437           ret = tf_session.TF_SessionRunCallable(
   1438               self._session._session, self._handle, args, status,
-> 1439               run_metadata_ptr)
   1440         if run_metadata:
   1441           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    526             None, None,
    527             compat.as_text(c_api.TF_Message(self.status.status)),
--> 528             c_api.TF_GetCode(self.status.status))
    529     # Delete the underlying status object from memory otherwise it stays alive
    530     # as there is a reference to status from this from the traceback due to

InvalidArgumentError: Saw a non-null label (index >= num_classes - 1) following a null label, batch: 6 num_classes: 46 labels: 8,13,36,12,0,13,24,36,18,14,20,19,7,4,0,18,19,36,0,18,8,0,13,36,2,20,11,19,20,17,4,18,37,36,13,4,22,36,24,4,0,17,36,8,18,36,2,4,11,4,1,17,0,19,4,3,36,8,13,36,19,7,8,18,36,12,14,13,19,7,36,44,8,13,2,11,20,3,8,13,6,36,18,14,13,6,10,17,0,13
	 [[{{node ctc/CTCLoss}} = CTCLoss[_class=["loc:@training/Adam/gradients/ctc/CTCLoss_grad/mul"], ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device="/job:localhost/replica:0/task:0/device:CPU:0"](ctc/Log, ctc/ToInt64, ctc/GatherNd, ctc/Squeeze_1)]]
2018-12-14 11:50:49,954 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:50:49,954 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '9c31fe0fdf714ef68c8830076fc70c95', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 48, 31, 246513, tzinfo=tzutc())}, 'msg_id': '9c31fe0fdf714ef68c8830076fc70c95', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'from keras.preprocessing.sequence import pad_sequences\ndf_train = data[0:int(0.9 * len(sentences_onehot))]\nbatch_x = df_train["X"][0:16]\n\n\nx_val = [len(item) for item in batch_x]\nmax_val = max(x_val)\nl = []\n\nfor item in batch_x:\n    s = np.transpose(item)\n    l.append(pad_sequences(item, maxlen = max_val, dtype=\'int\',padding=\'post\',truncating=\'post\'))\n    \nX_data = np.array(l)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 11:50:49,956 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:50:49,956 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '17c0c19cad8348b08308da26014abd59', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 48, 31, 247516, tzinfo=tzutc())}, 'msg_id': '17c0c19cad8348b08308da26014abd59', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': "X_data = np.array([np.transpose(pad_sequences(np.transpose(item), maxlen = max_val, dtype='int',padding='post',truncating='post')) for item in batch_x])", 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 11:50:49,957 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:50:49,957 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '48b0f14e93724aa39faf814d9c7da986', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 48, 31, 249020, tzinfo=tzutc())}, 'msg_id': '48b0f14e93724aa39faf814d9c7da986', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'X_data[0].shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 11:50:49,958 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:50:49,958 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'a1616cae609741f59a949d4501b8fcc7', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 48, 31, 250023, tzinfo=tzutc())}, 'msg_id': 'a1616cae609741f59a949d4501b8fcc7', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'batch_x[0].shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 11:50:49,960 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:50:49,960 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '40123959204a40058ba3bb5534b80d0d', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 48, 31, 251025, tzinfo=tzutc())}, 'msg_id': '40123959204a40058ba3bb5534b80d0d', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'batch_y = df_train["Y"][0:16]\ny_val = [len(l) for l in batch_y]\nmax_y = max(y_val)\nlabels = np.array([np.transpose(pad_sequences(np.transpose(item), maxlen = max_y, dtype=\'int\',padding=\'post\',truncating=\'post\')) for item in batch_x])\nlabels.shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 11:50:49,961 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 11:50:49,961 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '6c2446f5f22e444d83c7351ff59700de', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 10, 48, 31, 252028, tzinfo=tzutc())}, 'msg_id': '6c2446f5f22e444d83c7351ff59700de', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'list(alphabets.keys())[list(alphabets.values()).index(1)]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=5625, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=624, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-14 12:35:31,173 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
<ipython-input-21-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-20-117266548185> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    211                 outs = model.train_on_batch(x, y,
    212                                             sample_weight=sample_weight,
--> 213                                             class_weight=class_weight)
    214 
    215                 outs = to_list(outs)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in train_on_batch(self, x, y, sample_weight, class_weight)
   1213             ins = x + y + sample_weights
   1214         self._make_train_function()
-> 1215         outputs = self.train_function(ins)
   1216         return unpack_singleton(outputs)
   1217 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in __call__(self, inputs)
   2664                 return self._legacy_call(inputs)
   2665 
-> 2666             return self._call(inputs)
   2667         else:
   2668             if py_any(is_tensor(x) for x in inputs):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in _call(self, inputs)
   2634                                 symbol_vals,
   2635                                 session)
-> 2636         fetched = self._callable_fn(*array_vals)
   2637         return fetched[:len(self.outputs)]
   2638 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\client\session.py in __call__(self, *args, **kwargs)
   1437           ret = tf_session.TF_SessionRunCallable(
   1438               self._session._session, self._handle, args, status,
-> 1439               run_metadata_ptr)
   1440         if run_metadata:
   1441           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

KeyboardInterrupt: 
2018-12-14 12:35:31,176 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 12:35:31,176 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '4ed8daa3e77645c08c744b3e7da6b400', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'kernel_info_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 11, 35, 29, 403092, tzinfo=tzutc())}, 'msg_id': '4ed8daa3e77645c08c744b3e7da6b400', 'msg_type': 'kernel_info_request', 'parent_header': {}, 'metadata': {}, 'content': {}, 'buffers': []}
Using TensorFlow backend.
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=562, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=62, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-14 12:40:37,691 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
<ipython-input-19-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-17-117266548185> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    211                 outs = model.train_on_batch(x, y,
    212                                             sample_weight=sample_weight,
--> 213                                             class_weight=class_weight)
    214 
    215                 outs = to_list(outs)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in train_on_batch(self, x, y, sample_weight, class_weight)
   1213             ins = x + y + sample_weights
   1214         self._make_train_function()
-> 1215         outputs = self.train_function(ins)
   1216         return unpack_singleton(outputs)
   1217 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in __call__(self, inputs)
   2664                 return self._legacy_call(inputs)
   2665 
-> 2666             return self._call(inputs)
   2667         else:
   2668             if py_any(is_tensor(x) for x in inputs):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in _call(self, inputs)
   2634                                 symbol_vals,
   2635                                 session)
-> 2636         fetched = self._callable_fn(*array_vals)
   2637         return fetched[:len(self.outputs)]
   2638 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\client\session.py in __call__(self, *args, **kwargs)
   1437           ret = tf_session.TF_SessionRunCallable(
   1438               self._session._session, self._handle, args, status,
-> 1439               run_metadata_ptr)
   1440         if run_metadata:
   1441           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

KeyboardInterrupt: 
2018-12-14 12:40:37,694 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 12:40:37,694 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '86354d5fef7f4e54bea2f45dc43f7149', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 11, 36, 31, 97129, tzinfo=tzutc())}, 'msg_id': '86354d5fef7f4e54bea2f45dc43f7149', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'from keras.preprocessing.sequence import pad_sequences\ndf_train = data[0:int(0.9 * len(sentences_onehot))]\nbatch_x = df_train["X"][0:16]\n\n\nx_val = [len(item) for item in batch_x]\nmax_val = max(x_val)\nl = []\n\nfor item in batch_x:\n    s = np.transpose(item)\n    l.append(pad_sequences(item, maxlen = max_val, dtype=\'int\',padding=\'post\',truncating=\'post\'))\n    \nX_data = np.array(l)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 12:40:37,696 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 12:40:37,696 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '6d68482532594d9c91afd52879ec74ed', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 11, 36, 31, 100131, tzinfo=tzutc())}, 'msg_id': '6d68482532594d9c91afd52879ec74ed', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': "X_data = np.array([np.transpose(pad_sequences(np.transpose(item), maxlen = max_val, dtype='int',padding='post',truncating='post')) for item in batch_x])", 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 12:40:37,697 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 12:40:37,698 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '064bab5d3ab74da98cdd751715b2e309', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 11, 36, 31, 101133, tzinfo=tzutc())}, 'msg_id': '064bab5d3ab74da98cdd751715b2e309', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'X_data[0].shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 12:40:37,699 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 12:40:37,699 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'b2c0d8872ba84fc98c73ea50da0aefeb', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 11, 36, 31, 103139, tzinfo=tzutc())}, 'msg_id': 'b2c0d8872ba84fc98c73ea50da0aefeb', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'batch_x[0].shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 12:40:37,700 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 12:40:37,700 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '40d4094131654dbe8e5c6b85f74dadbc', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 11, 36, 31, 108152, tzinfo=tzutc())}, 'msg_id': '40d4094131654dbe8e5c6b85f74dadbc', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'batch_y = df_train["Y"][0:16]\ny_val = [len(l) for l in batch_y]\nmax_y = max(y_val)\nlabels = np.array([np.transpose(pad_sequences(np.transpose(item), maxlen = max_y, dtype=\'int\',padding=\'post\',truncating=\'post\')) for item in batch_x])\nlabels.shape', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 12:40:37,702 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 12:40:37,702 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '9a89d85ec1e34472a817c4a1ebb9fade', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 11, 36, 31, 110158, tzinfo=tzutc())}, 'msg_id': '9a89d85ec1e34472a817c4a1ebb9fade', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'list(alphabets.keys())[list(alphabets.values()).index(1)]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 12:40:37,704 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 12:40:37,704 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '876426a040f94e869333d623679914e6', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'kernel_info_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 11, 40, 34, 647054, tzinfo=tzutc())}, 'msg_id': '876426a040f94e869333d623679914e6', 'msg_type': 'kernel_info_request', 'parent_header': {}, 'metadata': {}, 'content': {}, 'buffers': []}
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=562, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=62, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-14 12:54:52,413 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
<ipython-input-22-b3f47c9a9b7f> in <module>()
----> 1 main(args)

<ipython-input-17-117266548185> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    211                 outs = model.train_on_batch(x, y,
    212                                             sample_weight=sample_weight,
--> 213                                             class_weight=class_weight)
    214 
    215                 outs = to_list(outs)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in train_on_batch(self, x, y, sample_weight, class_weight)
   1213             ins = x + y + sample_weights
   1214         self._make_train_function()
-> 1215         outputs = self.train_function(ins)
   1216         return unpack_singleton(outputs)
   1217 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in __call__(self, inputs)
   2664                 return self._legacy_call(inputs)
   2665 
-> 2666             return self._call(inputs)
   2667         else:
   2668             if py_any(is_tensor(x) for x in inputs):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in _call(self, inputs)
   2634                                 symbol_vals,
   2635                                 session)
-> 2636         fetched = self._callable_fn(*array_vals)
   2637         return fetched[:len(self.outputs)]
   2638 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\client\session.py in __call__(self, *args, **kwargs)
   1437           ret = tf_session.TF_SessionRunCallable(
   1438               self._session._session, self._handle, args, status,
-> 1439               run_metadata_ptr)
   1440         if run_metadata:
   1441           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

KeyboardInterrupt: 
2018-12-14 12:54:52,415 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 12:54:52,415 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '53dcdfd5b72b400084a1e4a7e56e28ab', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'kernel_info_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 11, 54, 49, 883261, tzinfo=tzutc())}, 'msg_id': '53dcdfd5b72b400084a1e4a7e56e28ab', 'msg_type': 'kernel_info_request', 'parent_header': {}, 'metadata': {}, 'content': {}, 'buffers': []}
Using TensorFlow backend.
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=562, epochs=50, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=62, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-14 13:14:35,839 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-19-b3f47c9a9b7f> in <module>
----> 1 main(args)

<ipython-input-17-117266548185> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    245                     break
    246 
--> 247             callbacks.on_epoch_end(epoch, epoch_logs)
    248             epoch += 1
    249             if callback_model.stop_training:

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\callbacks.py in on_epoch_end(self, epoch, logs)
     75         logs = logs or {}
     76         for callback in self.callbacks:
---> 77             callback.on_epoch_end(epoch, logs)
     78 
     79     def on_batch_begin(self, batch, logs=None):

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in on_epoch_end(self, epoch, logs)
    110         if(self.shuffle_epoch_end):
    111             print("shuffle_epoch_end")
--> 112             self.validdata.genshuffle()
    113 
    114 

~\Documents\GitHub\Textfixer\KerasDeepSpeech\generator.py in genshuffle(self)
    160 
    161     def genshuffle(self):
--> 162         self.wavpath, self.transcript, self.finish = shuffle(self.wavpath,
    163                                                              self.transcript,
    164                                                              self.finish)

AttributeError: 'BatchGenerator' object has no attribute 'wavpath'
2018-12-14 13:14:35,844 - (Windows 10 nt) IPKernelApp - INFO - Finishing abort
Using TensorFlow backend.
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=281, epochs=10, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=31, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-14 14:26:07,293 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-19-b3f47c9a9b7f> in <module>
----> 1 main(args)

<ipython-input-17-8d0e6a447b41> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    245                     break
    246 
--> 247             callbacks.on_epoch_end(epoch, epoch_logs)
    248             epoch += 1
    249             if callback_model.stop_training:

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\callbacks.py in on_epoch_end(self, epoch, logs)
     75         logs = logs or {}
     76         for callback in self.callbacks:
---> 77             callback.on_epoch_end(epoch, logs)
     78 
     79     def on_batch_begin(self, batch, logs=None):

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in on_epoch_end(self, epoch, logs)
    113 
    114 
--> 115         self.validate_epoch_end(verbose=1)
    116 
    117         if self.save:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in validate_epoch_end(self, verbose)
     63                 count += 1
     64                 decode_sent = decoded_res[j]
---> 65                 corrected = correction(decode_sent)
     66                 label = word_batch['source_str'][j]
     67                 #print(label)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\text.py in correction(sentence)
    108     layer = [(0, [])]
    109     for word in words(sentence):
--> 110         layer = [(-log_probability(node + [cword]), node + [cword]) for cword in candidate_words(word) for
    111                  priority, node in layer]
    112         heapify(layer)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\text.py in candidate_words(word)
    117 def candidate_words(word):
    118     "Generate possible spelling corrections for word."
--> 119     return (known_words([word]) or known_words(edits1(word)) or known_words(edits2(word)) or [word])
    120 
    121 

~\Documents\GitHub\Textfixer\KerasDeepSpeech\text.py in known_words(words)
    122 def known_words(words):
    123     "The subset of `words` that appear in the dictionary of WORDS."
--> 124     return set(w for w in words if w in WORDS)
    125 
    126 

~\Documents\GitHub\Textfixer\KerasDeepSpeech\text.py in <genexpr>(.0)
    122 def known_words(words):
    123     "The subset of `words` that appear in the dictionary of WORDS."
--> 124     return set(w for w in words if w in WORDS)
    125 
    126 

NameError: name 'WORDS' is not defined
2018-12-14 14:26:07,298 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 14:26:07,298 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'c9ea1215f33e421d8dde483e641fca83', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 13, 22, 16, 288535, tzinfo=tzutc())}, 'msg_id': 'c9ea1215f33e421d8dde483e641fca83', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 14:26:07,299 - (Windows 10 nt) IPKernelApp - INFO - Finishing abort
Using TensorFlow backend.
2018-12-14 14:38:58,142 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
<ipython-input-17-8d0e6a447b41> in <module>
      8 
      9 #from KerasDeepSpeech.data import combine_all_wavs_and_trans_from_csvs
---> 10 from KerasDeepSpeech.generator import BatchGenerator
     11 from KerasDeepSpeech.model import *
     12 from KerasDeepSpeech.report import ReportCallback

~\Documents\GitHub\Textfixer\KerasDeepSpeech\__init__.py in <module>
----> 1 from KerasDeepSpeech.text import wer, wers, lers, levenshtein, get_model, words, log_probability, correction, candidate_words, known_words, edits1, edits2
      2 from KerasDeepSpeech.utils import text_to_int_sequence, int_to_text_sequence, save_trimmed_model, save_model, load_model_checkpoint, load_cmodel_checkpoint, MemoryCallback

~\Documents\GitHub\Textfixer\KerasDeepSpeech\text.py in <module>
    144 MODEL = None
    145 # Load known word set
--> 146 with open('./lm/words.txt') as f:
    147     WORDS = set(words(f.read()))

FileNotFoundError: [Errno 2] No such file or directory: './lm/words.txt'
2018-12-14 14:38:58,146 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 14:38:58,146 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'fa689d63a6e543eeb03d66fece6b1cd3', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 13, 36, 58, 793577, tzinfo=tzutc())}, 'msg_id': 'fa689d63a6e543eeb03d66fece6b1cd3', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'class Object(object):\n    pass\n\nargs = Object()', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 14:38:58,148 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 14:38:58,148 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'df5fb89ce077466aa2dae333586095fe', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 13, 36, 58, 795582, tzinfo=tzutc())}, 'msg_id': 'df5fb89ce077466aa2dae333586095fe', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'main(args)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 14:38:58,149 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 14:38:58,149 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '5f6159329b6349419fd345984dca2126', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 13, 36, 58, 797589, tzinfo=tzutc())}, 'msg_id': '5f6159329b6349419fd345984dca2126', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 14:38:58,150 - (Windows 10 nt) IPKernelApp - INFO - Finishing abort
Using TensorFlow backend.
2018-12-14 14:43:51,964 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
<ipython-input-17-8d0e6a447b41> in <module>
      8 
      9 #from KerasDeepSpeech.data import combine_all_wavs_and_trans_from_csvs
---> 10 from KerasDeepSpeech.generator import BatchGenerator
     11 from KerasDeepSpeech.model import *
     12 from KerasDeepSpeech.report import ReportCallback

~\Documents\GitHub\Textfixer\KerasDeepSpeech\__init__.py in <module>
----> 1 from KerasDeepSpeech.text import wer, wers, lers, levenshtein, get_model, words, log_probability, correction, candidate_words, known_words, edits1, edits2
      2 from KerasDeepSpeech.utils import text_to_int_sequence, int_to_text_sequence, save_trimmed_model, save_model, load_model_checkpoint, load_cmodel_checkpoint, MemoryCallback

~\Documents\GitHub\Textfixer\KerasDeepSpeech\text.py in <module>
    145 # Load known word set
    146 with open('./KerasDeepSpeech/lm/words.txt') as f:
--> 147     WORDS = set(words(f.read()))

~\AppData\Local\Continuum\anaconda3\lib\encodings\cp1252.py in decode(self, input, final)
     21 class IncrementalDecoder(codecs.IncrementalDecoder):
     22     def decode(self, input, final=False):
---> 23         return codecs.charmap_decode(input,self.errors,decoding_table)[0]
     24 
     25 class StreamWriter(Codec,codecs.StreamWriter):

UnicodeDecodeError: 'charmap' codec can't decode byte 0x8d in position 933900: character maps to <undefined>
2018-12-14 14:43:51,970 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 14:43:51,971 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '41b072bc658d4866910b15cf02a08f8b', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 13, 42, 0, 284131, tzinfo=tzutc())}, 'msg_id': '41b072bc658d4866910b15cf02a08f8b', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'class Object(object):\n    pass\n\nargs = Object()', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 14:43:51,972 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 14:43:51,972 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '3cc27154c29d42cfaf9c353530271530', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 13, 42, 0, 285608, tzinfo=tzutc())}, 'msg_id': '3cc27154c29d42cfaf9c353530271530', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'main(args)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 14:43:51,973 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 14:43:51,973 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'ba04beab3e9f497d8bbdf19db6a4a9ac', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 13, 42, 0, 287112, tzinfo=tzutc())}, 'msg_id': 'ba04beab3e9f497d8bbdf19db6a4a9ac', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 14:43:51,974 - (Windows 10 nt) IPKernelApp - INFO - Finishing abort
Using TensorFlow backend.
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=281, epochs=10, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=31, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-14 14:54:52,351 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-19-b3f47c9a9b7f> in <module>
----> 1 main(args)

<ipython-input-17-8d0e6a447b41> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    245                     break
    246 
--> 247             callbacks.on_epoch_end(epoch, epoch_logs)
    248             epoch += 1
    249             if callback_model.stop_training:

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\callbacks.py in on_epoch_end(self, epoch, logs)
     75         logs = logs or {}
     76         for callback in self.callbacks:
---> 77             callback.on_epoch_end(epoch, logs)
     78 
     79     def on_batch_begin(self, batch, logs=None):

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in on_epoch_end(self, epoch, logs)
    113 
    114 
--> 115         self.validate_epoch_end(verbose=1)
    116 
    117         if self.save:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in validate_epoch_end(self, verbose)
     63                 count += 1
     64                 decode_sent = decoded_res[j]
---> 65                 corrected = correction(decode_sent)
     66                 label = word_batch['source_str'][j]
     67                 #print(label)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\text.py in correction(sentence)
    109     layer = [(0, [])]
    110     for word in words(sentence):
--> 111         layer = [(-log_probability(node + [cword]), node + [cword]) for cword in candidate_words(word) for
    112                  priority, node in layer]
    113         heapify(layer)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\text.py in <listcomp>(.0)
    110     for word in words(sentence):
    111         layer = [(-log_probability(node + [cword]), node + [cword]) for cword in candidate_words(word) for
--> 112                  priority, node in layer]
    113         heapify(layer)
    114         layer = layer[:BEAM_WIDTH]

~\Documents\GitHub\Textfixer\KerasDeepSpeech\text.py in log_probability(sentence)
    101 def log_probability(sentence):
    102     "Log base 10 probability of `sentence`, a list of words"
--> 103     return get_model().score(' '.join(sentence), bos=False, eos=False)
    104 
    105 

AttributeError: 'NoneType' object has no attribute 'score'
2018-12-14 14:54:52,355 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-14 14:54:52,355 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'bc1510329eaf43d8870c39abf3a82eeb', 'username': 'username', 'session': 'f0ccc35988d14471b149ecceecfe9aee', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 14, 13, 51, 5, 541626, tzinfo=tzutc())}, 'msg_id': 'bc1510329eaf43d8870c39abf3a82eeb', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-14 14:54:52,356 - (Windows 10 nt) IPKernelApp - INFO - Finishing abort
2018-12-17 15:51:13,032 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-6-5b1e1246b2d8> in <module>
----> 1 print(len(sentences))
      2 for i in reversed(range(len(sentences))):
      3     if len(sentences[i]) < 20 or len(sentences[i]) > 100 \
      4         or sentences[i][0:9] == "Category:" \
      5         or sentences[i][0:13] == "Related pages" \

NameError: name 'sentences' is not defined
2018-12-17 15:51:13,036 - (Windows 10 nt) IPKernelApp - INFO - Finishing abort
Using TensorFlow backend.
2018-12-18 09:52:47,491 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-5-b1b4e11422ad>", line 4
    (?:(?:thumb|thumbnail|left|right|\d+px|upright(?:=[0-9\.]+)?)\|)+
     ^
SyntaxError: invalid syntax

2018-12-18 09:52:47,497 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:52:47,497 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '4bef6dec76074a688614715df0af12ee', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 52, 30, 858103, tzinfo=tzutc())}, 'msg_id': '4bef6dec76074a688614715df0af12ee', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': '# Simple wikipedia article texts into single sentences\n\nsentences = []\nsentences += [tokenize.sent_tokenize(texts[i]) for i in range(len(texts))]\n#sentences += [texts[i].split(". ") for i## 6. Divide into sentences in range(len(texts))] #len(texts)\n# Now sentences is a list of lists. The next expression flattens it into one long list.\nsentences = [item for sublist in sentences for item in sublist]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:52:47,500 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:52:47,500 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '71d30817dddf40aa8a09034483b81b57', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 52, 30, 859609, tzinfo=tzutc())}, 'msg_id': '71d30817dddf40aa8a09034483b81b57', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'pprint.pprint(sentences[0:3])', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:52:47,502 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:52:47,502 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '991ebd7e9f7e4c1f8e59587640f12cdf', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 52, 30, 861112, tzinfo=tzutc())}, 'msg_id': '991ebd7e9f7e4c1f8e59587640f12cdf', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'print(len(sentences))\nfor i in reversed(range(len(sentences))):\n    if len(sentences[i]) < 20 or len(sentences[i]) > 100 \\\n        or sentences[i][0:9] == "Category:" \\\n        or sentences[i][0:13] == "Related pages" \\\n        or sentences[i][0:10] == "Refere## 7. Clean-up sentences and remove too long and short onesMedian sentence length is 83 symbols. We remove the sentences shorter than 20 symbols and longer than 100 symbols to clean up the dataset.<br><br>\nWe also remove the sentences starting with "Category:", "Related pages", "References", "Other websites:". <br>\nThese are technical Wikipedia pages that we do not need. Need to check for more, e.g. "Gallery".nces" \\\n        or sentences[i][0:14] == "Other websites":\n        sentences.pop(i)\nprint(len(sentences))\n\n#Gallery', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:52:47,504 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:52:47,505 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '5785302d8d284a028a84044c46d97b48', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 52, 30, 862615, tzinfo=tzutc())}, 'msg_id': '5785302d8d284a028a84044c46d97b48', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'pprint.pprint(sentences[530000:530005])', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:52:47,506 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:52:47,506 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '66f4fc5fdeee46b681cfa94264fcfa59', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 52, 30, 865123, tzinfo=tzutc())}, 'msg_id': '66f4fc5fdeee46b681cfa94264fcfa59', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'import statistics\nsentence_lengths = [len(sentences[i]) for i in range(len(sentences))]\nprint(statistics.median(sentence_lengths))\n\nfrom collections import defaultdict\nappearances = defaultdict(int)\n\nsentence_lengths.sort()\n\nfor curr in sentence_lengths:\n    appearances[curr] += 1\n    \na = set(sentence_lengths) \nfor i in a:\n    print("{} - {}".format(i, appearances[i]))', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:52:47,508 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:52:47,508 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '5bbef72d3bb346518a4c43cf793e304d', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 52, 30, 867630, tzinfo=tzutc())}, 'msg_id': '5bbef72d3bb346518a4c43cf793e304d', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': '## 8. Generate training dataalphabets = {\'a\':0, \'b\':1, \'c\':2, \'d\':3, \'e\':4, \'f\':5, \'g\':6, \'h\':7, \'i\':8, \'j\':9, \'k\':10, \'l\':11, \'m\':12, \'n\':13, \'o\':14,\n            \'p\':15, \'q\':16, \'r\':17, \'s\':18, \'t\':19, \'u\':20, \'v\':21, \'w\':22, \'x\':23, \'y\':24, \'z\':25, \n            \'0\':26, \'1\':27, \'2\':28, \'3\':29, \'4\':30, \'5\':31, \'6\':32, \'7\':33, \'8\':34, \'9\':35, \n            \' \':36, \',\':37, \'.\':38, \':\':39, \';\':40, \'"\':41, "\'":42, \'\':43, \'(\':44, \')\':45} #43 = unknown symbol\n\nidxs = [alphabets[ch] if ch in alphabets else 43 for ch in \'az 123#\']\n\nidxs\n\n#one_hot = tf.one_hot(idxs, depth=len(alphabets), dtype=tf.uint8)\n\n#sess = tf.InteractiveSession()\n#one_hot.eval()\none_hot = to_categorical(idxs, num_classes = len(alphabets))\none_hot', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:52:47,510 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:52:47,510 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '3738dec7fb1c436d8061414f5690ffb4', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 52, 30, 868632, tzinfo=tzutc())}, 'msg_id': '3738dec7fb1c436d8061414f5690ffb4', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'sentences_idxs = []\nfor i in range(len(sentences)):\n    idx = []\n    for j in sentences[i]:\n        if j in alphabets:\n            idx += [alphabets[j]]\n        else:\n            idx += [43]\n    sentences_idxs.append(idx)\n    \n#sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(len(sentences_idxs))]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:52:47,512 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:52:47,512 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '27d89278c4be4910860840ecd0b798e0', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 52, 30, 870640, tzinfo=tzutc())}, 'msg_id': '27d89278c4be4910860840ecd0b798e0', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': '#sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(10000)]\nsentences_onehot = [to_categorical(sentences_idxs[i], num_classes = len(alphabets)) for i in range(10000)]\nsentences = sentences[0:10000]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:52:47,513 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:52:47,514 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '9663465509fb429db764db436e006ee7', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 52, 30, 871639, tzinfo=tzutc())}, 'msg_id': '9663465509fb429db764db436e006ee7', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': "# Generate the data examples\n# X and Y are identical for the test purposes\n\ndata = pd.DataFrame(\n    {'X': sentences_onehot,\n     'Y': sentences\n    })\n\nprint(len(sentences_onehot[100][0]))\nprint(len(sentences_onehot))", 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:52:47,515 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:52:47,515 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '191d7f463a354a1bbbe0b166e0d88164', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 52, 30, 873144, tzinfo=tzutc())}, 'msg_id': '191d7f463a354a1bbbe0b166e0d88164', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'len(sentences_idxs)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:52:47,517 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:52:47,518 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '15587cf12fa3468c9b475d7d069d6909', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 52, 30, 874647, tzinfo=tzutc())}, 'msg_id': '15587cf12fa3468c9b475d7d069d6909', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': '#####################################################\n\nimport os\n\nimport keras\nfrom keras.callbacks import TensorBoard\nfrom keras.optimizers import Adam, Nadam\n\n#from KerasDeepSpeech.data import combine_all_wavs_and_trans_from_csvs\nfrom KerasDeepSpeech.generator import BatchGenerator\nfrom KerasDeepSpeech.model import *\nfrom KerasDeepSpeech.report import ReportCallback\nfrom KerasDeepSpeech.utils import load_model_checkpoint, save_model, MemoryCallback\n\n#####################################################\n\n\n#######################################################\n\n# Prevent pool_allocator message\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n#######################################################\n\n\ndef main(args):\n    \'\'\'\n    There are 5 simple steps to this program\n    \'\'\'\n\n    #1. combine all data into 2 dataframes (train, valid)\n    print("Getting data from arguments")\n    #train_dataprops, df_train = combine_all_wavs_and_trans_from_csvs(args.train_files, sortagrad=args.sortagrad)\n    #valid_dataprops, df_valid = combine_all_wavs_and_trans_from_csvs(args.valid_files, sortagrad=args.sortagrad)\n\n    train_ratio = 0.9\n    args.model_arch = 0\n    args.opt = "adam"\n    args.train_steps = 0\n    args.epochs = 10\n    args.valid_steps = 0\n    args.batchsize = 32 #was 16\n    args.name = ""\n    args.loadcheckpointpath = ""\n    args.fc_size = 512\n    args.rnn_size = 512\n    args.learning_rate = 0.01\n    args.memcheck = False\n    args.tensorboard = True\n    \n    model_input_type = "text"\n    \n    \n    df_train = data[0:int(train_ratio * len(sentences_onehot))]\n    df_valid = data[int(train_ratio * len(sentences_onehot)) + 1:]\n\n\n    ## 2. init data generators\n    print("Creating data batch generators")\n    traindata = BatchGenerator(dataframe=df_train, dataproperties=None,\n                              training=True, batch_size=args.batchsize, model_input_type=model_input_type)\n    validdata = BatchGenerator(dataframe=df_valid, dataproperties=None,\n                              training=False, batch_size=args.batchsize, model_input_type=model_input_type)\n\n\n\n\n    output_dir = os.path.join(\'checkpoints/results\',\n                                  \'model%s_%s\' % (args.model_arch,\n                                             args.name))\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir)\n\n\n    ## 3. Load existing or create new model\n    if args.loadcheckpointpath:\n        # load existing\n        print("Loading model")\n\n        cp = args.loadcheckpointpath\n        assert(os.path.isdir(cp))\n\n        model_path = os.path.join(cp, "model")\n        # assert(os.path.isfile(model_path))\n\n        model = load_model_checkpoint(model_path)\n\n\n        print("Model loaded")\n    else:\n        # new model recipes here\n        print(\'New model DS{}\'.format(args.model_arch))\n        if (args.model_arch == 0):\n            # DeepSpeech1 with Dropout\n            model = ds1_dropout(input_dim=len(alphabets), fc_size=args.fc_size, rnn_size=args.rnn_size,dropout=[0.1,0.1,0.1], output_dim=len(alphabets) + 1)\n\n        elif(args.model_arch==1):\n            # DeepSpeech1 - no dropout\n            model = ds1(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n\n        elif(args.model_arch==2):\n            # DeepSpeech2 model\n            model = ds2_gru_model(input_dim=161, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n\n        elif(args.model_arch==3):\n            # own model\n            model = ownModel(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, dropout=[0.1, 0.1, 0.1], output_dim=29)\n\n        elif(args.model_arch==4):\n            # graves model\n            model = graves(input_dim=26, rnn_size=args.rnn_size, output_dim=29, std=0.5)\n\n        elif(args.model_arch==5):\n            #cnn city\n            model = cnn_city(input_dim=161, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n\n        elif(args.model_arch == 6):\n            # constrained model\n            model = const(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n        else:\n            raise("model not found")\n\n        print(model.summary(line_length=80))\n\n        #required to save the JSON\n        save_model(model, output_dir)\n\n    if (args.opt.lower() == \'sgd\'):\n        opt = SGD(lr=args.learning_rate, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n    elif (args.opt.lower() == \'adam\'):\n        opt = Adam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n    elif (args.opt.lower() == \'nadam\'):\n        opt = Nadam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n    else:\n        raise "optimiser not recognised"\n\n    model.compile(optimizer=opt, loss=ctc)\n\n    ## 4. train\n\n    if args.train_steps == 0:\n        args.train_steps = len(df_train.index) // args.batchsize\n        # print(args.train_steps)\n    # we use 1/xth of the validation data at each epoch end to test val score\n    if args.valid_steps == 0:\n\n        args.valid_steps = (len(df_valid.index) // args.batchsize)\n        # print(args.valid_steps)\n\n\n    if args.memcheck:\n        cb_list = [MemoryCallback()]\n    else:\n        cb_list = []\n\n    if args.tensorboard:\n        tb_cb = TensorBoard(log_dir=\'./tensorboard/{}/\'.format(args.name), write_graph=False, write_images=True)\n        cb_list.append(tb_cb)\n\n    y_pred = model.get_layer(\'ctc\').input[0]\n    input_data = model.get_layer(\'the_input\').input\n\n    report = K.function([input_data, K.learning_phase()], [y_pred])\n    report_cb = ReportCallback(report, validdata, model, args.name, save=True)\n\n    cb_list.append(report_cb)\n\n    model.fit_generator(generator=traindata.next_batch(),\n                        steps_per_epoch=args.train_steps,\n                        epochs=args.epochs,\n                        callbacks=cb_list,\n                        validation_data=validdata.next_batch(),\n                        validation_steps=args.valid_steps,\n                        initial_epoch=0,\n                        verbose=1,\n                        class_weight=None,\n                        max_q_size=10,\n                        workers=1,\n                        pickle_safe=False\n                        )\n\n    # K.clear_session()\n\n    ## These are the most important metrics\n    print("Mean WER   :", report_cb.mean_wer_log)\n    print("Mean LER   :", report_cb.mean_ler_log)\n    print("NormMeanLER:", report_cb.norm_mean_ler_log)\n\n    # export to csv?\n    K.clear_session()', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:52:47,519 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:52:47,519 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '82f4ddb34ce24662a03e07e6f9be99b0', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 52, 30, 876152, tzinfo=tzutc())}, 'msg_id': '82f4ddb34ce24662a03e07e6f9be99b0', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'class Object(object):\n    pass\n\nargs = Object()', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:52:47,520 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:52:47,520 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '9d2c1affbaa244c6acb896aba8d569c7', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 52, 30, 877154, tzinfo=tzutc())}, 'msg_id': '9d2c1affbaa244c6acb896aba8d569c7', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'main(args)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:52:47,521 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:52:47,522 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'a78826e6162a401b94496be4aa488458', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 52, 30, 878658, tzinfo=tzutc())}, 'msg_id': 'a78826e6162a401b94496be4aa488458', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:52:47,522 - (Windows 10 nt) IPKernelApp - INFO - Finishing abort
2018-12-18 09:54:38,322 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-9-1087aa0571f6>", line 6
    or sentences[i][0:10] == "Refere## 7. Clean-up sentences and remove too long and short onesMedian sentence length is 83 symbols. We remove the sentences shorter than 20 symbols and longer than 100 symbols to clean up the dataset.<br><br>
                                                                                                                                                                                                                                                 ^
SyntaxError: EOL while scanning string literal

2018-12-18 09:54:38,329 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:54:38,329 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'e02ab500cdc74544ba5c6c97e3b5d5c5', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 53, 34, 25706, tzinfo=tzutc())}, 'msg_id': 'e02ab500cdc74544ba5c6c97e3b5d5c5', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'pprint.pprint(sentences[530000:530005])', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:54:38,330 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:54:38,331 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'dca3d38c774c4c42b49d64d6ce7d2576', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 53, 34, 251811, tzinfo=tzutc())}, 'msg_id': 'dca3d38c774c4c42b49d64d6ce7d2576', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'import statistics\nsentence_lengths = [len(sentences[i]) for i in range(len(sentences))]\nprint(statistics.median(sentence_lengths))\n\nfrom collections import defaultdict\nappearances = defaultdict(int)\n\nsentence_lengths.sort()\n\nfor curr in sentence_lengths:\n    appearances[curr] += 1\n    \na = set(sentence_lengths) \nfor i in a:\n    print("{} - {}".format(i, appearances[i]))', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:54:38,333 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:54:38,333 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '2eebe58672774a4883a905f101c76d81', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 53, 35, 503665, tzinfo=tzutc())}, 'msg_id': '2eebe58672774a4883a905f101c76d81', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': '## 8. Generate training dataalphabets = {\'a\':0, \'b\':1, \'c\':2, \'d\':3, \'e\':4, \'f\':5, \'g\':6, \'h\':7, \'i\':8, \'j\':9, \'k\':10, \'l\':11, \'m\':12, \'n\':13, \'o\':14,\n            \'p\':15, \'q\':16, \'r\':17, \'s\':18, \'t\':19, \'u\':20, \'v\':21, \'w\':22, \'x\':23, \'y\':24, \'z\':25, \n            \'0\':26, \'1\':27, \'2\':28, \'3\':29, \'4\':30, \'5\':31, \'6\':32, \'7\':33, \'8\':34, \'9\':35, \n            \' \':36, \',\':37, \'.\':38, \':\':39, \';\':40, \'"\':41, "\'":42, \'\':43, \'(\':44, \')\':45} #43 = unknown symbol\n\nidxs = [alphabets[ch] if ch in alphabets else 43 for ch in \'az 123#\']\n\nidxs\n\n#one_hot = tf.one_hot(idxs, depth=len(alphabets), dtype=tf.uint8)\n\n#sess = tf.InteractiveSession()\n#one_hot.eval()\none_hot = to_categorical(idxs, num_classes = len(alphabets))\none_hot', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:54:38,335 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:54:38,336 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '3d9f9c14dc934a4885923b6511a2c446', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 53, 35, 950362, tzinfo=tzutc())}, 'msg_id': '3d9f9c14dc934a4885923b6511a2c446', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'sentences_idxs = []\nfor i in range(len(sentences)):\n    idx = []\n    for j in sentences[i]:\n        if j in alphabets:\n            idx += [alphabets[j]]\n        else:\n            idx += [43]\n    sentences_idxs.append(idx)\n    \n#sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(len(sentences_idxs))]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:54:38,337 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:54:38,337 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'a2cdac70c0064de887b90d5a508cbcbb', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 53, 36, 903417, tzinfo=tzutc())}, 'msg_id': 'a2cdac70c0064de887b90d5a508cbcbb', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': '#sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(10000)]\nsentences_onehot = [to_categorical(sentences_idxs[i], num_classes = len(alphabets)) for i in range(10000)]\nsentences = sentences[0:10000]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:54:38,339 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:54:38,339 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'ccf67d1f4cdf45f48181d97a7bda365b', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 53, 37, 241883, tzinfo=tzutc())}, 'msg_id': 'ccf67d1f4cdf45f48181d97a7bda365b', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': "# Generate the data examples\n# X and Y are identical for the test purposes\n\ndata = pd.DataFrame(\n    {'X': sentences_onehot,\n     'Y': sentences\n    })\n\nprint(len(sentences_onehot[100][0]))\nprint(len(sentences_onehot))", 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:54:38,340 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:54:38,341 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '0bc8aab0158c4e058e4e15911440cc54', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 53, 38, 232035, tzinfo=tzutc())}, 'msg_id': '0bc8aab0158c4e058e4e15911440cc54', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'len(sentences_idxs)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:54:38,343 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:54:38,343 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '11461bb2150f4b798dbbb850187e23fa', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 53, 39, 296889, tzinfo=tzutc())}, 'msg_id': '11461bb2150f4b798dbbb850187e23fa', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': '#####################################################\n\nimport os\n\nimport keras\nfrom keras.callbacks import TensorBoard\nfrom keras.optimizers import Adam, Nadam\n\n#from KerasDeepSpeech.data import combine_all_wavs_and_trans_from_csvs\nfrom KerasDeepSpeech.generator import BatchGenerator\nfrom KerasDeepSpeech.model import *\nfrom KerasDeepSpeech.report import ReportCallback\nfrom KerasDeepSpeech.utils import load_model_checkpoint, save_model, MemoryCallback\n\n#####################################################\n\n\n#######################################################\n\n# Prevent pool_allocator message\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n#######################################################\n\n\ndef main(args):\n    \'\'\'\n    There are 5 simple steps to this program\n    \'\'\'\n\n    #1. combine all data into 2 dataframes (train, valid)\n    print("Getting data from arguments")\n    #train_dataprops, df_train = combine_all_wavs_and_trans_from_csvs(args.train_files, sortagrad=args.sortagrad)\n    #valid_dataprops, df_valid = combine_all_wavs_and_trans_from_csvs(args.valid_files, sortagrad=args.sortagrad)\n\n    train_ratio = 0.9\n    args.model_arch = 0\n    args.opt = "adam"\n    args.train_steps = 0\n    args.epochs = 10\n    args.valid_steps = 0\n    args.batchsize = 32 #was 16\n    args.name = ""\n    args.loadcheckpointpath = ""\n    args.fc_size = 512\n    args.rnn_size = 512\n    args.learning_rate = 0.01\n    args.memcheck = False\n    args.tensorboard = True\n    \n    model_input_type = "text"\n    \n    \n    df_train = data[0:int(train_ratio * len(sentences_onehot))]\n    df_valid = data[int(train_ratio * len(sentences_onehot)) + 1:]\n\n\n    ## 2. init data generators\n    print("Creating data batch generators")\n    traindata = BatchGenerator(dataframe=df_train, dataproperties=None,\n                              training=True, batch_size=args.batchsize, model_input_type=model_input_type)\n    validdata = BatchGenerator(dataframe=df_valid, dataproperties=None,\n                              training=False, batch_size=args.batchsize, model_input_type=model_input_type)\n\n\n\n\n    output_dir = os.path.join(\'checkpoints/results\',\n                                  \'model%s_%s\' % (args.model_arch,\n                                             args.name))\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir)\n\n\n    ## 3. Load existing or create new model\n    if args.loadcheckpointpath:\n        # load existing\n        print("Loading model")\n\n        cp = args.loadcheckpointpath\n        assert(os.path.isdir(cp))\n\n        model_path = os.path.join(cp, "model")\n        # assert(os.path.isfile(model_path))\n\n        model = load_model_checkpoint(model_path)\n\n\n        print("Model loaded")\n    else:\n        # new model recipes here\n        print(\'New model DS{}\'.format(args.model_arch))\n        if (args.model_arch == 0):\n            # DeepSpeech1 with Dropout\n            model = ds1_dropout(input_dim=len(alphabets), fc_size=args.fc_size, rnn_size=args.rnn_size,dropout=[0.1,0.1,0.1], output_dim=len(alphabets) + 1)\n\n        elif(args.model_arch==1):\n            # DeepSpeech1 - no dropout\n            model = ds1(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n\n        elif(args.model_arch==2):\n            # DeepSpeech2 model\n            model = ds2_gru_model(input_dim=161, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n\n        elif(args.model_arch==3):\n            # own model\n            model = ownModel(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, dropout=[0.1, 0.1, 0.1], output_dim=29)\n\n        elif(args.model_arch==4):\n            # graves model\n            model = graves(input_dim=26, rnn_size=args.rnn_size, output_dim=29, std=0.5)\n\n        elif(args.model_arch==5):\n            #cnn city\n            model = cnn_city(input_dim=161, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n\n        elif(args.model_arch == 6):\n            # constrained model\n            model = const(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n        else:\n            raise("model not found")\n\n        print(model.summary(line_length=80))\n\n        #required to save the JSON\n        save_model(model, output_dir)\n\n    if (args.opt.lower() == \'sgd\'):\n        opt = SGD(lr=args.learning_rate, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n    elif (args.opt.lower() == \'adam\'):\n        opt = Adam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n    elif (args.opt.lower() == \'nadam\'):\n        opt = Nadam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n    else:\n        raise "optimiser not recognised"\n\n    model.compile(optimizer=opt, loss=ctc)\n\n    ## 4. train\n\n    if args.train_steps == 0:\n        args.train_steps = len(df_train.index) // args.batchsize\n        # print(args.train_steps)\n    # we use 1/xth of the validation data at each epoch end to test val score\n    if args.valid_steps == 0:\n\n        args.valid_steps = (len(df_valid.index) // args.batchsize)\n        # print(args.valid_steps)\n\n\n    if args.memcheck:\n        cb_list = [MemoryCallback()]\n    else:\n        cb_list = []\n\n    if args.tensorboard:\n        tb_cb = TensorBoard(log_dir=\'./tensorboard/{}/\'.format(args.name), write_graph=False, write_images=True)\n        cb_list.append(tb_cb)\n\n    y_pred = model.get_layer(\'ctc\').input[0]\n    input_data = model.get_layer(\'the_input\').input\n\n    report = K.function([input_data, K.learning_phase()], [y_pred])\n    report_cb = ReportCallback(report, validdata, model, args.name, save=True)\n\n    cb_list.append(report_cb)\n\n    model.fit_generator(generator=traindata.next_batch(),\n                        steps_per_epoch=args.train_steps,\n                        epochs=args.epochs,\n                        callbacks=cb_list,\n                        validation_data=validdata.next_batch(),\n                        validation_steps=args.valid_steps,\n                        initial_epoch=0,\n                        verbose=1,\n                        class_weight=None,\n                        max_q_size=10,\n                        workers=1,\n                        pickle_safe=False\n                        )\n\n    # K.clear_session()\n\n    ## These are the most important metrics\n    print("Mean WER   :", report_cb.mean_wer_log)\n    print("Mean LER   :", report_cb.mean_ler_log)\n    print("NormMeanLER:", report_cb.norm_mean_ler_log)\n\n    # export to csv?\n    K.clear_session()', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:54:38,345 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:54:38,345 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'bf8874a663b046218185bab027cb989c', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 53, 40, 165215, tzinfo=tzutc())}, 'msg_id': 'bf8874a663b046218185bab027cb989c', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'class Object(object):\n    pass\n\nargs = Object()', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:54:38,347 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:54:38,347 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'd7d7aee2a5ad40b4a4eec4281bb31bd5', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 53, 40, 348205, tzinfo=tzutc())}, 'msg_id': 'd7d7aee2a5ad40b4a4eec4281bb31bd5', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'main(args)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:54:38,347 - (Windows 10 nt) IPKernelApp - INFO - Finishing abort
2018-12-18 09:56:04,667 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
  File "<ipython-input-13-656c1f3a10e7>", line 2
    'p':15, 'q':16, 'r':17, 's':18, 't':19, 'u':20, 'v':21, 'w':22, 'x':23, 'y':24, 'z':25,
    ^
IndentationError: unexpected indent

2018-12-18 09:56:04,673 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:56:04,673 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '072aa29520ca451da5212a5d91b44f1a', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 55, 55, 319542, tzinfo=tzutc())}, 'msg_id': '072aa29520ca451da5212a5d91b44f1a', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'sentences_idxs = []\nfor i in range(len(sentences)):\n    idx = []\n    for j in sentences[i]:\n        if j in alphabets:\n            idx += [alphabets[j]]\n        else:\n            idx += [43]\n    sentences_idxs.append(idx)\n    \n#sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(len(sentences_idxs))]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:56:04,674 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:56:04,675 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '62cf6fb777d744289e70a41f8eced8a5', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 55, 56, 254057, tzinfo=tzutc())}, 'msg_id': '62cf6fb777d744289e70a41f8eced8a5', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': '#sentences_onehot = [tf.one_hot(sentences_idxs[i], depth=len(alphabets), dtype=tf.uint8) for i in range(10000)]\nsentences_onehot = [to_categorical(sentences_idxs[i], num_classes = len(alphabets)) for i in range(10000)]\nsentences = sentences[0:10000]', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:56:04,677 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:56:04,677 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '07256292b90f4c79884f2df5708d679b', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 55, 56, 462604, tzinfo=tzutc())}, 'msg_id': '07256292b90f4c79884f2df5708d679b', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': "# Generate the data examples\n# X and Y are identical for the test purposes\n\ndata = pd.DataFrame(\n    {'X': sentences_onehot,\n     'Y': sentences\n    })\n\nprint(len(sentences_onehot[100][0]))\nprint(len(sentences_onehot))", 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:56:04,679 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:56:04,680 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '696bf02457094047a47aafe992baae3e', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 55, 57, 37645, tzinfo=tzutc())}, 'msg_id': '696bf02457094047a47aafe992baae3e', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'len(sentences_idxs)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:56:04,682 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:56:04,682 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '2b5e01fc57354c80b9874db7c1f0a4ce', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 55, 57, 409642, tzinfo=tzutc())}, 'msg_id': '2b5e01fc57354c80b9874db7c1f0a4ce', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': '#####################################################\n\nimport os\n\nimport keras\nfrom keras.callbacks import TensorBoard\nfrom keras.optimizers import Adam, Nadam\n\n#from KerasDeepSpeech.data import combine_all_wavs_and_trans_from_csvs\nfrom KerasDeepSpeech.generator import BatchGenerator\nfrom KerasDeepSpeech.model import *\nfrom KerasDeepSpeech.report import ReportCallback\nfrom KerasDeepSpeech.utils import load_model_checkpoint, save_model, MemoryCallback\n\n#####################################################\n\n\n#######################################################\n\n# Prevent pool_allocator message\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n#######################################################\n\n\ndef main(args):\n    \'\'\'\n    There are 5 simple steps to this program\n    \'\'\'\n\n    #1. combine all data into 2 dataframes (train, valid)\n    print("Getting data from arguments")\n    #train_dataprops, df_train = combine_all_wavs_and_trans_from_csvs(args.train_files, sortagrad=args.sortagrad)\n    #valid_dataprops, df_valid = combine_all_wavs_and_trans_from_csvs(args.valid_files, sortagrad=args.sortagrad)\n\n    train_ratio = 0.9\n    args.model_arch = 0\n    args.opt = "adam"\n    args.train_steps = 0\n    args.epochs = 10\n    args.valid_steps = 0\n    args.batchsize = 32 #was 16\n    args.name = ""\n    args.loadcheckpointpath = ""\n    args.fc_size = 512\n    args.rnn_size = 512\n    args.learning_rate = 0.01\n    args.memcheck = False\n    args.tensorboard = True\n    \n    model_input_type = "text"\n    \n    \n    df_train = data[0:int(train_ratio * len(sentences_onehot))]\n    df_valid = data[int(train_ratio * len(sentences_onehot)) + 1:]\n\n\n    ## 2. init data generators\n    print("Creating data batch generators")\n    traindata = BatchGenerator(dataframe=df_train, dataproperties=None,\n                              training=True, batch_size=args.batchsize, model_input_type=model_input_type)\n    validdata = BatchGenerator(dataframe=df_valid, dataproperties=None,\n                              training=False, batch_size=args.batchsize, model_input_type=model_input_type)\n\n\n\n\n    output_dir = os.path.join(\'checkpoints/results\',\n                                  \'model%s_%s\' % (args.model_arch,\n                                             args.name))\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir)\n\n\n    ## 3. Load existing or create new model\n    if args.loadcheckpointpath:\n        # load existing\n        print("Loading model")\n\n        cp = args.loadcheckpointpath\n        assert(os.path.isdir(cp))\n\n        model_path = os.path.join(cp, "model")\n        # assert(os.path.isfile(model_path))\n\n        model = load_model_checkpoint(model_path)\n\n\n        print("Model loaded")\n    else:\n        # new model recipes here\n        print(\'New model DS{}\'.format(args.model_arch))\n        if (args.model_arch == 0):\n            # DeepSpeech1 with Dropout\n            model = ds1_dropout(input_dim=len(alphabets), fc_size=args.fc_size, rnn_size=args.rnn_size,dropout=[0.1,0.1,0.1], output_dim=len(alphabets) + 1)\n\n        elif(args.model_arch==1):\n            # DeepSpeech1 - no dropout\n            model = ds1(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n\n        elif(args.model_arch==2):\n            # DeepSpeech2 model\n            model = ds2_gru_model(input_dim=161, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n\n        elif(args.model_arch==3):\n            # own model\n            model = ownModel(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, dropout=[0.1, 0.1, 0.1], output_dim=29)\n\n        elif(args.model_arch==4):\n            # graves model\n            model = graves(input_dim=26, rnn_size=args.rnn_size, output_dim=29, std=0.5)\n\n        elif(args.model_arch==5):\n            #cnn city\n            model = cnn_city(input_dim=161, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n\n        elif(args.model_arch == 6):\n            # constrained model\n            model = const(input_dim=26, fc_size=args.fc_size, rnn_size=args.rnn_size, output_dim=29)\n        else:\n            raise("model not found")\n\n        print(model.summary(line_length=80))\n\n        #required to save the JSON\n        save_model(model, output_dir)\n\n    if (args.opt.lower() == \'sgd\'):\n        opt = SGD(lr=args.learning_rate, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n    elif (args.opt.lower() == \'adam\'):\n        opt = Adam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n    elif (args.opt.lower() == \'nadam\'):\n        opt = Nadam(lr=args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n    else:\n        raise "optimiser not recognised"\n\n    model.compile(optimizer=opt, loss=ctc)\n\n    ## 4. train\n\n    if args.train_steps == 0:\n        args.train_steps = len(df_train.index) // args.batchsize\n        # print(args.train_steps)\n    # we use 1/xth of the validation data at each epoch end to test val score\n    if args.valid_steps == 0:\n\n        args.valid_steps = (len(df_valid.index) // args.batchsize)\n        # print(args.valid_steps)\n\n\n    if args.memcheck:\n        cb_list = [MemoryCallback()]\n    else:\n        cb_list = []\n\n    if args.tensorboard:\n        tb_cb = TensorBoard(log_dir=\'./tensorboard/{}/\'.format(args.name), write_graph=False, write_images=True)\n        cb_list.append(tb_cb)\n\n    y_pred = model.get_layer(\'ctc\').input[0]\n    input_data = model.get_layer(\'the_input\').input\n\n    report = K.function([input_data, K.learning_phase()], [y_pred])\n    report_cb = ReportCallback(report, validdata, model, args.name, save=True)\n\n    cb_list.append(report_cb)\n\n    model.fit_generator(generator=traindata.next_batch(),\n                        steps_per_epoch=args.train_steps,\n                        epochs=args.epochs,\n                        callbacks=cb_list,\n                        validation_data=validdata.next_batch(),\n                        validation_steps=args.valid_steps,\n                        initial_epoch=0,\n                        verbose=1,\n                        class_weight=None,\n                        max_q_size=10,\n                        workers=1,\n                        pickle_safe=False\n                        )\n\n    # K.clear_session()\n\n    ## These are the most important metrics\n    print("Mean WER   :", report_cb.mean_wer_log)\n    print("Mean LER   :", report_cb.mean_ler_log)\n    print("NormMeanLER:", report_cb.norm_mean_ler_log)\n\n    # export to csv?\n    K.clear_session()', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:56:04,685 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:56:04,685 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': 'bd1b94abeebc42048b05ad48b04e931a', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 55, 58, 270108, tzinfo=tzutc())}, 'msg_id': 'bd1b94abeebc42048b05ad48b04e931a', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'class Object(object):\n    pass\n\nargs = Object()', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:56:04,687 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 09:56:04,687 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '632f115367b8437a86ab330ebe5c0977', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 8, 55, 58, 451093, tzinfo=tzutc())}, 'msg_id': '632f115367b8437a86ab330ebe5c0977', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'main(args)', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 09:56:04,688 - (Windows 10 nt) IPKernelApp - INFO - Finishing abort
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=281, epochs=10, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=31, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
2018-12-18 10:11:39,121 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-20-b3f47c9a9b7f> in <module>
----> 1 main(args)

<ipython-input-18-8d0e6a447b41> in main(args)
    177                         max_q_size=10,
    178                         workers=1,
--> 179                         pickle_safe=False
    180                         )
    181 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1413             use_multiprocessing=use_multiprocessing,
   1414             shuffle=shuffle,
-> 1415             initial_epoch=initial_epoch)
   1416 
   1417     @interfaces.legacy_generator_methods_support

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    245                     break
    246 
--> 247             callbacks.on_epoch_end(epoch, epoch_logs)
    248             epoch += 1
    249             if callback_model.stop_training:

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\callbacks.py in on_epoch_end(self, epoch, logs)
     75         logs = logs or {}
     76         for callback in self.callbacks:
---> 77             callback.on_epoch_end(epoch, logs)
     78 
     79     def on_batch_begin(self, batch, logs=None):

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in on_epoch_end(self, epoch, logs)
    113 
    114 
--> 115         self.validate_epoch_end(verbose=1)
    116 
    117         if self.save:

~\Documents\GitHub\Textfixer\KerasDeepSpeech\report.py in validate_epoch_end(self, verbose)
     63                 count += 1
     64                 decode_sent = decoded_res[j]
---> 65                 corrected = correction(decode_sent)
     66                 label = word_batch['source_str'][j]
     67                 #print(label)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\text.py in correction(sentence)
    109     layer = [(0, [])]
    110     for word in words(sentence):
--> 111         layer = [(-log_probability(node + [cword]), node + [cword]) for cword in candidate_words(word) for
    112                  priority, node in layer]
    113         heapify(layer)

~\Documents\GitHub\Textfixer\KerasDeepSpeech\text.py in <listcomp>(.0)
    110     for word in words(sentence):
    111         layer = [(-log_probability(node + [cword]), node + [cword]) for cword in candidate_words(word) for
--> 112                  priority, node in layer]
    113         heapify(layer)
    114         layer = layer[:BEAM_WIDTH]

~\Documents\GitHub\Textfixer\KerasDeepSpeech\text.py in log_probability(sentence)
    101 def log_probability(sentence):
    102     "Log base 10 probability of `sentence`, a list of words"
--> 103     return get_model().score(' '.join(sentence), bos=False, eos=False)
    104 
    105 

AttributeError: 'NoneType' object has no attribute 'score'
2018-12-18 10:11:39,125 - (Windows 10 nt) IPKernelApp - INFO - Finishing abort
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=281, epochs=10, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=31, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
Using TensorFlow backend.
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=281, epochs=10, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=31, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.
  warn("To exit: use 'exit', 'quit', or Ctrl-D.", stacklevel=1)
2018-12-18 10:59:04,756 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
An exception has occurred, use %tb to see the full traceback.

SystemExit

2018-12-18 10:59:04,761 - (Windows 10 nt) IPKernelApp - INFO - Aborting:
2018-12-18 10:59:04,762 - (Windows 10 nt) IPKernelApp - INFO - {'header': {'msg_id': '2dab7e953921418daeb25b446350eaa6', 'username': 'username', 'session': '9ef48d82855f44009ce13c77ec36053a', 'msg_type': 'execute_request', 'version': '5.2', 'date': datetime.datetime(2018, 12, 18, 9, 46, 46, 887077, tzinfo=tzutc())}, 'msg_id': '2dab7e953921418daeb25b446350eaa6', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'code': 'from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())', 'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': True}, 'buffers': []}
2018-12-18 10:59:04,762 - (Windows 10 nt) IPKernelApp - INFO - Finishing abort
2018-12-18 11:01:41,189 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-18-d4f64280444b> in <module>
      1 len(sentences_idxs)
----> 2 df_train = data[0:int(train_ratio * len(sentences_onehot))]
      3 
      4 len(df_train)

NameError: name 'train_ratio' is not defined
2018-12-18 11:01:41,192 - (Windows 10 nt) IPKernelApp - INFO - Finishing abort
2018-12-18 11:03:52,338 - (Windows 10 nt) IPKernelApp - INFO - Exception in execute request:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-25-77ca22ab64cd> in <module>
      4 data = pd.DataFrame(
      5     {'X': sentences_onehot,
----> 6      'Y': sentences
      7     })
      8 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy)
    346                                  dtype=dtype, copy=copy)
    347         elif isinstance(data, dict):
--> 348             mgr = self._init_dict(data, index, columns, dtype=dtype)
    349         elif isinstance(data, ma.MaskedArray):
    350             import numpy.ma.mrecords as mrecords

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in _init_dict(self, data, index, columns, dtype)
    457             arrays = [data[k] for k in keys]
    458 
--> 459         return _arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)
    460 
    461     def _init_ndarray(self, values, index, columns, dtype=None, copy=False):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in _arrays_to_mgr(arrays, arr_names, index, columns, dtype)
   7354     # figure out the index, if necessary
   7355     if index is None:
-> 7356         index = extract_index(arrays)
   7357 
   7358     # don't force copy because getting jammed in an ndarray anyway

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in extract_index(data)
   7400             lengths = list(set(raw_lengths))
   7401             if len(lengths) > 1:
-> 7402                 raise ValueError('arrays must all be same length')
   7403 
   7404             if have_dicts:

ValueError: arrays must all be same length
2018-12-18 11:03:52,341 - (Windows 10 nt) IPKernelApp - INFO - Finishing abort
C:\Users\anfro\AppData\Local\Continuum\anaconda3\lib\site-packages\ipykernel_launcher.py:179: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=2812, epochs=10, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=312, initial_epoch=0, verbose=1, class_weight=None, workers=1, use_multiprocessing=False, max_queue_size=10)`
